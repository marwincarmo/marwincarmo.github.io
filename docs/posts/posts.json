[
  {
    "path": "posts/2021-12-20-model-selection-bias/",
    "title": "Simulating post selection inference",
    "description": "",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2022-04-07",
    "categories": [
      "simulation",
      "regression",
      "bias",
      "model selection"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is model selection\r\nWhy is it a problem?\r\nIllustration\r\n\r\nOther issues\r\nMethod\r\nResults\r\nNoise\r\nSample size\r\nCandidate predictors\r\n\r\nVisualizing\r\ndistributions\r\nConclusion\r\nAcknowledgments\r\n\r\n\r\nWhat is model selection\r\nMuch of the current research questions on behavioral and social\r\nsciences are investigated using statistical models (Flora,\r\n2017). Models are simplified translations of a reality to\r\nmathematical expressions; its’ aim is to express how data were\r\ngenerated. Regression-based models are vastly employed in empirical\r\nresearch and often used to estimate causal effects (Berk, 2010). However, to perform causal\r\nmodeling, researchers must specify a “correct” model (i.e., an accurate\r\nmodel of the data generating process) prior to data collection and use\r\nthe obtained data only to estimate regression coefficients (see Berk (2010) for further discussion). In\r\npractice, researchers usually only have a vague idea of the right model\r\nto answer their research questions, or even if such model can be\r\nestimated. Often, what is framed as statistical inference or causal\r\nmodeling is in fact a descriptive analysis; what Berk (2010) name as Level I Regression\r\nAnalysis.\r\nTo determine which variables should be included in the model, a\r\ncommon solution is to resort to variable selection algorithms. The\r\ndrawback is that whenever data-driven variable selection procedures are\r\nemployed, classical inference guarantees are invalidated due to the\r\nmodel itself becoming stochastic (Berk et al., 2013). It means that if the\r\nmodel selection method evaluates the stochastic component of the data,\r\nthe model is also considered stochastic.\r\nWhy is it a problem?\r\nVariable selection procedures aren’t in themselves problematic.\r\nNevertheless, when the correct model is unknown prior to data analysis,\r\nand the same dataset is used for I) variable selection, II)\r\nparameter estimation, and III) statistical inferences, the estimated\r\nresults can be highly biased. This is because we add a new source of\r\nuncertainty when performing model selection. These procedures discards\r\nparameter estimates from the model, and, as we shall see, the sampling\r\ndistribution of the remaining regression parameters estimates gets\r\ndistorted. In addition, the selected model isn’t the same across\r\nsamples, so there is another source of uncertainty to the estimates.\r\n(Berk\r\net al., 2010).\r\nConsider a well defined population with its unknown regression\r\nparameter values. We draw a random sample and apply a model selection\r\nprocedure. The “best” model found by the variable selection is sample\r\nspecific and isn’t guarantee to be the correct model (if we assume that\r\nsuch model in fact exists). Suppose we repeat the process of drawing a\r\nrandom sample and performing model selection 10,000 times. In this\r\nexample there are six possible candidates, and only one correct model.\r\nWe can simulate the expected frequency in which each of these models is\r\nchosen given a probability of 1/3 for the correct model and 1/9\r\notherwise. As shown in the following table, even if the correct model\r\n(in this case, \\(\\hat{M}_2\\)) is three\r\ntimes more likely to be selected than the competing models, it is\r\nexpected to be chosen more frequently but not at the majority of the\r\ntime. Therefore, the expected selected model is an incorrect one.\r\n\r\n\r\nShow code\r\n\r\nmodels <- paste0(\"M\", 1:6)\r\n\r\ntibble::enframe(\r\n  table(\r\n    sample(\r\n      models, 10000, replace = TRUE, \r\n      prob = c(1/9,1/3,1/9,1/9,1/9,1/9)))/10000,\r\n  name = \"Model\", value = \"Frequency\") |> \r\n  knitr::kable()\r\n\r\n\r\nModel\r\nFrequency\r\nM1\r\n0.1268\r\nM2\r\n0.3692\r\nM3\r\n0.1274\r\nM4\r\n0.1297\r\nM5\r\n0.1224\r\nM6\r\n0.1245\r\n\r\nTo understand why the regression parameters estimates might be\r\nbiased, recall that in a multiple regression we estimate partial\r\nregression coefficients: in a regression equation, the weight of\r\nindependent variables are estimated in relation to the\r\nother independent variables in the model (Cohen & Cohen, 1983). For a\r\ndependent variable, \\(Y\\), predicted by\r\nvariables \\(X_1\\) and \\(X_2\\), \\(B_{Y1\r\n\\cdot 2}\\) is the partial regression coefficient for \\(Y\\) on \\(X_1\\) holding \\(X_2\\) constant, and \\(B_{Y2 \\cdot 1}\\) is the partial regression\r\ncoefficient for \\(Y\\) on \\(X_2\\) holding \\(X_1\\) constant. This regression equation is\r\nwritten as:\r\n\\[\\begin{equation}\r\n\\hat{Y} = B_{Y0 \\cdot 12} + B_{Y1 \\cdot 2}X_1 + B_{Y2 \\cdot 1}X_2 +\r\n\\varepsilon\r\n\\tag{1}\r\n\\end{equation}\\]\r\nwhere \\(B_{Y0 \\cdot 12}\\) is the\r\nmodel intercept when \\(X_1\\) and \\(X_2\\) are held constant and \\(\\varepsilon\\) is the error term.\r\nThe regression coefficient for \\(X_i\\), for \\(i =\r\n\\{1, 2\\}\\), is model dependent. To see why, let’s take a look at\r\nthe equations for the regression coefficients for \\(X_1\\) (\\(B_{Y1\r\n\\cdot 2}\\)) and \\(X_2\\) (\\(B_{Y2 \\cdot 1}\\))\r\n\\[\\begin{equation}\r\nB_{Y1 \\cdot 2} = \\frac{\\rho_{Y1} - \\rho_{Y2}\\rho_{12}}{(1 -\r\n\\rho_{12}^2)} \\times \\frac{\\sigma_Y}{\\sigma_1}\r\n\\tag{2}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\nB_{Y2 \\cdot 1} = \\frac{\\rho_{Y2} - \\rho_{Y1}\\rho_{21}}{(1 -\r\n\\rho_{21}^2)} \\times \\frac{\\sigma_Y}{\\sigma_2}\r\n\\tag{3}\r\n\\end{equation}\\]\r\nHere \\(\\rho\\) stands for the\r\npopulational correlation coefficient and \\(\\sigma\\) for the populational standard\r\ndeviation. Unless we have uncorrelated predictors (i.e. \\(\\rho_{12}\\) = 0), the value for any of the\r\nregression coefficients is determined by which other predictors are in\r\nthe model. If either one is excluded from the model, a different\r\nregression coefficient will be estimated: excluding \\(X_2\\), for example, would zero all the\r\ncorrelations involving this predictor, leaving,\r\n\\[\\begin{equation}\r\nB_{Y1 \\cdot 2} = \\frac{\\rho_{Y1} - 0 \\times 0}{(1 - 0^2)} \\times\r\n\\frac{\\sigma_Y}{\\sigma_1} = \\rho_{Y1} \\times \\frac{\\sigma_Y}{\\sigma_1} =\r\nB_{Y1}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nBerk et al. (2010) warns that the sampling\r\ndistribution of the estimated regression parameters is distorted because\r\nestimates made from incorrect models will also be included, resulting in\r\na mixture of distributions. Therefore, the model selection process must\r\nbe taken into account in the regression estimation whenever it is\r\napplied.\r\nIllustration\r\nWe can illustrate the discussion above expanding an analytic example\r\ngiven by Berk et al. (2010) with simulations. Consider a model\r\nfor a response variable \\(y\\) with two\r\npotential regressors, \\(x\\) and \\(z\\). Say we are interested in the\r\nrelationship between \\(y\\) and \\(x\\) while holding \\(z\\) constant, that is, \\(\\hat{\\beta}_{yx\\cdot z}\\). Framing this as\r\na linear regression model we have\r\n\\[\\begin{equation}\r\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\varepsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nNow, suppose that we’re in a scenario where \\(\\rho_{xz}\\) = 0.5, both \\(\\beta_1\\) and \\(\\beta_2\\) are set to 1 and \\(\\varepsilon \\sim N(0, 10)\\). We’ll use a\r\nsample size of 250 subjects, and 1,000 random samples will be drawn from\r\nthis population. We’ll calculate coverage and bias for\r\neach regressor. Coverage informs the frequency in which the true\r\ncoefficient value is captured by the 95% confidence interval (CI) of the\r\nestimate. The bias of the estimations is calculated as \\(\\frac{1}{R}\\sum(\\hat{\\theta_r}-\\theta)\\),\r\nwhere \\(R\\) is the number of\r\nrepetitions, \\(\\theta\\) represents a\r\npopulation parameter, and \\(\\hat{\\theta}_r\\) a sample estimate in each\r\nsimulation.\r\n\r\n\r\np <- 2 # number of predictors\r\nSigma <- matrix(.5, p, p) # correlation matrix\r\ndiag(Sigma) <- 1\r\nn = 250 # sample size\r\nb0 <- 10 # intercept (can be set to any value)\r\nbetas <- rep(1, 2) \r\nreps = 1000\r\ncoefs <- cover <- matrix(0, nrow = reps, ncol = 2) # defining the matrices to store simulation results\r\n\r\n\r\nfor (i in seq(reps)) {\r\n  # X is a matrix of regression coefficients\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 2) , Sigma)\r\n  # with the values randomly drawn in X, we'll estimate values for y\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, betas) + rnorm(n, 0, sqrt(10)))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"x\", \"z\"), \"y\")\r\n  # fit a linear model with x and z to predict y\r\n  fit <- lm(y ~ ., data = Xy)\r\n  coefs[i, ] <- coef(fit)[-1] # save the regression coefficients\r\n  cis <- confint(fit)[-1,] # save the 95% CIs\r\n  # if the true value is capture by the CI, sum 1, 0 otherwise \r\n  cover[i,] <- ifelse(cis[,1] < 1 & cis[,2] > 1, 1, 0)\r\n}\r\ncolnames(coefs) <- c(\"x\", \"z\")\r\ncoefs <- as.data.frame(coefs)\r\n\r\ntibble::tibble(\r\n  Predictor = c(\"x\", \"z\"),\r\n  Coverage = colMeans(cover),\r\n  Bias = colMeans(coefs - betas)\r\n) |> \r\n  knitr::kable()\r\n\r\n\r\nPredictor\r\nCoverage\r\nBias\r\nx\r\n0.944\r\n-0.0080394\r\nz\r\n0.959\r\n0.0039698\r\n\r\n\r\n\r\nShow code\r\n\r\nggplot(data = coefs, aes(x = x)) +\r\n  geom_histogram(color = \"black\", fill = \"white\", bins = 30) +\r\n  theme_classic() +\r\n  theme(axis.title.y=element_blank(),\r\n        axis.text.y=element_blank(), \r\n        axis.ticks.y=element_blank(),\r\n        axis.line.y = element_blank()) +\r\n  xlab( expression(paste(\"Values of \", beta[yx.z])))\r\n\r\n\r\n\r\n\r\nFigure 1: Sampling distributions of the regression coefficient for\r\nregressor \\(x\\) in the full model.\r\n\r\n\r\n\r\nIn this first scenario \\(\\beta_{yx\\cdot\r\nz}\\) is estimated assuming \\(x\\)\r\nand \\(z\\) are always included in the\r\nmodel.\r\nWhat would happen if by model selection we arrive at a model where\r\n\\(z\\) is excluded? As indicated in\r\nequation (4), if \\(z\\)\r\nis excluded, any correlation where \\(z\\) is involved is equivalent to zero, and\r\nwe’re left with,\r\n\\[\\begin{equation}\r\n\\beta_{yx} = \\rho_{xy}(\\frac{\\sigma_y}{\\sigma_x})\r\n\\tag{6}\r\n\\end{equation}\\]\r\nNote that \\(\\beta_{yx}\\) is not the\r\nsame as \\(\\beta_{yx\\cdot z}\\). If we do\r\nnot have a model specified prior to data collection and analysis, it is\r\nnot clear which definition of regression parameter for \\(x\\) we’re trying to estimate, if \\(\\beta_{yx\\cdot z}\\), as exemplified on\r\nequation (2), or if \\(\\beta_{yx}\\) from (6). Therefore,\r\nthe definition of \\(\\hat{\\beta}_1\\)\r\ndepends on the model in which it is placed.\r\n\r\n\r\np <- 2\r\nSigma <- matrix(.5, p, p)\r\ndiag(Sigma) <- 1\r\nn = 250\r\nb0 <- 10\r\nbetas <- rep(1, 2)\r\nreps = 1000\r\ncoefs <- cover <- matrix(NA, nrow = reps, ncol = 2)\r\nfor (i in seq(reps)) {\r\n  #print(i)\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 2) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, betas) + rnorm(n, 0, sqrt(10)))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x, data = Xy)\r\n  coefs[i, ] <- coef(fit)[-1]\r\n  cis <- confint(fit)[-1,]\r\n  cover[i,] <- ifelse(cis[1] < 1 & cis[2] > 1, 1, 0)\r\n}\r\ncolnames(coefs) <- c(\"x\", \"z\")\r\ncoefs <- as.data.frame(coefs)\r\n# coverage\r\ncover1b <- colMeans(cover)[1]\r\n# bias\r\nbias1b <- colMeans((coefs - betas)^2)[1]\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nggplot(data = coefs, aes(x = x)) +\r\n  geom_histogram(color = \"black\", fill = \"white\", bins = 30) +\r\n  theme_classic() +\r\n  theme(axis.title.y=element_blank(),\r\n        axis.text.y=element_blank(), \r\n        axis.ticks.y=element_blank(),\r\n        axis.line.y = element_blank())+\r\n  xlab( expression(paste(\"Values of \", beta[yx])))\r\n\r\n\r\n\r\n\r\nFigure 2: Sampling distributions of the regression coefficient for\r\nregressor X in a reduced model where Z is excluded.\r\n\r\n\r\n\r\nNotice how far off the model estimates the coefficient for \\(x\\) when we fit our model as\r\ny ~ x. Under these conditions, we should expect a coverage\r\nof the true coefficient value of \\(x\\)\r\nof 0.316 and a bias of 0.304. Under the correct model, bias is\r\nnegligible and coverage follows the Type I error rate of 5% that we’ve\r\nset for this exercise.\r\nThis simple example help us understand why estimating model\r\nparameters in the same single random sample used to find an appropriate\r\nmodel isn’t a good idea (at least not without taking this process into\r\naccount prior to making inferences). Discarding \\(z\\) from our model has distorted the\r\nsampling distribution of \\(x\\). Thus,\r\nas Berk et al. (2010) puts it: “when a single model is\r\nnot specified before the analysis begins, it is not clear what\r\npopulation parameter is the subject of study. And without this clarity,\r\nthe reasoning behind statistical inference becomes obscure”.\r\nOther issues\r\nWe already know that if two predictors are moderately correlated and\r\none is dropped from the model, our estimation will be biased. But what\r\nelse should we consider? If we are selecting the variable every time,\r\nthen there is not an issue. In the following sections we’ll consider\r\nwhat can make things go “bad”. Before that, please consider the equation\r\nfor the standard error of the regression coefficient, estimated in the\r\nexample model selection,\r\n\\[\\begin{equation}\r\nSE(\\beta_{yx\\cdot z}) = \\frac{\\hat{\\sigma_{\\varepsilon}}}{s_x\r\n\\sqrt{n-1}}\\sqrt{\\frac{1}{1-r^2_{xy}}}\r\n\\tag{7}\r\n\\end{equation}\\]\r\nWe have that \\(\\hat{\\sigma_{\\varepsilon}}\\) is an estimate\r\nof the residual standard deviation, \\(s_x\\) is the sample standard deviation of\r\n\\(x\\), \\(r^2_{xy}\\) is the square of the sample\r\ncorrelation between \\(x\\) and \\(z\\), and \\(n\\) is the sample size. If the standard\r\nerror for a regression coefficient is large, it means that its\r\ndistribution will be more dispersed. So, from this equation, we identify\r\nas crucial parameters for a wider sampling distribution: larger residual\r\nvariance, less variance in \\(x\\),\r\nsmaller sample size, and, stronger correlation between regressors and\r\nthe response variable.\r\nWe’ll use simulated data to aid our understanding. I have build a ShinyApp for\r\nthat end. Its purpose is to illustrate the problems that arise when\r\nmodel selection, parameters estimation and statistical inferences are\r\nundertaken with the same data set. Although I will be working with code\r\nin this post, most of it can also be reproduced with this app.\r\nMethod\r\nFor the purposes of this manuscript we use the stepwise approach for\r\nmodel selection with Akaike Information Criterion (AIC) as\r\nmodel selection criterion. But it is worth note that the concerns raised\r\nhere are irrespective of the chosen model selection method (Berk\r\net al., 2010). The aforementioned app also provide the option\r\nto choose BIC or Mallow’s Cp as selection method. In addition to bias\r\nand coverage, we’ll also estimate the average Mean Squared Error (MSE),\r\ncalculated as \\(\\frac{1}{R}\\sum[(\\hat{\\theta_r}-\\theta)^2]\\).\r\nResults\r\nNoise\r\nTo express variability we can use a signal-to-noise ratio (SNR),\r\ndefined here as:\r\n\\[\\begin{equation}\r\n\\frac{S}{N} = \\textbf{b}\\Sigma\\textbf{b}\\sigma^{-2}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nwhere, \\(\\textbf{b}\\) is a\r\n(p + 1) vector of regression coefficients, \\(\\Sigma\\) is the covariance matrix of\r\npredictors and \\(\\sigma^2\\) is the\r\nerror term variance. Note that “model selection bias also occurs when an\r\nexplanatory variable has a weak relationship with the response variable.\r\nThe relationship is real, but small. Therefore, it is rarely selected as\r\nsignificant” (Lukacs et\r\nal., 2009, p. 118). We can experiment with a range of values\r\nfor the \\(SNR\\) to see how it affects\r\nthe estimates. We’ll use most values set in our first simulation\r\nexercise and \\(SNR\\) values ranging\r\nfrom 0.1 to 2. One thousand (1,000) simulations will be run for each of\r\nthose values.\r\n\r\n\r\nsim_bias <- function(reps, p, n, SNR, b, corr) {\r\n  \r\n  \r\n  Sigma <- matrix(corr, p, p)\r\n  diag(Sigma) <- 1\r\n  beta <- rep(b, p)\r\n  names(beta) <- paste0(\"x\", 1:p)\r\n  b0 <- 1\r\n  sigma_error <-  sqrt(as.numeric(crossprod(beta, Sigma %*% beta) / SNR))\r\n\r\n  rsq <- NULL\r\n  coefs <- tvals <- matrix(NA, nrow = reps, ncol = p)\r\n  cover <- matrix(0, nrow = reps, ncol = p)\r\n  colnames(coefs) <- paste0(\"x\", 1:p)\r\n  colnames(cover) <- paste0(\"x\", 1:p)\r\n  colnames(tvals) <- paste0(\"x\", 1:p)\r\n\r\nfor (i in seq(reps)) {\r\n  \r\n  X <-  MASS::mvrnorm(n = n, rep(0, p) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, beta) + rnorm(n, 0, sigma_error))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(paste0(\"x\", 1:p), \"y\")\r\n  fit <- lm(y ~., data = Xy)\r\n  sel <- step(fit, k = 2, trace = FALSE)\r\n  s <- summary(sel)\r\n  tval <- s$coefficients[,3][-1]\r\n  tvals[i, names(tval)] <-  tval\r\n  coefs[i, names(tval)] <- coef(sel)[-1]\r\n  rsq[i] <- s$r.squared\r\n  cis <- confint(sel)[-1,]\r\n  if (length(cis) < 3) {\r\n                cover[i,names(tval)] <- ifelse(cis[1] < beta[names(tval)] & cis[2] > beta[names(tval)], 1, 0)\r\n            } else {\r\n                cover[i,names(tval)] <- ifelse(cis[names(tval),1] < beta[names(tval)] & cis[names(tval),2] > beta[names(tval)], 1, 0)\r\n            }\r\n  \r\n}\r\n\r\nres <- data.frame(\r\n  SNR = SNR,\r\n  N = n,\r\n  Predictor = paste0(\"x\", 1:p),\r\n  Estimate = colMeans(coefs, na.rm = TRUE),\r\n  Coverage = colMeans(cover),\r\n  Bias = colMeans((coefs - beta), na.rm = TRUE),\r\n  MSE = colMeans((coefs - beta)^2, na.rm = TRUE))\r\nrownames(res) <- NULL\r\nres\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsim_snr <- purrr::pmap_dfr(list(reps = 1000, p = 2, n = 250, SNR = c(.01, .1, .5, 1, 2), 1, corr = 0.5), sim_bias)\r\n\r\nsim_snr |> \r\n  knitr::kable()\r\n\r\n\r\nSNR\r\nN\r\nPredictor\r\nEstimate\r\nCoverage\r\nBias\r\nMSE\r\n0.01\r\n250\r\nx1\r\n2.2337202\r\n0.316\r\n1.2337202\r\n2.8044133\r\n0.01\r\n250\r\nx2\r\n2.2541444\r\n0.272\r\n1.2541444\r\n2.9413627\r\n0.10\r\n250\r\nx1\r\n1.1246794\r\n0.819\r\n0.1246794\r\n0.1243159\r\n0.10\r\n250\r\nx2\r\n1.1203016\r\n0.813\r\n0.1203016\r\n0.1372949\r\n0.50\r\n250\r\nx1\r\n0.9951877\r\n0.950\r\n-0.0048123\r\n0.0327321\r\n0.50\r\n250\r\nx2\r\n1.0053222\r\n0.947\r\n0.0053222\r\n0.0327637\r\n1.00\r\n250\r\nx1\r\n1.0008476\r\n0.945\r\n0.0008476\r\n0.0162497\r\n1.00\r\n250\r\nx2\r\n1.0025989\r\n0.965\r\n0.0025989\r\n0.0152379\r\n2.00\r\n250\r\nx1\r\n1.0004005\r\n0.944\r\n0.0004005\r\n0.0085356\r\n2.00\r\n250\r\nx2\r\n0.9970394\r\n0.948\r\n-0.0029606\r\n0.0082757\r\n\r\nAs expected, with greater amount of noise in relation to signal, the\r\nselected model includes the true coefficient at smaller frequencies.\r\nBecause model selection interferes with the true model composition,\r\ncoefficient estimates deviate from its true value. In this particular\r\nsetting, with \\(SNR\\) \\(\\ge\\) 0.5, coverage frequency approximates\r\n95% and the estimates get closer to their true value. Measures of bias\r\nand MSE are also useful to display how our uncertainty gets smaller with\r\nless variability in the data.\r\nSample size\r\nOnce we know (7) it is not difficult to suppose that\r\nlarger samples produce smaller standard errors for the regression\r\ncoefficients. We can confirm that using our sim_bias\r\nfunction again, but this time varying sample sizes.\r\n\r\n\r\nShow code\r\n\r\nsim_ss <- purrr::pmap_dfr(list(reps = 1000, p = 2, n = c(10, 50, 100, 200, 500), SNR = 0.5, b = 1, corr = 0.5), sim_bias)\r\nsim_ss|> \r\n  knitr::kable()\r\n\r\n\r\nSNR\r\nN\r\nPredictor\r\nEstimate\r\nCoverage\r\nBias\r\nMSE\r\n0.5\r\n10\r\nx1\r\n1.7978691\r\n0.409\r\n0.7978691\r\n1.6278924\r\n0.5\r\n10\r\nx2\r\n1.7599737\r\n0.413\r\n0.7599737\r\n1.6841848\r\n0.5\r\n50\r\nx1\r\n1.1501254\r\n0.792\r\n0.1501254\r\n0.1525905\r\n0.5\r\n50\r\nx2\r\n1.1407967\r\n0.801\r\n0.1407967\r\n0.1590938\r\n0.5\r\n100\r\nx1\r\n1.0346244\r\n0.926\r\n0.0346244\r\n0.0809554\r\n0.5\r\n100\r\nx2\r\n1.0071139\r\n0.929\r\n0.0071139\r\n0.0781265\r\n0.5\r\n200\r\nx1\r\n0.9959944\r\n0.944\r\n-0.0040056\r\n0.0413917\r\n0.5\r\n200\r\nx2\r\n1.0040690\r\n0.937\r\n0.0040690\r\n0.0426485\r\n0.5\r\n500\r\nx1\r\n1.0017850\r\n0.943\r\n0.0017850\r\n0.0164211\r\n0.5\r\n500\r\nx2\r\n1.0012394\r\n0.965\r\n0.0012394\r\n0.0156134\r\n\r\nCandidate predictors\r\nOK, so far we’ve seen that with 2 predictors, each with true\r\ncoefficient values of 1 and a correlation of 0.5, we get more precise\r\nestimates when \\(SNR \\ge\\) 0.5 and\r\n\\(n \\ge\\) 100. The number of candidate\r\npredictor variables and its covariance matrix are two important aspects\r\nnot addressed yet. To demonstrate their influence we can run simulations\r\nwith varying values for each, where each case will have a different\r\ncombination of number of predictors and correlation between them.\r\nFor simplicity, all the off-diagonal elements the correlation matrix\r\nof predictors will be equal, meaning the that every predictor in the\r\nfull model is correlated with the others by the same degree. We’ll need\r\nto do a slight modification on our previous function so we can then\r\napply a new function for each combination and build a dataframe\r\nsummarizing our results.\r\n\r\n\r\nShow code\r\n\r\nsim_bias_multi <- function(reps, p, n, SNR, b, corr) {\r\n  \r\n  \r\n  Sigma <- matrix(corr, p, p)\r\n  diag(Sigma) <- 1\r\n  beta <- rep(b, p)\r\n  names(beta) <- paste0(\"x\", 1:p)\r\n  b0 <- 1\r\n  sigma_error <-  sqrt(as.numeric(crossprod(beta, Sigma %*% beta) / SNR))\r\n  \r\n  rsq <- NULL\r\n  coefs <- tvals <- matrix(NA, nrow = reps, ncol = p)\r\n  cover <- matrix(0, nrow = reps, ncol = p)\r\n  colnames(coefs) <- paste0(\"x\", 1:p)\r\n  colnames(cover) <- paste0(\"x\", 1:p)\r\n  colnames(tvals) <- paste0(\"x\", 1:p)\r\n  \r\n  for (i in seq(reps)) {\r\n    \r\n    X <-  MASS::mvrnorm(n = n, rep(0, p) , Sigma)\r\n    y <- as.numeric(cbind(1, X) %*% c(b0, beta) + rnorm(n, 0, sigma_error))\r\n    Xy <- as.data.frame( cbind(X, y))\r\n    colnames(Xy) <- c(paste0(\"x\", 1:p), \"y\")\r\n    fit <- lm(y ~., data = Xy)\r\n    sel <- step(fit, k = 2, trace = FALSE)\r\n    s <- summary(sel)\r\n    tval <- s$coefficients[,3][-1]\r\n    tvals[i, names(tval)] <-  tval\r\n    coefs[i, names(tval)] <- coef(sel)[-1]\r\n    rsq[i] <- s$r.squared\r\n    cis <- confint(sel)[-1,]\r\n    if (length(cis) < 3) {\r\n      cover[i,names(tval)] <- ifelse(cis[1] < beta[names(tval)] & \r\n                                       cis[2] > beta[names(tval)], 1, 0)\r\n    } else {\r\n      cover[i,names(tval)] <- ifelse(cis[names(tval),1] < beta[names(tval)] & \r\n                                       cis[names(tval),2] > beta[names(tval)], \r\n                                     1, 0)\r\n    }\r\n    \r\n  }\r\n  \r\n  res <- list(coefs = coefs, tvals = tvals, cover = cover, \r\n              bias = coefs - beta, mse = (coefs - beta)^2, rsq = rsq, \r\n              corr = corr, p = p)\r\n  \r\n  res\r\n\r\n}\r\n\r\nsim_summary <- function(l) {\r\n  \r\n  df <- tibble::tibble(\r\n    \r\n    cor = l$corr,\r\n    npred = l$p,\r\n    predictor = colnames(l$cover),\r\n    coverage = colMeans(l$cover),\r\n    estimate = colMeans(l$coefs, na.rm = TRUE),\r\n    bias = colMeans((l$coefs - 1), na.rm = TRUE),\r\n    mse = colMeans((l$coefs - 1)^2, na.rm = TRUE),\r\n    rsq = mean(l$rsq)\r\n    \r\n  )\r\n  df\r\n  \r\n}\r\n\r\n\r\n\r\nTo keep the computing load at reasonable levels, we’ll limit the\r\nlength of vectors of \\(p\\) and \\(\\rho\\) to 10 values each. Our grid for\r\n\\(\\rho\\) ranges from 0.1 to 0.9 and\r\nwe’ll increase \\(p\\) from 2 to 10.\r\nWe’ll set \\(SNR\\) = 0.5, \\(n\\) = 100 and set all true coefficient\r\nvalues to 1. Again, we’re using AIC to select the best model, and\r\nperforming 1,000 simulation replicates.\r\n\r\n\r\nShow code\r\n\r\nfuture::plan(future::multisession)\r\nsims <- furrr::future_map(seq(2, 10), \r\n                          ~furrr::future_pmap(\r\n                            list(\r\n                              reps = 1000, p = .x, n = 100, SNR = 0.5, 1, \r\n                              corr = seq(0.1, 0.9, by = 0.1)),\r\n                                sim_bias_multi))\r\n\r\nsimdf <- purrr::map_dfr(sims, sim_summary)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsimdfplot |> \r\n  dplyr::mutate(dplyr::across(c(\"coverage\", \"bias\", \"mse\", \"rsq\"), scale)) |> \r\n  tidyr::pivot_longer(cols = c(coverage, bias, mse, rsq), names_to = \"measure\", values_to = 'value') |> \r\n  dplyr::mutate(measure = dplyr::recode(measure, \"coverage\" = \"Coverage\", \"bias\" = \"Bias\", \"mse\" = \"MSE\", \"rsq\" = \"R^2\")) |>\r\n  ggplot() +\r\n  aes(x = cor, y = npred, fill = value) +\r\n  geom_tile() +\r\n  scale_fill_viridis_c(direction = -1, option = \"inferno\", alpha = .9) +\r\n  scale_y_continuous(breaks = c(2, 4, 6, 8, 10)) +\r\n  labs(x = latex2exp::TeX(\"$\\\\rho$\"), y = \"p\", fill = \"SD\") +\r\n  theme_minimal(12) +\r\n  facet_wrap(~measure, scales = \"free\", \r\n             labeller = label_parsed)\r\n\r\n\r\n\r\n\r\nFigure 3: Standardized measures of Bias, Coverage, Mean Square\r\nError (MSE) and R2, across each simulation of pairs of number of\r\npredictors and correlation values\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nreactable(simdf, pageSizeOptions = c(5, 10, 15), defaultPageSize = 10, fullWidth = TRUE,\r\n          columns = list(\r\n    npred = colDef(name = \"p\", filterable = TRUE),\r\n    cor = colDef(name = \"Correlation\", filterable = TRUE),\r\n    predictor = colDef(name = \"Predictor\"),\r\n    coverage = colDef(name = \"Coverage\"),\r\n    estimate = colDef(name = \"Estimate\", format = colFormat(digits = 3)),\r\n    bias = colDef(name = \"Bias\", format = colFormat(digits = 3)),\r\n    mse = colDef(name = \"MSE\", format = colFormat(digits = 3)),\r\n    rsq = colDef(name = \"R2\", format = colFormat(digits = 3))\r\n  ))\r\n\r\n\r\n\r\n\r\nTo keep all four plots in the same scale, I have standardized the\r\nhorizontal axis values. In this heat map, darker colors represent larger\r\nvalues (greater standard deviation). The pattern for Bias and MSE is\r\nsimilar: performing model selection with a great number of highly\r\ncorrelated predictors is likely to produced highly biased estimates. As\r\nexpected, Coverage follows an inverted pattern from Bias and MSE: highly\r\nbiased estimates fall out of the 95% CI coverage.\r\nThese plots make clear that our best case scenario is to have few,\r\nweakly correlated predictors. Of course, this is a setting rarely seen\r\nin empirical research – whats the point of going through model selection\r\nif you have only a handful of variables? What should really get our\r\nattention here is that bias in estimates is likely to get bigger if\r\nmodel selection is naively performed with little prior information about\r\nthe data generating system to filter out candidate models. We have seen\r\nthat sample size, SNR, number of predictors, and correlation between\r\nparameters with themselves and with the response variable, are\r\ncharacteristics of the model that each in their own way contributes to\r\nproducing biased estimates. I encourage you to use this companion shinyapp to\r\nplay with different scenarios to practice what we’ve been discussing so\r\nfar.\r\nVisualizing distributions\r\nWe’ve discussed earlier how dropping a predictor from the model can\r\ndistort the coefficient sampling distribution of the remaining ones when\r\nthey’re not orthogonal. However, even when the preferred (or “correct”)\r\nmodel is selected, there is no guarantee about obtaining sound\r\nregression coefficient estimates. In this final example, we’ll show that\r\nif by chance the “correct” model is achieved by model selection, the\r\nsampling distributions of the resulting regression coefficients might be\r\ndifferent whether we condition on arriving at the correct model or on\r\nthe correct model being known in advance.\r\nConsider this example from Berk et al. (2010). As with the other examples, we’ll\r\nimplement forward stepwise regression using the AIC as a fit criterion.\r\nThe full regression model takes the form of\r\n\\[\\begin{equation}\r\ny_i = \\beta_0 + \\beta_1w_i + \\beta_2x_i + \\beta_3z_i + \\varepsilon_i\r\n\\tag{9}\r\n\\end{equation}\\]\r\nwhere \\(\\beta_0\\) = 3.0, \\(\\beta_1\\) = 0.0, \\(\\beta_2\\) = 1.0, and \\(\\beta_3\\) = 2.0. The variances and\r\ncovariance are set as: \\(\\sigma^2_\\varepsilon\\) = 10.0, \\(\\sigma^2_w\\) = 5.0, \\(\\sigma^2_x\\) = 6.0, \\(\\sigma^2_z\\) = 7.0, \\(\\sigma_{w,x}\\) = 4.0, \\(\\sigma_{w,z}\\) = 5.0, and \\(\\sigma_{x,z}\\) = 5.0. The sample size is\r\n200.\r\nNote that Berk et al. (2010) uses the term preferred\r\nmodel instead of correct model. They do so because a model that\r\nexcludes \\(W\\) can also be called\r\ncorrect the same as one like (9), as long as \\(\\beta_1\\) = 0 is allowed. To be consistent\r\nwith the original text, we’ll use preferred to refer to the\r\nmodel with \\(W\\) excluded. This model\r\nis preferred because it generates the same conditional expectations for\r\nthe response using up one less degree of freedom. The plots show the\r\nregression estimates t-values. “A distribution of t-values is\r\nmore informative than a distribution of regression coefficients because\r\nit takes the regression coefficients and their standard errors into\r\naccount” (Berk\r\net al., 2010, p. 266).\r\n\r\n\r\nreps = 1000\r\np <- 3\r\nSigma <- matrix(c(5,4,5,\r\n                  4,6,5, \r\n                  5,5,7), p, p)\r\nn = 200\r\nbetas <- c(3, 0, 1, 2)\r\n\r\n# values with model selection\r\nrsq <- NULL\r\ncoefs <- cover <- matrix(NA, nrow = reps, ncol = 3)\r\ncolnames(coefs) <- c(\"w\", \"x\", \"z\")\r\ncolnames(cover) <- c(\"w\", \"x\", \"z\")\r\n\r\nfor (i in seq(reps)) {\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 3) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% betas + rnorm(n, 0, 10))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"w\", \"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x + z, data = Xy)\r\n  sel <- step(fit, k = 2, trace = FALSE)\r\n  s <- summary(sel)\r\n  tvals <- s$coefficients[,3][-1]\r\n  coefs[i, names(tvals)] <-  tvals\r\n  rsq[i] <- s$r.squared\r\n}\r\n\r\n# values without model selection\r\nrsq_pref <- NULL\r\ncoefs_pref <- cover_pref <- matrix(NA, nrow = reps, ncol = 3)\r\ncolnames(coefs_pref) <- c(\"w\", \"x\", \"z\")\r\ncolnames(cover_pref) <- c(\"w\", \"x\", \"z\")\r\n\r\nfor (i in seq(reps)) {\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 3) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% betas + rnorm(n, 0, 10))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"w\", \"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x + z, data = Xy)\r\n  s <- summary(fit)\r\n  tvals <- s$coefficients[,3][-1]\r\n  coefs_pref[i, names(tvals)] <-  tvals\r\n  rsq_pref[i] <- s$r.squared\r\n}\r\n\r\n\r\n\r\n\r\n\r\nfull_model <- tibble::as_tibble(coefs_pref)\r\npref_selected <- tibble::as_tibble(coefs[!is.na(coefs[,\"x\"]  & coefs[,\"z\"]),])\r\nx_included <- tibble::as_tibble(coefs[!is.na(coefs[,\"x\"]),])\r\nz_included <- tibble::as_tibble(coefs[!is.na(coefs[,\"z\"]),])\r\n\r\nres_df <- dplyr::bind_rows(\"full\" = full_model, \"pref\" = pref_selected, \r\n                          \"x_included\" = x_included, \"z_included\" = z_included, .id=\"sim\") \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxplot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"x_included\")) |> \r\n  ggplot(aes(x, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.4) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"top\") +\r\n  scale_y_continuous(limits = c(0, 0.55)) +\r\n  labs(x = \"t-values for Regressor X\", y = \"Density\") +\r\n  scale_color_manual(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"predictor being included in a model\"),\r\n                      values = c(\"red3\", \"dodgerblue3\")) +\r\n  scale_fill_manual(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"predictor being included in a model\"),\r\n                      values = c(\"red3\", \"dodgerblue3\"))\r\n\r\nzplot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"z_included\")) |> \r\n  ggplot(aes(z, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.4) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"none\",\r\n        axis.title.y = element_blank(),\r\n        axis.text.y = element_blank()) +\r\n  scale_y_continuous(limits = c(0, 0.55)) +\r\n  labs(x = \"t-values for Regressor Z\", y = \"Density\") +\r\n  scale_color_manual(values = c(\"red3\", \"dodgerblue3\")) +\r\n  scale_fill_manual(values = c(\"red3\", \"dodgerblue3\")) +\r\n  guides(fill=\"none\", color = \"none\")\r\n\r\n\r\nxplot + zplot + plot_layout(guides = 'collect') &\r\n  theme(legend.position='top')\r\n\r\n\r\n\r\n\r\nFigure 4: Stepwise regression sampling distributions of the\r\nregression coefficient t-values for regressors X and Z. Red\r\ndensity plot is is conditional on the preferred model being known. The\r\nblue density plot is conditional on the regressor being included in a\r\nmodel\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxpref_plot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"pref\")) |> \r\n  ggplot(aes(x, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.4) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"top\") +\r\n  scale_y_continuous(limits = c(0, 0.55)) +\r\n  labs(x = \"t-values for Regressor X\", y = \"Density\") +\r\n  scale_color_manual(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"preferred model being selected\"),\r\n                      values = c(\"red3\", \"dodgerblue3\")) +\r\n  scale_fill_manual(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"preferred model being selected\"),\r\n                      values = c(\"red3\", \"dodgerblue3\"))\r\n\r\nzpref_plot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"pref\")) |> \r\n  ggplot(aes(z, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.4) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"none\",\r\n        axis.title.y = element_blank(),\r\n        axis.text.y = element_blank()) +\r\n  scale_y_continuous(limits = c(0, 0.55)) +\r\n  scale_color_manual(values = c(\"red3\", \"dodgerblue3\")) +\r\n  scale_fill_manual(values = c(\"red3\", \"dodgerblue3\")) +\r\n  labs(x = \"t-values for Regressor Z\") +\r\n  guides(fill=\"none\", color = \"none\")\r\n\r\nxpref_plot + zpref_plot + plot_layout(guides = 'collect') &\r\n  theme(legend.position='top')\r\n\r\n\r\n\r\n\r\nFigure 5: Stepwise regression sampling distributions of the\r\nregression coefficient t-values for regressors X and Z. Red\r\ndensity plot is is conditional on the preferred model being known. The\r\nblue density plot is conditional on the preferred model being selected\r\n\r\n\r\n\r\nIn both figures 4 and 5 the red\r\ndensity plot represents the regression estimates t-values\r\ndistribution when no model selection is performed. In 4\r\nthe blue density plot show the distributions when either \\(X\\) or \\(Z\\) are included in the model. For\r\n5 the blue distribution refers to distributions of\r\neither \\(X\\) or \\(Z\\) t-values when the preferred\r\nmodel is selected.\r\nThe contrast between red and blue curves are apparent. The difference\r\nthat is the most striking are in t-values distributions\r\npost-model-selection for regressor \\(Z\\) when conditioned on the regressor being\r\nincluded in a model. This curve displays a bimodal distribution and\r\nhighly biased mean and standard deviaton, as summarized below.\r\n\r\n\r\nShow code\r\n\r\nres_df |> \r\n  dplyr::group_by(sim) |> \r\n  dplyr::summarise(Mx = mean(x, na.rm = TRUE), \r\n            x_sd = sd(x, na.rm = TRUE),\r\n            Mz = mean(z, na.rm = TRUE),\r\n            z_sd = sd(z, na.rm = TRUE)) |> \r\n  dplyr::mutate(sim = dplyr::case_when(\r\n    sim == \"full\" ~ \"Full model\",\r\n    sim == \"pref\" ~ \"Preferred model\",\r\n    sim == \"x_included\" ~ \"X included\",\r\n    sim == \"z_included\" ~ \"Z included\"\r\n  )) |> \r\n  knitr::kable(col.names = c(\"Model\",\r\n                             \"$M_x$\",\r\n                              \"$\\\\sigma_x$\",\r\n                             \"$M_z$\",\r\n                              \"$\\\\sigma_z$\"),\r\n               digits = 3)\r\n\r\n\r\nModel\r\n\\(M_x\\)\r\n\\(\\sigma_x\\)\r\n\\(M_z\\)\r\n\\(\\sigma_z\\)\r\nFull model\r\n2.208\r\n0.991\r\n4.709\r\n1.017\r\nPreferred model\r\n2.559\r\n0.762\r\n4.455\r\n0.940\r\nX included\r\n2.569\r\n0.807\r\n4.455\r\n0.940\r\nZ included\r\n2.559\r\n0.762\r\n5.645\r\n2.556\r\n\r\nIt is especially telling observing those plots that the assumed\r\nunderlying distribution can be very different from what is obtained.\r\nStatistical inference performed in such scenarios would be misleading.\r\nFigure 5 confirms that conditioning on arriving at the\r\npreferred model does not guarantee trustable estimates.\r\nConclusion\r\nModel selection methods are routine in research on the social and\r\nbehavioral sciences, and commonly taught in applied statistics courses\r\nand textbooks. However, little is mentioned about the biased estimates\r\nobtained when such procedures are carried. With simulated data we have\r\nidentified specific characteristics of the data generating model that\r\ncan potentially increase the bias in estimates obtained through variable\r\nselection. Thus, we can conclude that post-model-selection sampling\r\ndistribution can deviate greatly from the assumed underlying\r\ndistribution, even when the best representative model of the data\r\ngeneration process has been selected.\r\nAcknowledgments\r\nSpecial thanks to Donald\r\nWilliams for mentoring me in this project and Rafael Bastos for review and\r\nfeedback.\r\n\r\n\r\n\r\nBerk, R. (2010). What you can and can’t properly do with regression.\r\nJournal of Quantitative Criminology, 26(4), 481–487.\r\nhttps://doi.org/10.1007/s10940-010-9116-4\r\n\r\n\r\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid\r\npost-selection inference. The Annals of Statistics,\r\n41(2), 802–837. https://doi.org/10.1214/12-AOS1077\r\n\r\n\r\nBerk, R., Brown, L., & Zhao, L. (2010). Statistical Inference\r\nAfter Model Selection. Journal of Quantitative\r\nCriminology, 26(2), 217–236. https://doi.org/10.1007/s10940-009-9077-7\r\n\r\n\r\nCohen, J., & Cohen, P. (1983). Applied Multiple\r\nRegression/Correlation Analysis for the\r\nBehavioral Sciences (2nd ed.).\r\n\r\n\r\nFlora, D. B. (2017). Statistical methods for the social and\r\nbehavioural sciences: A model-based approach. SAGE Publications. https://books.google.com.br/books?id=4mcCDgAAQBAJ\r\n\r\n\r\nLukacs, P. M., Burnham, K. P., & Anderson, D. R. (2009). Model\r\nselection bias and Freedmanâ€™s paradox. Annals of the\r\nInstitute of Statistical Mathematics, 62(1), 117. https://doi.org/10.1007/s10463-009-0234-4\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-20-model-selection-bias/model-selection-bias_files/figure-html5/predinc-1.png",
    "last_modified": "2022-07-21T13:04:49-07:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2021-11-04-simulating-regression-artifact/",
    "title": "Regression Artifact - Re-running Farmus et. al (2019)",
    "description": "In this post I'll how false positives can emerge if a model not fit to the research question is chosen. I also provide code for a simple Monte Carlo simulation of the frequency of these false positive results.",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2021-11-04",
    "categories": [
      "simulation",
      "regression"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nLord’s Paradox\r\nand the Regression Artifact\r\nRe-running Farmus et. al\r\n(2019)\r\nConclusion\r\n\r\nMotivation\r\nRecently I came across a publication (Farmus et al., 2019) where the authors\r\noutline a statistical artifact that may arise when researchers add\r\ncontinuous predictors correlated with baseline scores as covariates in\r\nregression models of pretest-posttest analysis. This effect is known\r\nwidely as Lord’s\r\nParadox.\r\nFirst I’ll briefly cover what is the regression artifact discussed in\r\nthe first part of the paper. My main goal, however, is to rerun the\r\nsimulation from part two of their study. This was motivated by the\r\nunavailability of the source code as well as an exercise on building a\r\nsimple simulation study. Replicating simulation studies is a good\r\npractice not only to spot potential errors or biases in the results but\r\nalso to learn and rehearse open research practices (Lohmann et al., 2021).\r\nThe code is pretty straightforward and there is nothing too fancy\r\nabout the methods, but I hope you may learn something from this post\r\nthat you can apply in your own studies. Having to think about how to\r\ntranslate words and equations into code also made me pause and think\r\nwith more depth on what I was reading.\r\nI’ll be using R for the simulations. For pedagogical reasons I have\r\nopted to work with base R, limiting the use of external packages only to\r\nfacilitate summarizing results and building plots. I’m saying that\r\nbecause there is the great faux package\r\ndeveloped by Lisa\r\nDeBruine for simulating factorial designs that could save us many\r\nlines of code.\r\nLord’s Paradox and\r\nthe Regression Artifact\r\nSay that we want to test a treatment in two preexisting groups, A and\r\nB. Given some prior knowledge about the nature of our hypothetical\r\ndependent variable, we have strong basis to suppose that a random\r\nsubject from group B is higher on average on DV scores than a random\r\nsubject from group A. Because those groups occur naturally it isn’t\r\npossible to perform random assignment of subjects to groups.\r\nWe need to specify how we’re going to analyse the data. Given our\r\nresearch design and question there are two potential statistical models\r\nto be chosen from (Farmus et al.,\r\n2019):\r\nA change score model (t test)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + pre_i + e_i\r\n\\tag{1}\r\n\\end{equation}\\]\r\nA regression based model (ANCOVA)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + \\beta_2pre_i + e_i\r\n\\tag{2}\r\n\\end{equation}\\]\r\nHere \\(pre_i\\) and \\(post_i\\) are the pretest and posttest\r\nscores of subject \\(i\\). In equation\r\n(1), \\(\\beta_1\\) is the\r\nmain effect of group (or group mean differences) on gain scores and in\r\nequation (2), it is the partial regression coefficient of\r\n\\(group\\) adjusted for pretest scores.\r\nThe model intercept is \\(\\beta_0\\) and\r\nthe model residuals is represented by \\(e_i\\).\r\nIt should be noted that using \\(post_i\\) or the change score (\\(post_i - pre_i\\)) as the outcome variable\r\nyields the same results if \\(pre_i\\) is\r\na covariate in the model (Senn, 2021).\r\nThough in practice we could choose any of these two models, we may\r\nreach different conclusions depending on which one we use. In fact, the\r\nchoice of the model relies on the questions we’re asking.\r\nThe ANCOVA model is preferred when we do not expect baseline\r\ndifferences between groups. This model estimates the difference in post\r\ntreatment scores controlling for baseline variations (Walker, 2021). Using pretest scores as\r\ncovariate when groups are known to be different at baseline is a poor\r\nchoice because we do not expect the initial differences to disappear\r\nwhen scores are averaged over the group (Pearl, 2016). As we’ll see shortly, this\r\nis exactly why the paradox emerges.\r\nThe change score model do not assume that groups are similar at\r\nbaseline. It tests if groups differ on average on how much they change\r\non the outcome variable from pretest to posttest. The effect of\r\ntreatment is the difference of pre to post differences between the two\r\ngroups (Walker, 2021).\r\nWhat Lord (1967) observed is\r\nthat given a treatment that produces no effect on the\r\noutcome, a researcher using the ANCOVA model would conclude that,\r\nassuming equal baseline scores, one subgroup appears to gain\r\nsignificantly more than the other. A second researcher analyzing the\r\nsame dataset with the change score model would not reject the null\r\nhypothesis that \\(\\beta_1\\) = 0.\r\nThough we have used a categorical predictor \\(group_i\\), the emergence of the paradox is\r\nextended to continuous variables. To give rise to the artifact, there\r\nneeds only to exists some correlation between this predictor and the\r\npretest scores as well as random error in the assessment of change (Eriksson & Häggström, 2014). It can\r\nhappen if, for example, participants are allocated into groups based on\r\nscores on some continuous predictor that happens to be associated with\r\nthe parameter under investigation (Wright, 2006).\r\nIn most research settings (e.g. psychology), measurement is made with\r\nsome degree of error. In the persence of random error of measurement,\r\nthis variance is incorporated in the pre and posttest scores. If we let\r\n\\(U_i\\) represent the “true score”\r\n(i.e. what we would get if there were no measurement error) of subject\r\n\\(i\\), we have:\r\n\\[\\begin{equation}\r\npre_i = U_i^{pre} + e_i^{pre}\r\n\\tag{3}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\npost_i = U_i^{post} + e_i^{post}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nThen, because the expected effect size of treatment is zero, \\(U_i^{post} - U_i^{pre}\\) reflects only the\r\ndifference between random errors (Eriksson & Häggström, 2014).\r\nTo keep consistent with the terminology used in Eriksson & Häggström (2014), we use\r\n\\(P_i\\) to denote the group variable\r\nfor the \\(i^{th}\\) subject, where \\(P_i = 0\\) for Group A and \\(P_i = 1\\) for Group B (keep in mind that\r\nthere is no restriction on \\(P\\) begin\r\nsolely categorical). Because the “true” pretest scores is estimated from\r\nthe grouping variable, we have:\r\n\\[\\begin{equation}\r\nU_i = a + bP_i + \\epsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nwhere \\(a\\) is the model intercept,\r\n\\(b\\) the regression coefficient of\r\n\\(P_i\\) on \\(U_i\\) and \\(\\epsilon_i\\) is the between-individual\r\nvariation in \\(U\\).\r\nLets assume the error term in the measurement of pretest and posttest\r\nscores is drawn from a normal distribution with mean 0 and standard\r\ndeviation, \\(\\sigma\\). Likewise, the\r\nerror term for the regression of property \\(P\\) on true score \\(U_i\\) is drawn from a normal distribution\r\nwith mean 0 and standard deviation \\(s\\).\r\nFor a sample of 200 subjects we set, \\(a\\) = 0.5, \\(b\\) = 0.4, \\(s\\) = 0.1, \\(\\sigma\\) = 0.1.\r\n\r\n\r\nset.seed(7895) # set seed for reproducibility\r\n\r\na <- 0.5 # model intercept\r\nb <- 0.4 # regression coefficient\r\nsigma_error <- .1 # pre and posttest error term standard deviation\r\nvar_error <-  .1  # P error term standard deviation\r\nn <- 200 # sample size\r\n\r\ngroup <- rep(c(1,0), each=n/2) # group predictor\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error) # true score\r\npost <- U + rnorm(n, 0, sigma_error) # posttest scores\r\npre <- U + rnorm(n, 0, sigma_error)  # pretest scorers\r\nchange <- post-pre                   # change scores\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n\r\n\r\n\r\n(ref:sim-plot-1) Scatterplot for the average gain from pre to post\r\ntreatment. Blue line is the slope for group A and the red line is the\r\nslope for group B. The black dashed line is the slope for average gain\r\ntaking both groups together\r\n\r\n\r\nShow code\r\n\r\npre_post <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = post, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = post, color=group),method='lm', \r\n              se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  stat_ellipse(aes(x= pre, y = post, color=group),size = 1) +\r\n  labs(x = \"Pretest scores\",\r\n       y = \"Posttest scores\") +\r\n  scale_color_manual(name = \"Group\", labels = c(\"A\", \"B\"), \r\n                     values = c(\"#1E88E5\", \"#D81B60\")) +\r\n  theme_minimal(12) +\r\n  geom_smooth(aes(x= pre, y = post), method='lm', se = FALSE, color = \"#000000\",\r\n              linetype = \"dashed\", formula= y~x, fullrange=TRUE) +\r\n  xlim(0, NA) + \r\n  ylim(0, NA)\r\n\r\npre_change <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = change, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = change, color=group),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  geom_smooth(aes(x= pre, y = change),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  theme_minimal(12)\r\n\r\npre_post\r\n\r\n\r\n\r\n\r\nFigure 1: (ref:sim-plot-1)\r\n\r\n\r\n\r\nSo, what do we see? The black dashed line crossing both elipses is\r\nclose to 45° which means that, taking both groups as a whole, looks like\r\nthere is no evidence of average differences between groups. In this\r\nparticular example the slopes for both groups behave as we should often\r\nexpect when there is no change in scores between two time points. It is\r\nvisually clear that both are very similar.\r\nNotice that when subjects from groups A and B fall on the same range\r\nof pretest scores we see more red dots on the upper half of the plot,\r\nsuggesting higher posttest scores from subjects in group B. As noted by\r\nEriksson &\r\nHäggström (2014) what is happening is simply an\r\neffect of regression to the mean. Because when subjects from group B\r\n(higher true score) score low on pretest, they are more likely to do\r\nbetter at posttest. A similar logic applies to high scorers from group A\r\nscoring poorer on the followup.\r\nA mediational perspective\r\nIt can also help to visualize the difference between the two models\r\nusing diagrams. As shown by Pearl (2016), we can represent Lord’s paradox\r\nas a mediation model,\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"img/dag2.jpg\")\r\n\r\n\r\n\r\n\r\nFigure 2: Linear version of the model showing pretest scores\r\n(Pre) as a mediator between Group (G) and posttest scores (Post).\r\nAdapted from Pearl (2016)\r\n\r\n\r\n\r\nwhere G is the group variable, Pre\r\nthe pretest scores, Post the posttest scores and\r\nY stands for the difference Post - Pre. What\r\ndifferentiates the ANCOVA and the change score perspectives lies on\r\nwhich effect we wish to estimate. Assuming no confounding, the total\r\neffect is estimated as\r\n\\[\\begin{equation}\r\nTE = (b + ac) - a\r\n\\tag{6}\r\n\\end{equation}\\]\r\nSetting \\(b = a(1-c)\\) all paths\r\ncancel out each other and we obtain the total effect of 0 observed in\r\nour made up experiment.\r\nIf we opt for adjusting for pretest scores, the path \\(ac\\) is blocked and only the direct\r\neffect is estimated, which is,\r\n\\[\\begin{equation}\r\nDE = b\r\n\\tag{7}\r\n\\end{equation}\\]\r\nBecause the direct effect is positive, it becomes clear why the\r\neffect for the ANCOVA model is non-zero.\r\nThe regression artifact\r\nThe issue raised in Farmus et al. (2019) is that Lord’s Paradox isn’t\r\nrestricted to categorical predictors. A similar effect can emerge if a\r\ncontinuous predictor of change is correlated with pretest scores and an\r\nANCOVA model is used. Through a literature review, the authors confirm\r\nthat studies meeting these conditions are not uncommon among papers\r\npublished in top psychology journals.\r\nFor simplicity I’ll not bother to show the mathematical derivation of\r\nthe regression artifact here but the interested reader can refer to\r\nEriksson &\r\nHäggström (2014) or Kim (2018). What we need to have in mind is\r\nthat the regression artifact is the coefficient \\(\\beta_1\\) from equation (2).\r\nUsing Eriksson &\r\nHäggström (2014) notation, the estimate for the\r\nregression artifact is expressed as:\r\n\\[\\begin{equation}\r\n\\hat{K} = \\frac{b\\sigma^2}{(s^2+\\sigma^2)}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nIf we input the parameters set to build figure 1\r\nand calculate the value of \\(K\\),\r\nassuming infinite samples, \\(K\\) =\r\n0.2.\r\nSurprisingly, re-running Eriksson & Häggström (2014)\r\nsimulation of the regression artifact, we get a different estimate for\r\nits standard deviation.\r\n\r\n\r\nShow code\r\n\r\nset.seed(7895)\r\n\r\nK <- c()\r\nreps <- 10000\r\n\r\nfor (i in seq(reps)) {\r\na <- 0.5\r\nb <- 0.4\r\nsigma_error <- .1\r\nvar_error <-  .1\r\nn <- 105\r\n\r\ngroup <- sample(c(0, 1), n, replace = TRUE)\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error)\r\npost <- U + rnorm(n, 0, sigma_error)\r\npre <- U + rnorm(n, 0, sigma_error)\r\nchange <- post-pre\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n  s <- summary(lm(change ~ pre + group, data = sim.df))\r\n  K[i] <- s$coefficients[3,1]\r\n}\r\n\r\nk_mean <- mean(K)\r\nk_sd <- sd(K)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntibble::enframe(K) |> \r\n  ggplot(aes(x = value)) + \r\n  geom_histogram(color = \"black\", fill = \"lightblue1\") +\r\n  labs(x = latex2exp::TeX(\"Coefficient for $\\\\hat{K}$\"),\r\n       y = \"Frequency\") +\r\n  theme_minimal(12)\r\n\r\n\r\n\r\n\r\nFigure 3: Distribution of the coefficients for predictor \\(P\\) on the change score\r\n\r\n\r\n\r\nAs expected, the mean value for \\(\\hat{K}\\) is 0.2. Our estimate for the\r\nstandard deviation of \\(\\hat{K}\\) is\r\n0.04, close to half the value Eriksson & Häggström (2014) reported.\r\nIn their study, they have found 98% of \\(\\hat{K}\\) values greater than zero. In\r\n10,000 simulated datasets, we found the frequency of \\(\\hat{K}\\) > 0 to be 100%1.\r\nRe-running Farmus et. al\r\n(2019)\r\nWe now return to Farmus et al. (2019) study. Including the baseline\r\nscores as covariate in a pretest to posttest change model may lead us to\r\nfalsely conclude a significant association if a non-negligible\r\ncorrelation exists between the predictor and the covariate. The authors\r\nuse Monte Carlo simulations to estimate Type I error rate in scenarios\r\nlike the one described, varying the size of \\(b\\) (equation (5)), and the\r\nsample size.\r\nOther parameters were fixed such that the standard deviation of \\(e_i^{pre}\\), \\(e_i^{post}\\), and \\(\\epsilon_i\\) were set to 1. The variance of\r\n\\(P\\) (\\(\\sigma^2_P\\)) was also set to 1. Both \\(\\beta_0\\) and \\(a\\) were set to 0. Coefficient \\(b\\) ranged from -1 to 1 by 0.5 steps and\r\nsample sizes of 20, 50, 100, and 1,000 were used. To keep every\r\ncondition exact as the original study, 5,000 simulations were run for\r\neach pair and the statistical significance level set at 0.05.\r\nThe simulation will only estimate the Type I error rates because the\r\nsize of parameters like the regression artifact (\\(K\\)) and the correlation between the\r\npretest scores and true score (\\(\\rho(pre,P)\\)) can be estimated from the\r\nparameters values fixed previously.\r\nLooking at equations (3) and (5) we know that,\r\n\\[\\begin{equation}\r\npre_i = a + bP_i + \\epsilon_i + e_i^{pre}\r\n\\tag{9}\r\n\\end{equation}\\]\r\nthen calculating \\(\\rho_{(pre,P)}\\)\r\nbecomes straightforward\r\n\\[\\begin{align}\r\n\\begin{split}\r\n  \\rho_{pre,P} & = \\frac{\\mathrm{Cov}(pre,\r\nP)}{\\sigma_{pre}\\sigma_{P}}\\\\\r\n  & = \\frac{b\\sigma^2_P}{(\\sqrt{b^2\\sigma^2_P + s^2 + \\sigma^2}) +\r\n\\sigma^2_P}\r\n\\end{split}\r\n  \\tag{10}\r\n\\end{align}\\]\r\nOur first step is to create a function that takes as arguments the\r\nnumber of runs, sample size and the population value of coefficient\r\n\\(b\\). We can store the\r\np-value of each analysis and count afterwards how many were\r\nless than the the alpha level.\r\n\r\n\r\nsim.reg.artifact <- function(n.sims, sample.size, beta) {\r\n\r\n  reps <- n.sims\r\n  n <- sample.size\r\n  beta <- beta\r\n  b0 <- 0\r\n  P.sd <- sigma_error <- var_error <-  1\r\n  pval <- NULL\r\n  r <- (beta*(P.sd)^2)/(sqrt((beta^2)*((P.sd)^2) + (sigma_error^2) + (var_error^2)))\r\n  K <- (beta*sigma_error)/(sigma_error + var_error) # the regression artifact\r\n\r\n  for (i in seq(1, reps)) {\r\n\r\n    P = rnorm(n = n, mean = 0, sd = 1) # Property P from equation 5\r\n    U <- b0 + beta*P + rnorm(n, 0, var_error) # The \"true score\" from equation 5\r\n\r\n    pre <- U + rnorm(n, 0, sigma_error) # Pretest scores - equation 3\r\n    post <-  U + rnorm(n, 0, sigma_error) # Posttest scores - equation 4\r\n\r\n    Xy <- as.data.frame(cbind(P, pre, post)) # A dataframe with outcome and predictors\r\n    colnames(Xy) <- c(\"P\", \"pre\", \"post\")\r\n    \r\n    fit <- lm(post ~ ., data = Xy)\r\n    s <- summary(fit)\r\n    pval[i] <- ifelse(s$coefficients[2,4] < 0.05, 1, 0) # p-value for the coefficient of P\r\n\r\n  }\r\n  \r\n  # store the simulated results in a data frame for display\r\n  res <- data.frame(\r\n    sample_size = n,\r\n    bx = beta,\r\n    correlation = r,\r\n    artifact = K,\r\n    error_rate = mean(pval)\r\n  )\r\n\r\n  return(res)\r\n}\r\n\r\n\r\n\r\nNesting for loops can easily become confusing, so I have opted to use\r\nthe purrr::map_dfr() function, which I find to be a simpler\r\nand more elegant solution.\r\n\r\n\r\nsamplesize <- list(20, 50, 100, 1000)\r\ncoefs <- as.list(seq(-1,1, by = 0.5))\r\n\r\n\r\nfinaltable <- purrr::map_dfr(samplesize,\r\n                             ~purrr::map2(.x, coefs,\r\n                                          ~sim.reg.artifact(5000, .x, .y)))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfinaltable |> \r\n  tidyr::pivot_wider(names_from = sample_size, values_from = error_rate) |>\r\n  kbl(\r\n    col.names = c(\"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"N = 20\",\r\n                  \"N = 50\", \"N = 100\", \"N = 1,000\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) |> \r\n  add_header_above(c(\" \" = 3, \"Type I error rate\" = 4 ))\r\n\r\n\r\n\r\n\r\n\r\n\r\nType I error rate\r\n\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nN = 20\r\n\r\n\r\nN = 50\r\n\r\n\r\nN = 100\r\n\r\n\r\nN = 1,000\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.577\r\n\r\n\r\n-0.50\r\n\r\n\r\n0.267\r\n\r\n\r\n0.612\r\n\r\n\r\n0.900\r\n\r\n\r\n1.000\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.333\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.131\r\n\r\n\r\n0.258\r\n\r\n\r\n0.471\r\n\r\n\r\n1.000\r\n\r\n\r\n0.0\r\n\r\n\r\n0.000\r\n\r\n\r\n0.00\r\n\r\n\r\n0.051\r\n\r\n\r\n0.051\r\n\r\n\r\n0.048\r\n\r\n\r\n0.052\r\n\r\n\r\n0.5\r\n\r\n\r\n0.333\r\n\r\n\r\n0.25\r\n\r\n\r\n0.120\r\n\r\n\r\n0.259\r\n\r\n\r\n0.475\r\n\r\n\r\n1.000\r\n\r\n\r\n1.0\r\n\r\n\r\n0.577\r\n\r\n\r\n0.50\r\n\r\n\r\n0.263\r\n\r\n\r\n0.618\r\n\r\n\r\n0.895\r\n\r\n\r\n1.000\r\n\r\n\r\nGreat! Our results match pretty closely those reported by Farmus et al. (2019). Notice\r\nthat in a world where \\(\\rho_{(pre,P)}\\) is zero, in the long run\r\nthe Type I error rate stay within range of what was expected.\r\nTo wrap up, we can use the function just created to also reproduce\r\nthe simulations Farmus et\r\nal. (2019) ran\r\nusing median parameter values discovered in their literature review of\r\nexperimental psychology studies.\r\n\r\n\r\nShow code\r\n\r\nsim2 <- sim.reg.artifact(n.sims = 5000, sample.size = 239, beta = .4)\r\n\r\nsim2 |> \r\n  kbl(\r\n    col.names = c(\"Sample size\", \"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"Type I error rate\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\n\r\n\r\nSample size\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nType I error rate\r\n\r\n\r\n239\r\n\r\n\r\n0.4\r\n\r\n\r\n0.272\r\n\r\n\r\n0.2\r\n\r\n\r\n0.674\r\n\r\n\r\nAgain we have a match of results with the original study. What makes\r\nthose results alarming is the Type I error rate of 0.67! As the authors\r\nemphasize, common conditions in psychological research may elicit\r\nunacceptably high error rates which researchers may not be aware of if\r\nan inappropriate model is employed.\r\nConclusion\r\nWe have covered a definition of Lord’s Paradox and argued that\r\nchoosing a model that does not fit the research question can lead to\r\nbiased conclusions. As Farmus et al. (2019) outline, correlation between the\r\npredictor and the outcome and some amount of measurement error are\r\nnecessary conditions for the emergence of a regression artifact. It is\r\nimportant that researchers take this into consideration before drawing\r\nconclusions about the data.\r\nFollowing recent calls to reproduce simulation studies (Lohmann et al., 2021) we have worked to\r\nreproduce simulations performed by two papers discussing Lord’s Paradox\r\nwith continuous predictors. Monte Carlo simulations are not needed to\r\ndemonstrate the regression artifact and no major recommendations are\r\nbeing proposed by the original studies based on the simulation results.\r\nEven so, a minor discrepancy from Eriksson & Häggström (2014) was\r\nfound, highlighting that computer code is not error free. Reproducing\r\nsimulation analysis proved to be fruitful not only in finding divergent\r\nresults but also as a coding exercise.\r\nLord’s Paradox have been and continues to be widely debated in the\r\nquantitative methods literature. Many of the papers referenced in the\r\ntext broaden the discussion investigating this topic in different and\r\nmore complex scenarios. Hopefully this post has reached its instructive\r\npurpose of teaching a few things about simulating data in R. If you have\r\nany comments or questions, feel free to drop a comment below or reach me\r\nvia email or twitter.\r\n\r\n\r\n\r\nEriksson, K., & Häggström, O. (2014). Lord’s Paradox in\r\na Continuous Setting and a Regression Artifact\r\nin Numerical Cognition Research. PLOS ONE,\r\n9(4), e95949. https://doi.org/f2z5hc\r\n\r\n\r\nFarmus, L., Arpin-Cribbie, C. A., & Cribbie, R. A. (2019).\r\nContinuous predictors of pretest-posttest change: Highlighting the\r\nimpact of the regression artifact. Frontiers in Applied Mathematics\r\nand Statistics, 4, 64. https://doi.org/gk92gx\r\n\r\n\r\nKim, S. B. (2018). Explaining lord’s paradox in introductory statistical\r\ntheory courses. International Journal of Statistics and\r\nProbability, 7(4), 1. https://doi.org/10.5539/ijsp.v7n4p1\r\n\r\n\r\nLohmann, A., Astivia, O. L. O., Morris, T., & Groenwold, R. H. H.\r\n(2021). It’s time! 10 + 1 reasons we should start replicating\r\nsimulation studies. https://doi.org/10.31234/osf.io/agsnt\r\n\r\n\r\nLord, F. M. (1967). A paradox in the interpretation of group\r\ncomparisons. Psychological Bulletin, 68(5), 304–305.\r\nhttps://doi.org/10.1037/h0025105\r\n\r\n\r\nPearl, J. (2016). Lord’s Paradox Revisited \r\n(Oh Lord! Kumbaya!). Journal of Causal Inference,\r\n4(2). https://doi.org/10.1515/jci-2016-0021\r\n\r\n\r\nSenn, S. (2021). Cause for concern. In LinkedIn. LinkedIn. https://www.linkedin.com/pulse/cause-concern-stephen-senn/\r\n\r\n\r\nWalker, J. A. (2021). Models for longitudinal experiments – pre-post\r\ndesigns. In Elements of statistical modeling for experimental\r\nbiology. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html\r\n\r\n\r\nWright, D. B. (2006). Comparing groups in a before-after design: When t\r\ntest and ANCOVA produce different results. British Journal of\r\nEducational Psychology, 76(3), 663–675. https://doi.org/10.1348/000709905x52210\r\n\r\n\r\nI’m not excluding the possibility\r\nthat it may have been me that got it wrong. If this is the case and you\r\nidentify where I screwed up, please let me know!↩︎\r\n",
    "preview": "posts/2021-11-04-simulating-regression-artifact/simulating-regression-artifact_files/figure-html5/sim-plot-1-1.png",
    "last_modified": "2022-04-03T13:34:15-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
