[
  {
    "path": "posts/2021-12-20-model-selection-bias/",
    "title": "Simulating post selection inference",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2021-12-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWhat is model selection\r\nWhy is it a problem?\r\nIllustration\r\n\r\nOther issues\r\nNoise\r\nSample size\r\nPredictors\r\n\r\nVisualizing\r\ndistributions\r\nConclusion\r\n\r\nWhat is model selection\r\nMuch of the current research questions on the behavioral and social\r\nsciences are investigated using statistical models (Flora,\r\n2017). Models are simplified translations of a reality to\r\nmathematical expressions; it’s aim is express how the data were\r\ngenerated. Regression based models are vastly employed on empirical\r\nresearch and often used to estimate causal effects (Berk, 2010). However, to perform causal\r\nmodeling, researchers must specify a “correct” model (i.e. an accurate\r\nmodel of the data generating process) prior to data collection and use\r\nthe obtained data only to estimate regression coefficients (see Berk (2010) for a further discussion). In\r\npractice, researchers usually have only a vague idea of the right model\r\nto answer their research questions or even if such model can be\r\nestimated. Often, what is framed as statistical inference or causal\r\nmodeling is in fact a descriptive analysis; what Berk (2010) name as Level I Regression\r\nAnalysis.\r\nTo determine which variables should be included in the model, a\r\ncommon solution is to resort to variable selection algorithms. The\r\ndrawback is that whenever data driven variable selection procedures are\r\nemployed, classical inference guarantees are invalidated due to the\r\nmodel itself becoming stochastic. It means\r\nthat if the model selection method evaluates the stochastic component of\r\nthe data, the model is also considered stochastic (Berk et al., 2013).\r\nWhy is it a problem?\r\nVariable selection procedures aren’t in themselves problematic. But\r\nwhen the correct model is unknown prior to data analysis and the\r\nsame dataset is used for variable selection, parameter\r\nestimation and statistical inferences, the estimated results can be\r\nhighly biased. We add a new source of uncertainty when performing model\r\nselection. These procedures discards parameter estimates from the model\r\nand, as we shall see, the sampling distribution of the remaining\r\nregression parameters estimates gets distorted. Additionally, the model\r\nselected isn’t the same across samples, so there is another source of\r\nuncertainty to the estimates. (Berk\r\net al., 2010).\r\nConsider a well defined population with it’s unknown regression\r\nparameter values. We draw a random sample and apply a procedure of model\r\nselection. The “best” model found via variable selection is sample\r\nspecific and isn’t guarantee to be the correct model (if we assume that\r\none such model in fact exists). Suppose we repeat the process of drawing\r\na random sample and performing model selection countless times. In this\r\nexample there are six possible candidate models and only one correct. As\r\nshown in the following table, even if the correct model (in this case,\r\n\\(\\hat{M}_2\\)) is three times more\r\nlikely to be selected than the competing models, it will be chosen more\r\nfrequently but not at the majority of the time.\r\n\r\n\r\nShow code\r\n\r\nmodels <- paste0(\"M\", 1:6)\r\n\r\ntibble::enframe(\r\n  table(\r\n    sample(\r\n      models, 10000, replace = TRUE, \r\n      prob = c(1/9,1/3,1/9,1/9,1/9,1/9)))/10000,\r\n  name = \"Model\", value = \"Frequency\") |> \r\n  knitr::kable()\r\n\r\n\r\nModel\r\nFrequency\r\nM1\r\n0.1182\r\nM2\r\n0.3790\r\nM3\r\n0.1280\r\nM4\r\n0.1217\r\nM5\r\n0.1239\r\nM6\r\n0.1292\r\n\r\nTo understand why the regression parameters estimates might be\r\nbiased, recall that in a multiple regression we estimate partial\r\nregression coefficients: in a regression equation, the weight of\r\nindependent variables are estimated in relation to the\r\nother independent variables in the model (Cohen & Cohen, 1983). For a\r\ndependent variable, \\(Y\\), predicted by\r\nvariables \\(X_1\\) and \\(X_2\\), \\(B_{Y1\r\n\\cdot 2}\\) is the partial regression coefficient for \\(Y\\) on \\(X_1\\) holding \\(X_2\\) constant, and \\(B_{Y2 \\cdot 1}\\) is the partial regression\r\ncoefficient for \\(Y\\) on \\(X_2\\) holding \\(X_1\\) constant. This regression equation is\r\nwritten as:\r\n\\[\\begin{equation}\r\n\\hat{Y} = B_{Y0 \\cdot 12} + B_{Y1 \\cdot 2}X_1 + B_{Y2 \\cdot 1}X_2 +\r\n\\varepsilon\r\n\\tag{1}\r\n\\end{equation}\\]\r\nwhere \\(B_{Y0 \\cdot 12}\\) is the\r\nmodel intercept when \\(X_1\\) and \\(X_2\\) are held constant and \\(\\varepsilon\\) is the error term.\r\nThe regression coefficient for \\(X_i\\) (\\(i = \\{1,\r\n2\\}\\)) is model dependent. To see why, let’s take a look at the\r\nequations for the regression coefficients for \\(X_1\\) and \\(X_2\\)\r\n\\[\\begin{equation}\r\nB_{Y1 \\cdot 2} = \\frac{\\rho_{Y1} - \\rho_{Y2}\\rho_{12}}{(1 -\r\n\\rho_{12}^2)} \\times \\frac{\\sigma_Y}{\\sigma_1}\r\n\\tag{2}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\nB_{Y2 \\cdot 1} = \\frac{\\rho_{Y2} - \\rho_{Y1}\\rho_{21}}{(1 -\r\n\\rho_{21}^2)} \\times \\frac{\\sigma_Y}{\\sigma_2}\r\n\\tag{3}\r\n\\end{equation}\\]\r\nwhere \\(\\rho\\) stands for the\r\npopulational correlation coefficient and \\(\\sigma\\) for the populational standard\r\ndeviation. So, unless we have uncorrelated predictors (i.e. \\(\\rho_{12}\\) = 0 and/or \\(\\rho_{21}\\) = 0), the value for any of the\r\nregression coefficients is determined by which other predictors are in\r\nthe model. If either one is excluded from the model, a different\r\nregression coefficient will be estimated: excluding \\(X_2\\), for example, would zero all the\r\ncorrelations involving this predictor, leaving,\r\n\\[\\begin{equation}\r\nB_{Y1 \\cdot 2} = \\frac{\\rho_{Y1} - 0 \\times 0}{(1 - 0^2)} \\times\r\n\\frac{\\sigma_Y}{\\sigma_1} = \\rho_{Y1} \\times \\frac{\\sigma_Y}{\\sigma_1} =\r\nB_{Y1}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nBerk et al. (2010) warns that the sampling\r\ndistribution of the estimated regression parameters is distorted because\r\nestimates made from incorrect models will also be included, resulting in\r\na mixture of distributions. Therefore the model selection process must\r\nbe taken into account in the regression estimation whenever it is\r\napplied.\r\nIllustration\r\nTo illustrate what we’ve just seen, let’s expand an analytic example\r\ngiven by Berk et al. (2010) with simulations. Consider a model\r\nfor a response variable \\(y\\) with two\r\npotential regressors, \\(x\\) and \\(z\\). Say we’re interest in the relationship\r\nbetween \\(y\\) and \\(x\\) while holding \\(z\\) constant, that is, \\(\\hat{\\beta}_{yx\\cdot z}\\). Framing this as\r\na linear regression model we have\r\n\\[\\begin{equation}\r\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\varepsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nNow, suppose that we’re in a scenario, where \\(\\rho_{xz}\\) = 0.5, both \\(\\beta_1\\) and \\(\\beta_2\\) are set to 1 and \\(\\varepsilon \\sim N(0, 10)\\). We’ll use a\r\nsample size of 250 subjects and 1000 random samples will be drawn from\r\nthis population. We’ll calculate coverage and bias for each regressor.\r\nCoverage corresponds to the frequency in which the true\r\ncoefficient value is captured by the 95% CI of the estimate. The\r\nbias of the estimations is calculated as \\(\\frac{1}{R}\\sum(\\hat{\\theta_r}-\\theta)\\),\r\nwhere \\(R\\) is the number of\r\nrepetitions, \\(\\theta\\) represents a\r\npopulation parameter, and \\(\\hat{\\theta}_r\\) a sample estimate in each\r\nsimulation.\r\n\r\n\r\nShow code\r\n\r\np <- 2 # number of predictors\r\nSigma <- matrix(.5, p, p) # correlation matrix\r\ndiag(Sigma) <- 1\r\nn = 250 # sample size\r\nb0 <- 10 # intercept (can be set to any value)\r\nbetas <- rep(1, 2) \r\nreps = 1000\r\ncoefs <- cover <- matrix(0, nrow = reps, ncol = 2) # defining the matrices to store simulation results\r\n\r\n\r\nfor (i in seq(reps)) {\r\n  # X is a matrix of regression coefficients\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 2) , Sigma)\r\n  # with the values randomly drawn in X, we'll estimate values for y\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, betas) + rnorm(n, 0, sqrt(10)))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"x\", \"z\"), \"y\")\r\n  # fit a linear model with x and z to predict y\r\n  fit <- lm(y ~ ., data = Xy)\r\n  coefs[i, ] <- coef(fit)[-1] # save the regression coefficients\r\n  cis <- confint(fit)[-1,] # save the 95% CIs\r\n  # if the true value is capture by the CI, sum 1, 0 otherwise \r\n  cover[i,] <- ifelse(cis[,1] < 1 & cis[,2] > 1, 1, 0)\r\n}\r\ncolnames(coefs) <- c(\"x\", \"z\")\r\ncoefs <- as.data.frame(coefs)\r\n\r\ntibble::tibble(\r\n  Predictor = c(\"x\", \"z\"),\r\n  Coverage = colMeans(cover),\r\n  Bias = colMeans(coefs - betas)\r\n) |> \r\n  knitr::kable()\r\n\r\n\r\nPredictor\r\nCoverage\r\nBias\r\nx\r\n0.951\r\n-0.0006048\r\nz\r\n0.930\r\n0.0028854\r\n\r\n\r\n\r\nShow code\r\n\r\nggplot(data = coefs, aes(x = x)) +\r\n  geom_histogram(color = \"black\", fill = \"white\", bins = 30) +\r\n  theme_classic() +\r\n  theme(axis.title.y=element_blank(),\r\n        axis.text.y=element_blank(), \r\n        axis.ticks.y=element_blank(),\r\n        axis.line.y = element_blank()) +\r\n  xlab( expression(paste(\"Values of \", beta[yx.z])))\r\n\r\n\r\n\r\n\r\nIn this first scenario \\(\\beta_{yx\\cdot\r\nz}\\) is estimated assuming \\(x\\)\r\nand \\(z\\) are always included in the\r\nmodel.\r\nWhat would happen if by model selection we arrive at a model where\r\n\\(z\\) is excluded? As indicated in\r\nequation (4), if \\(z\\)\r\nis excluded, any correlation where \\(z\\) is involved is equivalent to zero, and\r\nwe’re left with,\r\n\\[\\begin{equation}\r\n\\beta_{yx} = \\rho_{xy}(\\frac{\\sigma_y}{\\sigma_x})\r\n\\tag{6}\r\n\\end{equation}\\]\r\nNote that \\(\\beta_{yx}\\) is not the\r\nsame as \\(\\beta_{yx\\cdot z}\\). If we do\r\nnot have a model specified prior to data collection and analysis, it is\r\nnot clear which definition of regression parameter for \\(x\\) we’re trying to estimate, if \\(\\beta_{yx\\cdot z}\\), as exemplified on\r\nequation (2), or if \\(\\beta_{yx}\\). Therefore, the definition of\r\n\\(\\hat{\\beta}_1\\) depends on the model\r\nin which it is placed.\r\n\r\n\r\nShow code\r\n\r\np <- 2\r\nSigma <- matrix(.5, p, p)\r\ndiag(Sigma) <- 1\r\nn = 250\r\nb0 <- 10\r\nbetas <- rep(1, 2)\r\nreps = 1000\r\ncoefs <- cover <- matrix(NA, nrow = reps, ncol = 2)\r\nfor (i in seq(reps)) {\r\n  #print(i)\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 2) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, betas) + rnorm(n, 0, sqrt(10)))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x, data = Xy)\r\n  coefs[i, ] <- coef(fit)[-1]\r\n  cis <- confint(fit)[-1,]\r\n  cover[i,] <- ifelse(cis[1] < 1 & cis[2] > 1, 1, 0)\r\n}\r\ncolnames(coefs) <- c(\"x\", \"z\")\r\ncoefs <- as.data.frame(coefs)\r\n# coverage\r\ncover1b <- colMeans(cover)[1]\r\n# bias\r\nbias1b <- colMeans((coefs - betas)^2)[1]\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nggplot(data = coefs, aes(x = x)) +\r\n  geom_histogram(color = \"black\", fill = \"white\", bins = 30) +\r\n  theme_classic() +\r\n  theme(axis.title.y=element_blank(),\r\n        axis.text.y=element_blank(), \r\n        axis.ticks.y=element_blank(),\r\n        axis.line.y = element_blank())+\r\n  xlab( expression(paste(\"Values of \", beta[yx])))\r\n\r\n\r\n\r\n\r\nNotice how far off the latter model estimates the coefficient for\r\n\\(x\\). Under these conditions, we\r\nshould expect a coverage of the true coefficient value of \\(x\\) of 0.349 and a bias of 0.293. Under the\r\ncorrect model, bias is negligible and coverage follows the Type I error\r\nrate of 5% that we’ve set for this exercise.\r\nThis simple example help us understand why estimate model parameters\r\nin the same single random sample used to find an appropriate model isn’t\r\na good idea (at least not without taking this process in account prior\r\nto making inferences). Discarding \\(z\\)\r\nfrom our model has distorted the sampling distribution of \\(x\\). Thus, as Berk et al. (2010) puts it: “when a single model is\r\nnot specified before the analysis begins, it is not clear what\r\npopulation parameter is the subject of study. And without this clarity,\r\nthe reasoning behind statistical inference becomes obscure”.\r\nOther issues\r\nWe already know that if two predictors are moderately correlated and\r\none is dropped from the model, our estimation will be biased. But what\r\nelse should we consider? If we are selecting the variable every time,\r\nthen there is not issue. In the following sections we’ll consider what\r\ncan make things “bad”. But before that, please consider the equation for\r\nthe standard error of the regression coefficient estimated in our first\r\nexample that hasn’t gone through model selection,\r\n\\[\\begin{equation}\r\nSE(\\beta_{yx\\cdot z}) = \\frac{\\hat{\\sigma_{\\varepsilon}}}{s_x\r\n\\sqrt{n-1}}\\sqrt{\\frac{1}{1-r^2_{xy}}}\r\n\\tag{7}\r\n\\end{equation}\\]\r\nWe have that \\(\\hat{\\sigma_{\\varepsilon}}\\) is an estimate\r\nof the residual standard deviation, \\(s_x\\) is the sample standard deviation of\r\n\\(x\\), \\(r^2_{xy}\\) is the square of the sample\r\ncorrelation between \\(x\\) and \\(z\\), and \\(n\\) is the sample size. If the standard\r\nerror for a regression coefficient is large, it means that its\r\ndistribution will be more dispersed. So, from this equation we identify\r\nas crucial parameters for wider sampling distribution: larger residual\r\nvariance, less variance in \\(x\\), small\r\nsample size, and, stronger correlation between regressors and the\r\nresponse variable.\r\nWe’ll use simulated data to aid our understanding. I have build a ShinyApp for\r\nthat end. Its purpose is to illustrate the problems that arise when\r\nmodel selection, parameters estimation and statistical inferences are\r\nundertaken with the same data set. Although I will be working with code\r\nin this post, most of it can also be reproduced with this app.\r\nFor convenience, I’ll use as model selection approach stepwise\r\nregression with AIC as model selection criterion. Some penalties are\r\nharsher than others, but the concerns raised here are irrespective of\r\nthe chosen model selection method (Berk\r\net al., 2010). In the app I provide a few other selection\r\nmethods to be chosen from. In addition to bias and coverage, we’ll also\r\nestimate the average Mean Squared Error (MSE), calculated as \\(\\frac{1}{R}\\sum[(\\hat{\\theta_r}-\\theta)^2]\\).\r\nNoise\r\nTo express the variability we can use a signal-to-noise ratio (SNR).\r\nThis term is defined here as:\r\n\\[\\begin{equation}\r\n\\frac{S}{N} = \\textbf{b}\\Sigma\\textbf{b}\\sigma^{-2}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nwhere, \\(\\textbf{b}\\) is a\r\n(p + 1) vector of regression coefficients, \\(\\Sigma\\) is the covariance matrix of\r\npredictors and \\(\\sigma^2\\) is the\r\nerror term variance. Note that “model selection bias also occurs when an\r\nexplanatory variable has a weak relationship with the response variable.\r\nThe relationship is real, but small. Therefore, it is rarely selected as\r\nsignificant” (Lukacs et\r\nal., 2009, p. 118) We can experiment with a range of values\r\nfor the \\(SNR\\) to see how it affects\r\nthe estimates. We’ll use most values set in our first simulation\r\nexercise and \\(SNR\\) values ranging\r\nfrom 0.1 to 2. One thousand simulations will be run for each of those\r\nvalues.\r\n\r\n\r\nsim_bias <- function(reps, p, n, SNR, b, corr) {\r\n  \r\n  \r\n  Sigma <- matrix(corr, p, p)\r\n  diag(Sigma) <- 1\r\n  beta <- rep(b, p)\r\n  names(beta) <- paste0(\"x\", 1:p)\r\n  b0 <- 1\r\n  sigma_error <-  sqrt(as.numeric(crossprod(beta, Sigma %*% beta) / SNR))\r\n\r\n  rsq <- NULL\r\n  coefs <- tvals <- matrix(NA, nrow = reps, ncol = p)\r\n  cover <- matrix(0, nrow = reps, ncol = p)\r\n  colnames(coefs) <- paste0(\"x\", 1:p)\r\n  colnames(cover) <- paste0(\"x\", 1:p)\r\n  colnames(tvals) <- paste0(\"x\", 1:p)\r\n\r\nfor (i in seq(reps)) {\r\n  \r\n  X <-  MASS::mvrnorm(n = n, rep(0, p) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% c(b0, beta) + rnorm(n, 0, sigma_error))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(paste0(\"x\", 1:p), \"y\")\r\n  fit <- lm(y ~., data = Xy)\r\n  sel <- step(fit, k = 2, trace = FALSE)\r\n  s <- summary(sel)\r\n  tval <- s$coefficients[,3][-1]\r\n  tvals[i, names(tval)] <-  tval\r\n  coefs[i, names(tval)] <- coef(sel)[-1]\r\n  rsq[i] <- s$r.squared\r\n  cis <- confint(sel)[-1,]\r\n  if (length(cis) < 3) {\r\n                cover[i,names(tval)] <- ifelse(cis[1] < beta[names(tval)] & cis[2] > beta[names(tval)], 1, 0)\r\n            } else {\r\n                cover[i,names(tval)] <- ifelse(cis[names(tval),1] < beta[names(tval)] & cis[names(tval),2] > beta[names(tval)], 1, 0)\r\n            }\r\n  \r\n}\r\n\r\nres <- data.frame(\r\n  SNR = SNR,\r\n  N = n,\r\n  Predictor = paste0(\"x\", 1:p),\r\n  Estimate = colMeans(coefs, na.rm = TRUE),\r\n  Coverage = colMeans(cover),\r\n  Bias = colMeans((coefs - beta), na.rm = TRUE),\r\n  MSE = colMeans((coefs - beta)^2, na.rm = TRUE))\r\nrownames(res) <- NULL\r\nres\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsim_snr <- purrr::pmap_dfr(list(reps = 1000, p = 2, n = 250, SNR = c(.01, .1, .5, 1, 2), 1, corr = 0.5), sim_bias)\r\n\r\nsim_snr |> \r\n  knitr::kable()\r\n\r\n\r\nSNR\r\nN\r\nPredictor\r\nEstimate\r\nCoverage\r\nBias\r\nMSE\r\n0.01\r\n250\r\nx1\r\n2.3283247\r\n0.277\r\n1.3283247\r\n2.9851612\r\n0.01\r\n250\r\nx2\r\n2.2525649\r\n0.317\r\n1.2525649\r\n2.8880256\r\n0.10\r\n250\r\nx1\r\n1.1352750\r\n0.796\r\n0.1352750\r\n0.1474245\r\n0.10\r\n250\r\nx2\r\n1.1205934\r\n0.797\r\n0.1205934\r\n0.1323206\r\n0.50\r\n250\r\nx1\r\n1.0003849\r\n0.951\r\n0.0003849\r\n0.0339769\r\n0.50\r\n250\r\nx2\r\n1.0019099\r\n0.945\r\n0.0019099\r\n0.0336724\r\n1.00\r\n250\r\nx1\r\n1.0061681\r\n0.953\r\n0.0061681\r\n0.0159229\r\n1.00\r\n250\r\nx2\r\n0.9929049\r\n0.955\r\n-0.0070951\r\n0.0159254\r\n2.00\r\n250\r\nx1\r\n1.0041400\r\n0.941\r\n0.0041400\r\n0.0083921\r\n2.00\r\n250\r\nx2\r\n0.9959335\r\n0.954\r\n-0.0040665\r\n0.0081750\r\n\r\nAs expected, with greater amount of noise in relation to signal, the\r\nselected model includes the true coefficient at smaller frequencies.\r\nBecause model selection interferes with the true model composition,\r\ncoefficient estimates deviate from its true value. In this particular\r\nsetting, with \\(SNR\\) \\(\\ge\\) 0.5, coverage frequency approximates\r\n95% and the estimates get closer to their true value. Measures of bias\r\nand MSE are also useful to display how our uncertainty gets smaller with\r\nless variability in the data.\r\n(lukacs)\r\nSample size\r\nOnce we know (7) it is not difficult to suppose that\r\nlarger samples produce smaller standard errors for the regression\r\ncoefficients. We can confirm that using our sim_bias\r\nfunction again, but this time varying sample sizes.\r\n\r\n\r\nShow code\r\n\r\nsim_ss <- purrr::pmap_dfr(list(reps = 1000, p = 2, n = c(10, 50, 100, 200, 500), SNR = 1, b = 1, corr = 0.5), sim_bias)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsim_ss|> \r\n  knitr::kable()\r\n\r\n\r\nSNR\r\nN\r\nPredictor\r\nEstimate\r\nCoverage\r\nBias\r\nMSE\r\n1\r\n10\r\nx1\r\n1.5212415\r\n0.561\r\n0.5212415\r\n0.7314914\r\n1\r\n10\r\nx2\r\n1.4950486\r\n0.525\r\n0.4950486\r\n0.8116910\r\n1\r\n50\r\nx1\r\n1.0187963\r\n0.926\r\n0.0187963\r\n0.0772569\r\n1\r\n50\r\nx2\r\n1.0317910\r\n0.931\r\n0.0317910\r\n0.0803227\r\n1\r\n100\r\nx1\r\n1.0011917\r\n0.966\r\n0.0011917\r\n0.0370676\r\n1\r\n100\r\nx2\r\n1.0044273\r\n0.964\r\n0.0044273\r\n0.0387603\r\n1\r\n200\r\nx1\r\n0.9989825\r\n0.956\r\n-0.0010175\r\n0.0197056\r\n1\r\n200\r\nx2\r\n1.0001283\r\n0.946\r\n0.0001283\r\n0.0202181\r\n1\r\n500\r\nx1\r\n0.9969380\r\n0.950\r\n-0.0030620\r\n0.0081867\r\n1\r\n500\r\nx2\r\n1.0005004\r\n0.944\r\n0.0005004\r\n0.0084909\r\n\r\nPredictors\r\nOK, so far we’ve seen that with 2 predictors, each with true\r\ncoefficient values of 1 and a correlation of 0.5, we get more precise\r\nestimates when \\(SNR \\ge\\) 0.5 and\r\n\\(n \\ge\\) 100. The number of predictors\r\nand its covariance matrix are two important aspects not addressed yet.\r\nTo demonstrate their influence we can run simulations with varying\r\nvalues for each, where each case will have a different combination of\r\nnumber of predictors and correlation between them.\r\nFor simplicity, all the off-diagonal elements the correlation matrix\r\nof predictors will be equal, meaning the that every predictor in the\r\nfull model is correlated with the others by the same degree. The varying\r\nvalues for number of predictors and correlation is presented as a ratio\r\n\\(\\frac{\\rho}{p}\\), where \\(\\rho\\) is the correlation coefficient and\r\n\\(p\\) the number of predictors.\r\nWe’ll need to do a slight modification on our previous function so we\r\ncan then apply a new function for each combination and build a dataframe\r\nsummarising our results.\r\n\r\n\r\nShow code\r\n\r\nsim_bias_multi <- function(reps, p, n, SNR, b, corr) {\r\n  \r\n  \r\n  Sigma <- matrix(corr, p, p)\r\n  diag(Sigma) <- 1\r\n  beta <- rep(b, p)\r\n  names(beta) <- paste0(\"x\", 1:p)\r\n  b0 <- 1\r\n  sigma_error <-  sqrt(as.numeric(crossprod(beta, Sigma %*% beta) / SNR))\r\n  \r\n  rsq <- NULL\r\n  coefs <- tvals <- matrix(NA, nrow = reps, ncol = p)\r\n  cover <- matrix(0, nrow = reps, ncol = p)\r\n  colnames(coefs) <- paste0(\"x\", 1:p)\r\n  colnames(cover) <- paste0(\"x\", 1:p)\r\n  colnames(tvals) <- paste0(\"x\", 1:p)\r\n  \r\n  for (i in seq(reps)) {\r\n    \r\n    X <-  MASS::mvrnorm(n = n, rep(0, p) , Sigma)\r\n    y <- as.numeric(cbind(1, X) %*% c(b0, beta) + rnorm(n, 0, sigma_error))\r\n    Xy <- as.data.frame( cbind(X, y))\r\n    colnames(Xy) <- c(paste0(\"x\", 1:p), \"y\")\r\n    fit <- lm(y ~., data = Xy)\r\n    sel <- step(fit, k = 2, trace = FALSE)\r\n    s <- summary(sel)\r\n    tval <- s$coefficients[,3][-1]\r\n    tvals[i, names(tval)] <-  tval\r\n    coefs[i, names(tval)] <- coef(sel)[-1]\r\n    rsq[i] <- s$r.squared\r\n    cis <- confint(sel)[-1,]\r\n    if (length(cis) < 3) {\r\n      cover[i,names(tval)] <- ifelse(cis[1] < beta[names(tval)] & cis[2] > beta[names(tval)], 1, 0)\r\n    } else {\r\n      cover[i,names(tval)] <- ifelse(cis[names(tval),1] < beta[names(tval)] & cis[names(tval),2] > beta[names(tval)], 1, 0)\r\n    }\r\n    \r\n  }\r\n  \r\n  res <- list(coefs = coefs, tvals = tvals, cover = cover, bias = coefs - beta, mse = (coefs - beta)^2, rsq = rsq, corr = corr, p = p)\r\n  \r\n  res\r\n\r\n}\r\n\r\nsim_summary <- function(l) {\r\n  \r\n  df <- tibble::tibble(\r\n    \r\n    cor = l$corr,\r\n    npred = l$p,\r\n    predictor = colnames(l$cover),\r\n    ratio = cor/npred,\r\n    coverage = colMeans(l$cover),\r\n    estimate = colMeans(l$coefs, na.rm = TRUE),\r\n    bias = colMeans((l$coefs - 1), na.rm = TRUE),\r\n    mse = colMeans((l$coefs - 1)^2, na.rm = TRUE),\r\n    rsq = mean(l$rsq)\r\n    \r\n  )\r\n  df\r\n  \r\n}\r\n\r\n\r\n\r\nTo keep the computing load at reasonable levels, we’ll limit the\r\nlength of vectors of \\(p\\) and \\(\\rho\\) to 10 values each. Our grid for\r\n\\(\\rho\\) ranges from 0.1 to 0.9 and\r\nwe’ll increase \\(p\\) from 2 to 10.\r\nWe’ll set \\(SNR\\) = 0.5, \\(n\\) = 100 and set all true coefficient\r\nvalues to 1. Again, we’re using AIC to select the best model and\r\nperforming 1,000 simulation replicates.\r\n\r\n\r\nShow code\r\n\r\nfuture::plan(future::multisession)\r\nsims <- furrr::future_map(seq(2, 10), ~furrr::future_pmap(list(reps = 1000, p = .x, n = 100, SNR = 0.5, 1, corr = seq(0.1, 0.9, by = 0.1)),\r\n                                sim_bias_multi))\r\n\r\nsimdf <- purrr::map_dfr(sims, sim_summary)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsimdfplot |> \r\n  dplyr::mutate(dplyr::across(c(\"coverage\", \"bias\", \"mse\", \"rsq\"), scale)) |> \r\n  tidyr::pivot_longer(cols = c(coverage, bias, mse, rsq), names_to = \"measure\", values_to = 'value') |> \r\n  dplyr::mutate(measure = dplyr::recode(measure, \"coverage\" = \"Coverage\", \"bias\" = \"Bias\", \"mse\" = \"MSE\", \"rsq\" = \"R^2\")) |>\r\n  ggplot() +\r\n  aes(x = cor, y = npred, fill = value) +\r\n  geom_tile() +\r\n  scale_fill_viridis_c(direction = -1, option = \"inferno\", alpha = .9) +\r\n  scale_y_continuous(breaks = c(2, 4, 6, 8, 10)) +\r\n  labs(x = NULL, y = NULL, fill = \"SD\") +\r\n  theme_minimal(12) +\r\n  facet_wrap(~measure, scales = \"free\", \r\n             labeller = label_parsed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nreactable(simdf, pageSizeOptions = c(5, 10, 15), defaultPageSize = 10, fullWidth = TRUE,\r\n          columns = list(\r\n    npred = colDef(name = \"p\", filterable = TRUE),\r\n    cor = colDef(name = \"Correlation\", filterable = TRUE),\r\n    ratio = colDef(name = \"Ratio\", filterable = TRUE),\r\n    predictor = colDef(name = \"Predictor\"),\r\n    coverage = colDef(name = \"Coverage\"),\r\n    estimate = colDef(name = \"Estimate\", format = colFormat(digits = 3)),\r\n    bias = colDef(name = \"Bias\", format = colFormat(digits = 3)),\r\n    mse = colDef(name = \"MSE\", format = colFormat(digits = 3)),\r\n    rsq = colDef(name = \"R2\", format = colFormat(digits = 3))\r\n  ))\r\n\r\n\r\n\r\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"cor\":[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,0.1,0.1,0.1,0.2,0.2,0.2,0.3,0.3,0.3,0.4,0.4,0.4,0.5,0.5,0.5,0.6,0.6,0.6,0.7,0.7,0.7,0.8,0.8,0.8,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9],\"npred\":[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],\"predictor\":[\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\"],\"ratio\":[0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.03,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.07,0.07,0.07,0.07,0.07,0.07,0.07,0.07,0.07,0.07,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.09,0.09,0.09,0.09,0.09,0.09,0.09,0.09,0.09,0.09,0.05,0.05,0.1,0.1,0.15,0.15,0.2,0.2,0.25,0.25,0.3,0.3,0.35,0.35,0.4,0.4,0.45,0.45,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.1,0.1,0.1,0.133333333333333,0.133333333333333,0.133333333333333,0.166666666666667,0.166666666666667,0.166666666666667,0.2,0.2,0.2,0.233333333333333,0.233333333333333,0.233333333333333,0.266666666666667,0.266666666666667,0.266666666666667,0.3,0.3,0.3,0.025,0.025,0.025,0.025,0.05,0.05,0.05,0.05,0.075,0.075,0.075,0.075,0.1,0.1,0.1,0.1,0.125,0.125,0.125,0.125,0.15,0.15,0.15,0.15,0.175,0.175,0.175,0.175,0.2,0.2,0.2,0.2,0.225,0.225,0.225,0.225,0.02,0.02,0.02,0.02,0.02,0.04,0.04,0.04,0.04,0.04,0.06,0.06,0.06,0.06,0.06,0.08,0.08,0.08,0.08,0.08,0.1,0.1,0.1,0.1,0.1,0.12,0.12,0.12,0.12,0.12,0.14,0.14,0.14,0.14,0.14,0.16,0.16,0.16,0.16,0.16,0.18,0.18,0.18,0.18,0.18,0.0166666666666667,0.0166666666666667,0.0166666666666667,0.0166666666666667,0.0166666666666667,0.0166666666666667,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.05,0.05,0.05,0.05,0.05,0.05,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0833333333333333,0.0833333333333333,0.0833333333333333,0.0833333333333333,0.0833333333333333,0.0833333333333333,0.1,0.1,0.1,0.1,0.1,0.1,0.116666666666667,0.116666666666667,0.116666666666667,0.116666666666667,0.116666666666667,0.116666666666667,0.133333333333333,0.133333333333333,0.133333333333333,0.133333333333333,0.133333333333333,0.133333333333333,0.15,0.15,0.15,0.15,0.15,0.15,0.0142857142857143,0.0142857142857143,0.0142857142857143,0.0142857142857143,0.0142857142857143,0.0142857142857143,0.0142857142857143,0.0285714285714286,0.0285714285714286,0.0285714285714286,0.0285714285714286,0.0285714285714286,0.0285714285714286,0.0285714285714286,0.0428571428571429,0.0428571428571429,0.0428571428571429,0.0428571428571429,0.0428571428571429,0.0428571428571429,0.0428571428571429,0.0571428571428571,0.0571428571428571,0.0571428571428571,0.0571428571428571,0.0571428571428571,0.0571428571428571,0.0571428571428571,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0857142857142857,0.0857142857142857,0.0857142857142857,0.0857142857142857,0.0857142857142857,0.0857142857142857,0.0857142857142857,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.114285714285714,0.114285714285714,0.114285714285714,0.114285714285714,0.114285714285714,0.114285714285714,0.114285714285714,0.128571428571429,0.128571428571429,0.128571428571429,0.128571428571429,0.128571428571429,0.128571428571429,0.128571428571429,0.0125,0.0125,0.0125,0.0125,0.0125,0.0125,0.0125,0.0125,0.025,0.025,0.025,0.025,0.025,0.025,0.025,0.025,0.0375,0.0375,0.0375,0.0375,0.0375,0.0375,0.0375,0.0375,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.075,0.075,0.075,0.075,0.075,0.075,0.075,0.075,0.0875,0.0875,0.0875,0.0875,0.0875,0.0875,0.0875,0.0875,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1125,0.1125,0.1125,0.1125,0.1125,0.1125,0.1125,0.1125,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0111111111111111,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0222222222222222,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0333333333333333,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0444444444444444,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0555555555555556,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0666666666666667,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0777777777777778,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.0888888888888889,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1],\"coverage\":[0.565,0.534,0.575,0.572,0.559,0.569,0.552,0.546,0.591,0.553,0.444,0.455,0.43,0.457,0.45,0.462,0.463,0.454,0.432,0.429,0.379,0.342,0.384,0.401,0.38,0.357,0.355,0.357,0.349,0.409,0.324,0.285,0.305,0.293,0.307,0.319,0.317,0.322,0.308,0.32,0.252,0.239,0.283,0.248,0.275,0.256,0.258,0.26,0.253,0.26,0.238,0.219,0.21,0.206,0.217,0.203,0.227,0.23,0.224,0.222,0.174,0.191,0.176,0.208,0.178,0.167,0.199,0.189,0.196,0.176,0.156,0.148,0.147,0.159,0.138,0.147,0.135,0.153,0.142,0.161,0.136,0.13,0.122,0.123,0.118,0.112,0.119,0.135,0.138,0.112,0.948,0.956,0.938,0.954,0.95,0.953,0.95,0.943,0.933,0.941,0.916,0.914,0.818,0.819,0.608,0.592,0.216,0.217,0.953,0.952,0.95,0.931,0.939,0.933,0.897,0.899,0.893,0.84,0.851,0.84,0.785,0.77,0.791,0.692,0.687,0.702,0.593,0.572,0.598,0.455,0.453,0.439,0.158,0.153,0.162,0.913,0.911,0.92,0.915,0.855,0.849,0.858,0.856,0.78,0.78,0.753,0.761,0.72,0.687,0.714,0.709,0.597,0.633,0.619,0.652,0.546,0.533,0.524,0.533,0.462,0.456,0.473,0.441,0.385,0.355,0.369,0.335,0.138,0.166,0.15,0.152,0.857,0.849,0.858,0.852,0.84,0.782,0.778,0.741,0.789,0.771,0.673,0.68,0.674,0.681,0.669,0.619,0.576,0.589,0.617,0.544,0.512,0.513,0.536,0.52,0.484,0.466,0.425,0.406,0.464,0.434,0.378,0.35,0.357,0.358,0.37,0.274,0.284,0.302,0.298,0.282,0.139,0.16,0.142,0.158,0.136,0.76,0.777,0.764,0.783,0.762,0.793,0.668,0.653,0.677,0.663,0.665,0.655,0.566,0.604,0.6,0.564,0.582,0.587,0.488,0.512,0.519,0.496,0.518,0.496,0.422,0.427,0.438,0.428,0.443,0.427,0.368,0.359,0.377,0.377,0.37,0.366,0.28,0.295,0.299,0.286,0.302,0.299,0.246,0.215,0.265,0.226,0.25,0.231,0.134,0.133,0.128,0.152,0.151,0.129,0.732,0.706,0.684,0.723,0.72,0.711,0.706,0.628,0.598,0.608,0.585,0.6,0.607,0.606,0.532,0.485,0.554,0.517,0.534,0.517,0.488,0.43,0.439,0.455,0.433,0.44,0.437,0.42,0.345,0.369,0.401,0.384,0.387,0.4,0.375,0.315,0.31,0.296,0.31,0.298,0.321,0.306,0.258,0.243,0.234,0.247,0.266,0.234,0.232,0.176,0.193,0.202,0.197,0.217,0.222,0.212,0.122,0.138,0.157,0.142,0.13,0.138,0.133,0.632,0.656,0.647,0.644,0.667,0.661,0.661,0.666,0.546,0.536,0.515,0.55,0.54,0.542,0.543,0.552,0.443,0.434,0.515,0.442,0.474,0.444,0.474,0.443,0.402,0.371,0.385,0.395,0.364,0.379,0.367,0.397,0.331,0.339,0.321,0.35,0.324,0.339,0.335,0.331,0.298,0.261,0.292,0.289,0.287,0.283,0.268,0.269,0.202,0.197,0.229,0.235,0.201,0.248,0.214,0.256,0.171,0.184,0.174,0.193,0.173,0.187,0.196,0.184,0.125,0.139,0.134,0.119,0.136,0.121,0.14,0.138,0.615,0.627,0.596,0.613,0.589,0.614,0.607,0.616,0.59,0.476,0.487,0.509,0.491,0.466,0.51,0.486,0.506,0.507,0.422,0.392,0.428,0.421,0.385,0.418,0.395,0.42,0.387,0.355,0.365,0.341,0.37,0.375,0.345,0.34,0.347,0.372,0.31,0.295,0.282,0.306,0.298,0.311,0.284,0.27,0.308,0.246,0.248,0.251,0.243,0.256,0.252,0.238,0.26,0.239,0.206,0.168,0.199,0.184,0.191,0.219,0.19,0.188,0.22,0.175,0.156,0.149,0.155,0.174,0.165,0.167,0.156,0.157,0.128,0.155,0.125,0.117,0.13,0.13,0.127,0.147,0.136],\"estimate\":[1.48212455427486,1.50320660781006,1.5270088560898,1.49374880165201,1.53159770593366,1.47138408431054,1.50399057598131,1.4755527287169,1.52601723355963,1.49245028335111,1.81832470153324,1.81459596759042,1.79983761507829,1.84643491519722,1.78529937151446,1.79054965857971,1.8342570462384,1.84261890943984,1.8272593646198,1.81665115781513,2.09061869824378,2.09112700690235,2.17816427657677,2.03686803053617,2.15321288550206,2.05884149763904,2.17103109551452,2.17062963690638,2.1219332704338,2.15493026118875,2.46381365343474,2.52098253777995,2.43572166448287,2.34669802638196,2.3848617740815,2.41047803601028,2.33134668488213,2.48699093571909,2.47422950402301,2.35757793895972,2.77778215150291,2.73159401060911,2.7427057742391,2.71988993250759,2.77109619956861,2.8027655394593,2.86544701588114,2.75218531994526,2.63999613561842,2.6362102085604,2.97392252323139,3.08520189995986,3.03543428690887,3.09514989702632,3.22040808458466,3.01974637213795,2.92425456806308,3.07278285018453,3.00770228168978,3.16595460447848,3.3333260013279,3.57749993257936,3.46679385405356,3.14783473941871,3.36211621436215,3.41624532813105,3.4334562707942,3.39648856423001,3.03720789823589,3.06823316022245,3.59525995639872,3.89257578785785,4.16223894083064,3.72302684341002,3.48254111162686,4.03039590211241,4.08688806468424,3.8822535712929,3.7494221093398,4.14243358497249,4.28972849176243,4.45127093452374,4.3331315629978,3.37439041818781,3.70064150591263,4.22809486058794,4.00984595347498,4.71545694821332,4.27954653525802,4.40101700549167,0.998955979087414,0.995651567127508,1.0155292846412,1.0060263731804,1.00245787850054,1.00352364521834,1.02590521283904,1.000845970185,1.02633933146977,1.02249228844805,1.02923321645913,1.05909700885107,1.09957097761326,1.08838442363864,1.26832385247327,1.24783712068624,1.69034611242944,1.67893308889572,1.00797205031942,1.00207447731957,1.01857119104388,1.0258883029142,1.04291982004656,1.02513926421552,1.05163714416595,1.06999760035871,1.0531020336063,1.10174552103938,1.12573100049686,1.10379893400293,1.18106279549119,1.17942438114313,1.18197624848623,1.29430942133806,1.30346144507843,1.2826162474353,1.46923929480506,1.47144539072214,1.45917571384367,1.67920679602758,1.68651678466982,1.74008653601704,2.29745688007437,2.37559156733446,2.34384180046032,1.03198400078583,1.04841537398914,1.04534531927074,1.04005517476997,1.08655980827515,1.08800378830443,1.11515464340565,1.08551847858937,1.19799473952189,1.18651055185555,1.20581224964332,1.18005672467611,1.28576070996194,1.26480742934755,1.26031522892932,1.29210719932981,1.43242079707903,1.42947994639985,1.41835397495731,1.39767519714205,1.59279061684422,1.60275235113342,1.56573130945463,1.57277148156962,1.74865856587974,1.84086687984678,1.81063473625152,1.79301214381657,2.07400109426012,2.12674869551222,2.15123475118906,2.14545087194402,2.83500530342777,2.77625509537222,2.78720691990953,2.80673376623189,1.09287486105257,1.11121357966494,1.09583968551689,1.11375971019436,1.11224870582257,1.20158818759542,1.20024468092302,1.18000539898067,1.19746781043601,1.17025889094839,1.32213686938317,1.31343386394659,1.32195224569511,1.35557648755012,1.33701106669048,1.46498855439401,1.4851396417869,1.47508001420863,1.46606743325001,1.46394207155754,1.62983134547105,1.64199066426574,1.63081390083353,1.67614022046235,1.67449883974187,1.86483822561698,1.84326313587829,1.83413216680221,1.88315792110983,1.8454875510744,2.10308487312571,2.17278088573762,2.12185228880731,2.16240132496171,2.16396926325517,2.57981568496157,2.39448889962597,2.60439450993895,2.50604286013268,2.47426219810181,3.24935184314678,3.21917889528684,3.18997388872789,3.28940752998317,3.2380604634483,1.17804290440987,1.18527220881449,1.17307692795988,1.16433193027898,1.18531106485852,1.20207183685895,1.34071293341661,1.3134137361608,1.34436032891575,1.3179129672886,1.29583044768833,1.36500866731109,1.49283435467487,1.49397194718387,1.49572468571876,1.49104439214846,1.51637136273188,1.47402120354716,1.69510780614146,1.65769387152724,1.66471446348103,1.66205743235578,1.67476001780899,1.71049675361809,1.90386283430667,1.92829805738064,1.8725289300239,1.94148184221793,1.89720253501024,1.89710508973531,2.15375016366168,2.16112170224363,2.09948765932845,2.0947906346537,2.17102942841632,2.10386337745291,2.50214802774141,2.48372887005636,2.40877386019216,2.4449529126078,2.44815656298357,2.55431987289708,2.89858370995159,2.92819166364052,2.79292123398954,2.8620775228508,2.71751396052019,2.81455501927544,3.38396994911477,3.68164130666388,3.77520919369007,3.46118241141237,3.73563734052136,3.20794760200186,1.23807137851785,1.2682353344487,1.24529818130218,1.25423329113493,1.27024131275053,1.2855283175639,1.25401726424403,1.45287880732018,1.4622004086283,1.45139463577462,1.43160702659893,1.41346809317981,1.43174164256234,1.45311161250955,1.61966178776019,1.67003017614537,1.64217127530595,1.6587686276461,1.67289626303521,1.62272113083396,1.65047645123956,1.92072667841916,1.87664191537492,1.8584263974247,1.87326131958946,1.87139441163752,1.93342760863269,1.86572690225312,2.19582481464415,2.0625529850076,2.05251283347779,2.09666947126394,2.04522949622242,2.06787443869793,2.14063707584643,2.34144328364175,2.48181770904682,2.43518943286509,2.41502408935784,2.38479601648464,2.40897120393144,2.42012872815368,2.73740502162979,2.68135887758383,2.80429417460617,2.81383781718837,2.81447144772185,2.7919859611466,2.67754192238441,2.95799612487178,3.21682813939888,3.10052479368004,2.89170966355565,3.29349095771793,3.0417499939894,3.27201916028459,3.76125988954029,3.4981088448812,3.68713240597488,3.58293395416182,4.25197671376895,3.95092343113325,3.62779763585242,1.35751881288198,1.33840245970606,1.33280845655283,1.36239824327212,1.35815090698548,1.32690459743538,1.32977537938847,1.31623494596466,1.53777587546169,1.59198141576652,1.56043462061466,1.57116359971706,1.58202706547258,1.56771798810806,1.58593480267787,1.59390959351365,1.81745380621153,1.78204855871338,1.76947620217352,1.7365914034002,1.85627192605239,1.85689517093945,1.84213473002408,1.8669111092374,2.02092278254266,2.10563552194197,2.07724757395866,2.08021703150755,2.08756920409867,2.08994968022983,2.06962595822674,2.04819320645007,2.27222750855438,2.32008143423894,2.30834622560021,2.27360909415914,2.37547902803255,2.35092111806614,2.36334540990778,2.33994254958159,2.57981292812756,2.77293780820901,2.5637428294524,2.57445735136478,2.62855488330793,2.67405564331816,2.56904157663771,2.62417268603821,2.98132677249317,2.94917658407456,2.98494852437822,2.83541372966815,3.01731256884464,2.89273499322282,2.89029769596545,2.97974479051442,3.13556681113226,3.3936449550296,3.42970755307329,3.33889515157842,3.24842589672753,3.3640354440791,3.34618496996536,3.45084227365087,4.23854924222766,3.76392042641595,3.72806149108082,4.35164009184971,3.8944474709212,3.68725579805565,3.62339071110078,4.09270777074313,1.44136521432235,1.41608226552714,1.42846285277145,1.4268598713506,1.43334867310854,1.41196602105966,1.40028472249881,1.41956285655713,1.43029375980501,1.7505205058795,1.69736724108544,1.74827134111502,1.70980306350538,1.68189902308393,1.69819319089933,1.68415997151857,1.68629577409108,1.6768331249891,1.91058846912935,2.006745704366,1.95988847820776,1.98065912201925,2.04265932035511,1.95522928185444,1.88535607365754,1.97925701049629,1.95216604193218,2.26395839941933,2.27798400788536,2.15586780444527,2.21388213228252,2.22573039197846,2.21246587443768,2.17142594186249,2.2927348657588,2.27156155058234,2.60940407924593,2.52067429332541,2.46134444619684,2.53007433556577,2.52638151759335,2.40642860631669,2.53084324032477,2.50626470898909,2.58047148245131,2.82489316688177,2.86152411387006,2.98285145128028,2.83638152965567,2.88572693494137,2.81547375376285,2.89601089013983,2.84831409942473,2.71290836890375,3.02238923667426,3.06484920674609,3.38229597348044,3.23635775888632,3.36167278834093,3.08014389791938,3.31106458109456,3.11141406117286,3.1153242438063,3.49882505756341,3.70755977763134,3.91338591011114,3.32048383927023,3.60961718693885,3.65367230278392,3.54832569437659,3.43898177587181,3.71952360596091,4.27571660104566,3.67345171151948,4.1313372097409,4.15862340572599,3.96416863023053,3.62759474264931,4.26698873235809,4.29905675985984,4.34568266938424],\"bias\":[0.482124554274857,0.503206607810065,0.527008856089799,0.493748801652011,0.531597705933658,0.47138408431054,0.503990575981306,0.475552728716897,0.526017233559626,0.49245028335111,0.81832470153324,0.814595967590418,0.799837615078289,0.846434915197221,0.785299371514465,0.790549658579706,0.834257046238403,0.842618909439836,0.827259364619796,0.816651157815131,1.09061869824378,1.09112700690235,1.17816427657677,1.03686803053617,1.15321288550206,1.05884149763904,1.17103109551452,1.17062963690638,1.1219332704338,1.15493026118875,1.46381365343474,1.52098253777995,1.43572166448287,1.34669802638196,1.3848617740815,1.41047803601028,1.33134668488213,1.48699093571909,1.47422950402301,1.35757793895972,1.77778215150291,1.73159401060911,1.7427057742391,1.71988993250759,1.77109619956861,1.8027655394593,1.86544701588114,1.75218531994526,1.63999613561842,1.6362102085604,1.97392252323139,2.08520189995986,2.03543428690887,2.09514989702632,2.22040808458466,2.01974637213795,1.92425456806308,2.07278285018453,2.00770228168978,2.16595460447848,2.3333260013279,2.57749993257936,2.46679385405356,2.14783473941871,2.36211621436215,2.41624532813105,2.4334562707942,2.39648856423001,2.03720789823589,2.06823316022245,2.59525995639872,2.89257578785785,3.16223894083064,2.72302684341002,2.48254111162686,3.03039590211241,3.08688806468424,2.8822535712929,2.7494221093398,3.14243358497249,3.28972849176243,3.45127093452374,3.3331315629978,2.37439041818781,2.70064150591263,3.22809486058794,3.00984595347498,3.71545694821332,3.27954653525802,3.40101700549167,-0.00104402091258643,-0.00434843287249175,0.0155292846411981,0.00602637318039541,0.00245787850054155,0.00352364521834418,0.025905212839045,0.000845970185004565,0.0263393314697708,0.0224922884480482,0.0292332164591318,0.0590970088510686,0.0995709776132558,0.0883844236386376,0.268323852473271,0.247837120686242,0.690346112429441,0.678933088895716,0.00797205031942146,0.00207447731957068,0.0185711910438782,0.0258883029142014,0.0429198200465553,0.0251392642155206,0.0516371441659517,0.0699976003587117,0.0531020336063003,0.101745521039385,0.125731000496859,0.103798934002932,0.181062795491187,0.179424381143126,0.181976248486234,0.294309421338055,0.303461445078431,0.282616247435301,0.469239294805058,0.471445390722142,0.459175713843672,0.679206796027584,0.686516784669818,0.740086536017044,1.29745688007437,1.37559156733446,1.34384180046032,0.0319840007858281,0.0484153739891383,0.0453453192707358,0.0400551747699738,0.0865598082751545,0.0880037883044261,0.115154643405652,0.0855184785893724,0.197994739521893,0.186510551855554,0.205812249643323,0.180056724676113,0.285760709961942,0.264807429347554,0.260315228929317,0.29210719932981,0.432420797079033,0.429479946399855,0.418353974957307,0.39767519714205,0.592790616844216,0.602752351133425,0.565731309454635,0.572771481569624,0.74865856587974,0.840866879846781,0.810634736251517,0.793012143816566,1.07400109426012,1.12674869551222,1.15123475118906,1.14545087194402,1.83500530342777,1.77625509537222,1.78720691990953,1.80673376623189,0.0928748610525693,0.11121357966494,0.0958396855168882,0.113759710194363,0.11224870582257,0.201588187595416,0.200244680923017,0.180005398980669,0.197467810436011,0.170258890948391,0.322136869383174,0.313433863946593,0.321952245695106,0.355576487550121,0.337011066690479,0.464988554394014,0.485139641786897,0.475080014208635,0.466067433250006,0.46394207155754,0.629831345471047,0.641990664265745,0.630813900833525,0.676140220462347,0.674498839741868,0.864838225616984,0.843263135878292,0.834132166802209,0.883157921109826,0.845487551074397,1.10308487312571,1.17278088573762,1.12185228880731,1.16240132496171,1.16396926325517,1.57981568496157,1.39448889962597,1.60439450993895,1.50604286013268,1.47426219810181,2.24935184314678,2.21917889528684,2.18997388872789,2.28940752998317,2.2380604634483,0.178042904409866,0.185272208814491,0.173076927959879,0.164331930278978,0.185311064858525,0.20207183685895,0.34071293341661,0.313413736160802,0.344360328915746,0.317912967288597,0.295830447688333,0.365008667311093,0.492834354674866,0.493971947183874,0.495724685718756,0.491044392148462,0.516371362731877,0.474021203547165,0.695107806141464,0.657693871527239,0.664714463481034,0.662057432355778,0.674760017808986,0.710496753618093,0.90386283430667,0.928298057380636,0.872528930023897,0.941481842217925,0.897202535010245,0.897105089735305,1.15375016366168,1.16112170224363,1.09948765932845,1.09479063465369,1.17102942841632,1.10386337745291,1.50214802774141,1.48372887005636,1.40877386019216,1.4449529126078,1.44815656298357,1.55431987289708,1.89858370995159,1.92819166364052,1.79292123398954,1.8620775228508,1.71751396052019,1.81455501927544,2.38396994911477,2.68164130666388,2.77520919369007,2.46118241141237,2.73563734052136,2.20794760200186,0.238071378517852,0.268235334448701,0.245298181302175,0.254233291134931,0.270241312750531,0.285528317563903,0.254017264244031,0.452878807320181,0.462200408628303,0.451394635774621,0.431607026598928,0.413468093179814,0.431741642562337,0.45311161250955,0.619661787760186,0.670030176145369,0.642171275305945,0.6587686276461,0.67289626303521,0.622721130833958,0.650476451239564,0.920726678419157,0.876641915374923,0.858426397424697,0.873261319589464,0.871394411637523,0.933427608632687,0.865726902253118,1.19582481464415,1.0625529850076,1.05251283347779,1.09666947126394,1.04522949622242,1.06787443869793,1.14063707584643,1.34144328364175,1.48181770904682,1.43518943286509,1.41502408935784,1.38479601648464,1.40897120393144,1.42012872815368,1.73740502162979,1.68135887758383,1.80429417460617,1.81383781718837,1.81447144772185,1.7919859611466,1.67754192238441,1.95799612487178,2.21682813939888,2.10052479368004,1.89170966355565,2.29349095771793,2.0417499939894,2.27201916028459,2.76125988954029,2.4981088448812,2.68713240597488,2.58293395416182,3.25197671376895,2.95092343113325,2.62779763585242,0.35751881288198,0.338402459706056,0.332808456552832,0.362398243272125,0.358150906985482,0.326904597435379,0.329775379388473,0.316234945964665,0.537775875461686,0.591981415766517,0.560434620614658,0.571163599717058,0.582027065472584,0.567717988108057,0.585934802677866,0.593909593513651,0.817453806211533,0.782048558713376,0.769476202173516,0.736591403400205,0.856271926052386,0.856895170939452,0.842134730024076,0.866911109237399,1.02092278254266,1.10563552194197,1.07724757395866,1.08021703150755,1.08756920409867,1.08994968022983,1.06962595822674,1.04819320645007,1.27222750855438,1.32008143423894,1.30834622560021,1.27360909415914,1.37547902803255,1.35092111806614,1.36334540990778,1.33994254958159,1.57981292812756,1.77293780820901,1.5637428294524,1.57445735136478,1.62855488330793,1.67405564331816,1.56904157663771,1.62417268603821,1.98132677249317,1.94917658407456,1.98494852437822,1.83541372966815,2.01731256884464,1.89273499322282,1.89029769596545,1.97974479051442,2.13556681113226,2.3936449550296,2.42970755307329,2.33889515157842,2.24842589672753,2.3640354440791,2.34618496996536,2.45084227365087,3.23854924222766,2.76392042641595,2.72806149108082,3.35164009184971,2.8944474709212,2.68725579805565,2.62339071110078,3.09270777074313,0.441365214322349,0.416082265527138,0.428462852771452,0.426859871350598,0.43334867310854,0.411966021059659,0.400284722498806,0.419562856557134,0.430293759805006,0.750520505879501,0.697367241085441,0.748271341115024,0.709803063505385,0.681899023083932,0.69819319089933,0.684159971518573,0.686295774091084,0.6768331249891,0.910588469129349,1.006745704366,0.959888478207755,0.980659122019248,1.04265932035511,0.955229281854442,0.885356073657543,0.979257010496288,0.952166041932185,1.26395839941933,1.27798400788536,1.15586780444527,1.21388213228252,1.22573039197846,1.21246587443768,1.17142594186249,1.2927348657588,1.27156155058234,1.60940407924593,1.52067429332541,1.46134444619684,1.53007433556577,1.52638151759335,1.40642860631669,1.53084324032477,1.50626470898909,1.58047148245131,1.82489316688177,1.86152411387006,1.98285145128028,1.83638152965567,1.88572693494137,1.81547375376285,1.89601089013983,1.84831409942473,1.71290836890375,2.02238923667426,2.06484920674609,2.38229597348044,2.23635775888632,2.36167278834093,2.08014389791938,2.31106458109456,2.11141406117286,2.1153242438063,2.49882505756341,2.70755977763134,2.91338591011114,2.32048383927023,2.60961718693885,2.65367230278392,2.54832569437659,2.43898177587181,2.71952360596091,3.27571660104566,2.67345171151948,3.1313372097409,3.15862340572599,2.96416863023053,2.62759474264931,3.26698873235809,3.29905675985984,3.34568266938424],\"mse\":[0.483609597161079,0.47311293123205,0.499844721447982,0.463830128072564,0.542678769490974,0.450135505745018,0.473145332860854,0.527597207663229,0.50795880424507,0.475360596447631,1.12225678309505,0.948538636514823,1.00273925888537,1.12578451971196,1.11593792111527,0.918245598765458,1.04117649551153,1.02987273312095,1.11538566028793,1.16245167644099,1.83776833762655,1.9131574991927,2.02649732866849,1.85725444612446,1.93218923029197,1.98049733836036,2.05934067768938,2.05490035683638,1.86167153680256,1.88398729890469,3.278361286942,3.44112983265994,3.29491166866565,3.06452158429882,3.13078309392043,3.04708873665315,3.14623976498913,3.16429949178046,3.17131769190241,3.19555217151217,4.87752933407601,5.48590529745113,4.91451913334066,5.36441163490556,5.11455714815312,5.10699622136618,5.10226857473783,5.1072550632951,4.99256199218998,4.96331576693586,7.26112196947102,7.76358819453774,7.85789902379355,7.6932059306748,7.44806472674615,8.20448011045782,7.32482129515943,7.37593593664412,7.19791974686567,8.12785236763795,12.4826898060112,11.9540404020288,12.2497529124393,12.2382503234896,12.899844334225,12.6523428330765,12.7235190215426,11.6221923758943,12.8693079952792,12.3885352326031,21.616411040718,22.0628314267032,21.8433902080944,20.2897754330305,22.4405856031799,21.199216428739,19.8264009645492,21.7359621027186,22.4509152978825,22.4556036887426,53.2055591303548,54.0682017879541,53.7484459240023,52.7706424414638,55.0271365654644,52.9012683701907,56.1026397020395,51.1894488848529,47.4380135668625,50.2214348240944,0.0438365388201666,0.0446167259204864,0.0540930813242653,0.050736026369279,0.0590975677359227,0.0601109996450251,0.069498961741416,0.0651199236751007,0.0741006316051195,0.0748425129065311,0.0888545187380685,0.0940978837248193,0.129095989389957,0.135026679501562,0.255980325272649,0.253509039058915,0.6931429119944,0.707471936368468,0.0712389543626044,0.0694822001500392,0.0712095757649367,0.0845229654574395,0.0856919835405342,0.0765692860592476,0.100825724804625,0.101350004160052,0.104113643046453,0.128645061616352,0.125053697183127,0.123393949012056,0.162614981363428,0.170655852224951,0.160269076191933,0.24554393037807,0.261702966352131,0.260366053756493,0.413858366954144,0.446629672492603,0.408223345048976,0.829440283528978,0.845105317247706,0.910431514246064,2.3513019413704,2.59731422291693,2.48488038336935,0.0946966547603108,0.096070062281674,0.0969664443701743,0.0877978380667571,0.116736550191964,0.118866917147583,0.122150742105318,0.11108414766286,0.172307874183,0.16948903556039,0.186111773914244,0.161896730468451,0.239179337890426,0.218993023118764,0.242970743857494,0.25416873649496,0.37870901389503,0.372053663246672,0.349257969433454,0.347714854254901,0.583204827293238,0.626039267263642,0.614348812259387,0.598519620572428,1.02934476156823,1.01951208839017,1.00770879474046,0.952614684450246,1.85120627061781,1.90182650039903,1.94259035176836,1.91949562453923,6.04313131555482,5.43964156233764,5.73421884600195,5.83253255598064,0.114286759146199,0.133273315327473,0.109996063675364,0.124490049715443,0.127385066070344,0.180130719281232,0.178256533109556,0.154273191599272,0.165872255647223,0.168821425885432,0.28186117835077,0.251270510536771,0.27931400138525,0.300771987820169,0.273103213991723,0.437566389003882,0.449538499959431,0.460879799586925,0.458089779764158,0.447260712628346,0.691434859165445,0.655741154622124,0.686366258055556,0.735413254064352,0.771843118236479,1.08104932737724,1.19991208031928,1.1601355479534,1.20143080597631,1.14481003344475,1.98388602976686,2.11604593313438,1.94052322346785,2.02082491233308,2.05951697563683,3.68759477529255,3.79498727603388,3.56580676986966,3.54482956852156,3.70174585638675,9.71398539947739,9.42432144742958,10.6178986862871,9.52147362760532,10.1334001274142,0.165531675443833,0.176223514850907,0.154938492126777,0.162830351254122,0.174280817435543,0.170861966569809,0.300135636340885,0.276247830338175,0.301195147175386,0.277083154139914,0.247759937319591,0.33427553410437,0.449076731978321,0.463779951449253,0.494294841430889,0.441909467708122,0.521664488996043,0.467286435059479,0.769235923683706,0.809445106566863,0.764605649835669,0.826925421758879,0.715240219031191,0.813275304843205,1.24253052545302,1.19062436681712,1.18399872633056,1.30946505625601,1.15524921634104,1.17888889739587,1.96933555511339,1.96680578041622,2.03294836981968,1.93714726612379,2.12512201830679,1.86016348531546,3.34677019608374,3.39796143416603,3.39637621729111,3.23934625196479,3.37867273445924,3.5660082191642,6.1630579917585,6.10149229863748,5.864625833094,6.16466966214501,5.2002199245877,6.13894614516554,15.8887481912655,16.3539948499809,15.6784425464229,15.4378193123852,15.0881844581665,15.7501755994877,0.200622322319755,0.219082697732329,0.209998945858248,0.233862715371426,0.229357569488302,0.230410079764084,0.213964128107104,0.415160266616706,0.433464549992116,0.374053149932916,0.40333680526584,0.358748865942164,0.386667267917199,0.401156539956126,0.673952315417782,0.781372871157207,0.644976903539819,0.681591363709768,0.689328377771561,0.768246994150954,0.752440163016473,1.2886638155977,1.23130451872683,1.19038350233539,1.14963354610635,1.17620105569289,1.24652856407813,1.15949229697792,2.12159779162162,1.91845056932369,1.74293464995463,1.78070928827409,1.88613651805215,1.8776338159033,1.97205669953731,3.11011123068219,3.13442938800058,3.10511746027831,3.1830856157106,2.86395067472783,2.73037752628084,2.93963738328758,4.77982644609245,5.13625700507947,5.40077655070057,5.12520273479609,4.57018869307573,4.92404364988384,5.13448870785747,9.29283377765119,9.83442832750025,9.92395632939967,9.25642689380388,9.08889380375398,8.58472898824726,8.94864075883516,23.6306044864239,22.2664262582234,20.262355963246,22.1044025683282,23.277284466908,22.6688769952163,24.5190756894092,0.307626601229683,0.298771593603054,0.301461122613569,0.313128724260093,0.306123374829326,0.278513459196692,0.270393767444513,0.269361300902575,0.539548123689399,0.581891539095062,0.596624907098301,0.590490702416323,0.603854887105194,0.554255835438952,0.590040522630652,0.628009828357167,1.08248384574214,0.952884669320623,0.908714615509474,0.995168510923815,1.08032042818616,1.10106186955292,1.05396459431956,1.12243591924428,1.52678870785067,1.86613861293071,1.78018727492272,1.73897347734617,1.76168692700673,1.7912357253961,1.72556822300971,1.66150165977337,2.42221185064496,2.71907551233336,2.71327219003125,2.47820847558168,2.91495152113999,2.74073411357742,2.87120222589464,2.73702298261566,4.15796032930333,4.44268279387044,4.04362807129299,4.3771068957823,4.63609676208412,4.18269994747188,4.01353389065128,4.75917058594225,7.09093764306099,7.26189428399154,7.04816537271152,7.22796232214058,7.77284605816768,6.66707164157173,7.23040886222896,6.84074758531548,11.979288022983,13.304459191472,12.9735163723366,12.4957645318739,12.8535274357677,11.9055317237522,12.4068817747269,12.5075710343652,31.8806974764591,30.8811928340967,32.0639341186529,31.9828030099171,32.7654520500942,32.7656685276798,32.9164675235846,32.2555427828065,0.382120752092017,0.382785003827698,0.391885760314957,0.375370084135015,0.396197105177704,0.384778827178787,0.349041765195565,0.371973898519803,0.393837701629543,0.830521839191275,0.740955843636142,0.839892355734135,0.812871904897165,0.752845763797635,0.803483960101702,0.777721342622635,0.812162319298602,0.702202311373871,1.34356619531955,1.54335380567254,1.40814892356012,1.38263784843146,1.59238302072786,1.43609817860949,1.38593468029258,1.37828168091378,1.49965552794514,2.39917576783927,2.25796241030013,2.35484809679609,2.11016013925478,2.09905799072783,2.23502236912448,2.35655296247112,2.40395076037346,2.31626926807736,3.80536756818433,3.5133521977437,4.0459457285602,3.7738685480507,3.74816629277688,3.64393211684503,3.78309982926531,3.82883832709154,3.67297003574001,5.97296417831207,6.42517605662863,5.52253531650955,5.82508428809037,5.57286656690741,5.78986513521429,5.95257484871143,5.92454336574649,5.14116693438063,9.19664563030902,9.47121121945072,9.60877396121836,9.56357353342505,10.0002138474084,9.42234491964042,10.4194147313874,9.14302552037171,8.93469621474552,17.7959807447815,17.3446855392194,17.7667058481535,17.3114622709163,17.3596771006433,16.629104458959,16.2803121321547,17.5587734737861,17.6723156897978,39.4251088716789,37.9839509674709,41.112509605264,39.7811503345625,40.2269022427912,40.3966705446532,39.5754426253879,38.2270874630387,36.2908974563112],\"rsq\":[0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.37553032135553,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.377163362922495,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.375035358588893,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377412998189026,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.377317133160201,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.376480335242765,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.375415726730873,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.378917832652995,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.373820672382339,0.341046150707402,0.341046150707402,0.346318457208177,0.346318457208177,0.343861126044942,0.343861126044942,0.345047495601077,0.345047495601077,0.344467904548118,0.344467904548118,0.341587317058591,0.341587317058591,0.339004854456466,0.339004854456466,0.337192642202061,0.337192642202061,0.343753490465033,0.343753490465033,0.34810975786106,0.34810975786106,0.34810975786106,0.351745534976126,0.351745534976126,0.351745534976126,0.34758587140929,0.34758587140929,0.34758587140929,0.347387954911177,0.347387954911177,0.347387954911177,0.34494065066516,0.34494065066516,0.34494065066516,0.344120134085989,0.344120134085989,0.344120134085989,0.343072498186702,0.343072498186702,0.343072498186702,0.341013669865034,0.341013669865034,0.341013669865034,0.342245265829951,0.342245265829951,0.342245265829951,0.354756311791188,0.354756311791188,0.354756311791188,0.354756311791188,0.350725436005471,0.350725436005471,0.350725436005471,0.350725436005471,0.351549606473407,0.351549606473407,0.351549606473407,0.351549606473407,0.349090520106969,0.349090520106969,0.349090520106969,0.349090520106969,0.350384927153213,0.350384927153213,0.350384927153213,0.350384927153213,0.34987076258395,0.34987076258395,0.34987076258395,0.34987076258395,0.346497036234182,0.346497036234182,0.346497036234182,0.346497036234182,0.346136880197868,0.346136880197868,0.346136880197868,0.346136880197868,0.347835058607209,0.347835058607209,0.347835058607209,0.347835058607209,0.358661270388647,0.358661270388647,0.358661270388647,0.358661270388647,0.358661270388647,0.357940594417092,0.357940594417092,0.357940594417092,0.357940594417092,0.357940594417092,0.357096317858756,0.357096317858756,0.357096317858756,0.357096317858756,0.357096317858756,0.35642701686876,0.35642701686876,0.35642701686876,0.35642701686876,0.35642701686876,0.355825985587405,0.355825985587405,0.355825985587405,0.355825985587405,0.355825985587405,0.355486217844384,0.355486217844384,0.355486217844384,0.355486217844384,0.355486217844384,0.359063300156699,0.359063300156699,0.359063300156699,0.359063300156699,0.359063300156699,0.355026235323443,0.355026235323443,0.355026235323443,0.355026235323443,0.355026235323443,0.350790743038428,0.350790743038428,0.350790743038428,0.350790743038428,0.350790743038428,0.364899668072189,0.364899668072189,0.364899668072189,0.364899668072189,0.364899668072189,0.364899668072189,0.36121009008021,0.36121009008021,0.36121009008021,0.36121009008021,0.36121009008021,0.36121009008021,0.36364519148834,0.36364519148834,0.36364519148834,0.36364519148834,0.36364519148834,0.36364519148834,0.364571420472884,0.364571420472884,0.364571420472884,0.364571420472884,0.364571420472884,0.364571420472884,0.360909988774516,0.360909988774516,0.360909988774516,0.360909988774516,0.360909988774516,0.360909988774516,0.35798608389949,0.35798608389949,0.35798608389949,0.35798608389949,0.35798608389949,0.35798608389949,0.360415619173178,0.360415619173178,0.360415619173178,0.360415619173178,0.360415619173178,0.360415619173178,0.355534533968209,0.355534533968209,0.355534533968209,0.355534533968209,0.355534533968209,0.355534533968209,0.355589115202991,0.355589115202991,0.355589115202991,0.355589115202991,0.355589115202991,0.355589115202991,0.368415072178648,0.368415072178648,0.368415072178648,0.368415072178648,0.368415072178648,0.368415072178648,0.368415072178648,0.358434909223188,0.358434909223188,0.358434909223188,0.358434909223188,0.358434909223188,0.358434909223188,0.358434909223188,0.366642880824469,0.366642880824469,0.366642880824469,0.366642880824469,0.366642880824469,0.366642880824469,0.366642880824469,0.363584629170849,0.363584629170849,0.363584629170849,0.363584629170849,0.363584629170849,0.363584629170849,0.363584629170849,0.3621538217172,0.3621538217172,0.3621538217172,0.3621538217172,0.3621538217172,0.3621538217172,0.3621538217172,0.36047123270955,0.36047123270955,0.36047123270955,0.36047123270955,0.36047123270955,0.36047123270955,0.36047123270955,0.360763411135306,0.360763411135306,0.360763411135306,0.360763411135306,0.360763411135306,0.360763411135306,0.360763411135306,0.363583927609312,0.363583927609312,0.363583927609312,0.363583927609312,0.363583927609312,0.363583927609312,0.363583927609312,0.366582117406818,0.366582117406818,0.366582117406818,0.366582117406818,0.366582117406818,0.366582117406818,0.366582117406818,0.370552689655383,0.370552689655383,0.370552689655383,0.370552689655383,0.370552689655383,0.370552689655383,0.370552689655383,0.370552689655383,0.368273338098993,0.368273338098993,0.368273338098993,0.368273338098993,0.368273338098993,0.368273338098993,0.368273338098993,0.368273338098993,0.368935598833224,0.368935598833224,0.368935598833224,0.368935598833224,0.368935598833224,0.368935598833224,0.368935598833224,0.368935598833224,0.367649578568047,0.367649578568047,0.367649578568047,0.367649578568047,0.367649578568047,0.367649578568047,0.367649578568047,0.367649578568047,0.366903320410018,0.366903320410018,0.366903320410018,0.366903320410018,0.366903320410018,0.366903320410018,0.366903320410018,0.366903320410018,0.371224805314208,0.371224805314208,0.371224805314208,0.371224805314208,0.371224805314208,0.371224805314208,0.371224805314208,0.371224805314208,0.367681953360243,0.367681953360243,0.367681953360243,0.367681953360243,0.367681953360243,0.367681953360243,0.367681953360243,0.367681953360243,0.365412679361025,0.365412679361025,0.365412679361025,0.365412679361025,0.365412679361025,0.365412679361025,0.365412679361025,0.365412679361025,0.367048659116737,0.367048659116737,0.367048659116737,0.367048659116737,0.367048659116737,0.367048659116737,0.367048659116737,0.367048659116737,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.376222809931967,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.373780374118195,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372132376933152,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372829937827533,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.372215922449597,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.374524980558583,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.36890640927816,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368507488006661,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694,0.368585597886694]},\"columns\":[{\"accessor\":\"cor\",\"name\":\"Correlation\",\"type\":\"numeric\",\"filterable\":true},{\"accessor\":\"npred\",\"name\":\"p\",\"type\":\"numeric\",\"filterable\":true},{\"accessor\":\"predictor\",\"name\":\"Predictor\",\"type\":\"character\"},{\"accessor\":\"ratio\",\"name\":\"Ratio\",\"type\":\"numeric\",\"filterable\":true},{\"accessor\":\"coverage\",\"name\":\"Coverage\",\"type\":\"numeric\"},{\"accessor\":\"estimate\",\"name\":\"Estimate\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":3},\"aggregated\":{\"digits\":3}}},{\"accessor\":\"bias\",\"name\":\"Bias\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":3},\"aggregated\":{\"digits\":3}}},{\"accessor\":\"mse\",\"name\":\"MSE\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":3},\"aggregated\":{\"digits\":3}}},{\"accessor\":\"rsq\",\"name\":\"R2\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":3},\"aggregated\":{\"digits\":3}}}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"03650eefa9ea2edd1ca08072fcd6ac47\",\"key\":\"03650eefa9ea2edd1ca08072fcd6ac47\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\r\nThese plots make clear that our best case scenario is to have few,\r\nweakly correlated predictors. Of course, this is a setting rarely seen\r\nin empirical research – whats the point of going through model selection\r\nif you have only a handful of variables? What should really get our\r\nattention here is that bias in estimates is likely to get bigger if\r\nmodel selection is naively performed with little prior information about\r\nthe data generating system to filter out candidate models. We have seen\r\nthat sample size, SNR, number of predictors, and correlation between\r\nparameters with themselves and with the response variable, are\r\ncharacteristics of the model that each in their own way contributes to\r\nproducing biased estimates. I encourage you to use this companion shinyapp to\r\nplay with different scenarios to practice what we’ve been discussing so\r\nfar.\r\nVisualizing distributions\r\nWe’ve discussed earlier how dropping a predictor from the model can\r\ndistort the coefficient sampling distribution of the remaining ones when\r\nthey’re not orthogonal. However even when the preferred (or “correct”)\r\nmodel is selected, there is no guarantee about obtaining sound\r\nregression coefficient estimates. In this final example, we’ll show that\r\nif by chance the “correct” model is achieved by model selection, the\r\nsampling distributions of the resulting regression coefficients might be\r\ndifferent wheter we condition on arriving at the correct model or on the\r\ncorrect model being known in advance.\r\nConsider this example from Berk et al. (2010). As with the other examples, we’ll\r\nimplement selection forward stepwise regression using the AIC as a fit\r\ncriterion. The full regression model takes the form of\r\n\\[\\begin{equation}\r\ny_i = \\beta_0 + \\beta_1w_i + \\beta_2x_i + \\beta_3z_i + \\varepsilon_i\r\n\\tag{9}\r\n\\end{equation}\\]\r\nwhere \\(\\beta_0\\) = 3.0, \\(\\beta_1\\) = 0.0, \\(\\beta_2\\) = 1.0, and \\(\\beta_3\\) = 2.0. The variances and\r\ncovariance are set as: \\(\\sigma^2_\\varepsilon\\) = 10.0, \\(\\sigma^2_w\\) = 5.0, \\(\\sigma^2_x\\) = 6.0, \\(\\sigma^2_z\\) = 7.0, \\(\\sigma_{w,x}\\) = 4.0, \\(\\sigma_{w,z}\\) = 5.0, and \\(\\sigma_{x,z}\\) = 5.0. The sample size is\r\n200.\r\nNote that Berk et al. (2010) here uses the term\r\npreferred model instead of correct model. They do so\r\nbecause a model that excludes \\(W\\) can\r\nalso be called correct the same as one like (9) as long at\r\n\\(\\beta_1\\) = 0 is allowed. To be\r\nconsistent with the original text, we’ll use preferred to refer\r\nto the model with w excluded. This model is preferred because it\r\ngenerates the same conditional expectations for the response using up\r\none less degree of freedom. The plots show the regression estimates\r\nt-values. “A distribution of t-values is more informative than\r\na distribution of regression coefficients because it takes the\r\nregression coefficients and their standard errors into account” (Berk\r\net al., 2010, p. 266).\r\n\r\n\r\nShow code\r\n\r\nreps = 1000 # mudar p/ 10000 depois\r\np <- 3\r\nSigma <- matrix(c(5,4,5,\r\n                  4,6,5, \r\n                  5,5,7), p, p)\r\nn = 200\r\nbetas <- c(3, 0, 1, 2)\r\nrsq <- NULL\r\ncoefs <- cover <- matrix(NA, nrow = reps, ncol = 3)\r\ncolnames(coefs) <- c(\"w\", \"x\", \"z\")\r\ncolnames(cover) <- c(\"w\", \"x\", \"z\")\r\n\r\nfor (i in seq(reps)) {\r\n  #print(i)\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 3) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% betas + rnorm(n, 0, 10))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"w\", \"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x + z, data = Xy)\r\n  sel <- step(fit, k = 2, trace = FALSE)\r\n  s <- summary(sel)\r\n  tvals <- s$coefficients[,3][-1]\r\n  coefs[i, names(tvals)] <-  tvals\r\n  rsq[i] <- s$r.squared\r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nreps = 1000\r\np <- 3\r\nSigma <- matrix(c(5,4,5,\r\n                  4,6,5, \r\n                  5,5,7), p, p)\r\nn = 200\r\nbetas <- c(3, 0, 1, 2)\r\nrsq_pref <- NULL\r\ncoefs_pref <- cover_pref <- matrix(NA, nrow = reps, ncol = 3)\r\ncolnames(coefs_pref) <- c(\"w\", \"x\", \"z\")\r\ncolnames(cover_pref) <- c(\"w\", \"x\", \"z\")\r\n\r\nfor (i in seq(reps)) {\r\n  #print(i)\r\n  X <-  MASS::mvrnorm(n = n, rep(0, 3) , Sigma)\r\n  y <- as.numeric(cbind(1, X) %*% betas + rnorm(n, 0, 10))\r\n  Xy <- as.data.frame( cbind(X, y))\r\n  colnames(Xy) <- c(c(\"w\", \"x\", \"z\"), \"y\")\r\n  fit <- lm(y ~ x + z, data = Xy)\r\n  s <- summary(fit)\r\n  tvals <- s$coefficients[,3][-1]\r\n  coefs_pref[i, names(tvals)] <-  tvals\r\n  rsq_pref[i] <- s$r.squared\r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfull_model <- tibble::as_tibble(coefs_pref)\r\npref_selected <- tibble::as_tibble(coefs[!is.na(coefs[,\"x\"]  & coefs[,\"z\"]),])\r\nx_included <- tibble::as_tibble(coefs[!is.na(coefs[,\"x\"]),])\r\nz_included <- tibble::as_tibble(coefs[!is.na(coefs[,\"z\"]),])\r\n\r\nres_df <- dplyr::bind_rows(\"full\" = full_model, \"pref\" = pref_selected, \r\n                          \"x_included\" = x_included, \"z_included\" = z_included, .id=\"sim\") \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxplot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"x_included\")) |> \r\n  ggplot(aes(x, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.5) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"top\") +\r\n  scale_y_continuous(limits = c(0, 0.5)) +\r\n  labs(x = \"t-values for Regressor X\", y = \"Density\") +\r\n  scale_color_discrete(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"predictor being included in a model\")) +\r\n  scale_fill_discrete(name = \"Conditional on\",\r\n                      labels = c(\"preferred model being known\", \"predictor being included in a model\"))\r\n\r\nzplot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"z_included\")) |> \r\n  ggplot(aes(z, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.5) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"none\",\r\n        axis.title.y = element_blank(),\r\n        axis.text.y = element_blank()) +\r\n  scale_y_continuous(limits = c(0, 0.5)) +\r\n  labs(x = \"t-values for Regressor Z\", y = \"Density\")\r\n\r\n\r\nxplot + zplot\r\n\r\n\r\n\r\n\r\nFigure 1: Stepwise regression sampling distributions of the\r\nregression coefficient t-values for regressors X and Z. Red\r\ndensity plot is is conditional on the preferred model being known. The\r\nblue density plot is conditional on the regressor being included in a\r\nmodel\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxpref_plot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"pref\")) |> \r\n  ggplot(aes(x, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.5) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"top\") +\r\n  scale_y_continuous(limits = c(0, 0.5)) +\r\n  labs(x = \"t-values for Regressor X\", y = \"Density\")\r\n\r\nzpref_plot <- res_df |> \r\n  dplyr::filter(sim %in% c(\"full\", \"pref\")) |> \r\n  ggplot(aes(z, fill = sim, color = sim)) +\r\n  geom_density(adjust = 2, alpha = 0.5) +\r\n  theme_minimal(12) +\r\n  theme(legend.position = \"none\",\r\n        axis.title.y = element_blank(),\r\n        axis.text.y = element_blank()) +\r\n  scale_y_continuous(limits = c(0, 0.5)) +\r\n  labs(x = \"t-values for Regressor Z\")\r\n\r\nxpref_plot + zpref_plot\r\n\r\n\r\n\r\n\r\nFigure 2: Stepwise regression sampling distributions of the\r\nregression coefficient t-values for regressors X and Z. Red\r\ndensity plot is is conditional on the preferred model being known. The\r\nblue density plot is conditional on the preferred model being selected\r\n\r\n\r\n\r\nIn both figures 1 and 2 the red\r\ndensity plot represents the regression estimates t-values\r\ndistribution when no model selection is performed. In 1\r\nthe blue density plot show the distributions when either \\(X\\) or \\(Z\\) are included in the model. For\r\n2 the blue distribution refers to distributions of\r\neither \\(X\\) or \\(Z\\) t-values when the prefferred\r\nmodel is selected.\r\nThe contrast between red and blue curves are apparent. Most striking\r\nare the difference in t-values distributions\r\npost-model-selection for regressor \\(Z\\) when conditioned on the regressor being\r\nincluded in a model. This curve displays a bimodal distribution and\r\nhighly biased mean and standard deviaton, as summarized below.\r\n\r\n\r\nShow code\r\n\r\nres_df |> \r\n  group_by(sim) |> \r\n  summarise(Mx = mean(x, na.rm = TRUE), \r\n            x_sd = sd(x, na.rm = TRUE),\r\n            Mz = mean(z, na.rm = TRUE),\r\n            z_sd = sd(z, na.rm = TRUE)) |> \r\n  knitr::kable()\r\n\r\n\r\nsim\r\nMx\r\nx_sd\r\nMz\r\nz_sd\r\nfull\r\n2.140490\r\n1.0077928\r\n4.802557\r\n1.0728233\r\npref\r\n2.578367\r\n0.7612412\r\n4.480407\r\n0.9663504\r\nx_included\r\n2.578367\r\n0.7612412\r\n4.480407\r\n0.9663504\r\nz_included\r\n2.578367\r\n0.7612412\r\n5.674328\r\n2.5056967\r\n\r\nIt is especially telling observing those plots that the assumed\r\nunderlying distribution can be very different from what is obtained.\r\nStatistical inference performed in such scenarios would be misleading.\r\nFigure 2 confirms that conditioning on arriving at the\r\npreferred model does not guarantee trustable estimates.\r\nConclusion\r\nModel selection methods are routine in research on the social and\r\nbehavioral sciences, and commonly taught in applied statistics courses\r\nand textbooks. Little is mentioned about the biased estimates obtained\r\nwhen such procedures are carried. With simulated data we have identified\r\nspecific characteristics of the data generating model that can\r\npotentially increase the bias in estimates obtained through variable\r\nselection. The take home point is that post-model-selection sampling\r\ndistribution can deviate greatly from the assumed underlying\r\ndistribution, even when the model best representative of the data\r\ngeneration process has been selected.\r\n\r\n\r\n\r\nBerk, R. (2010). What you can and can’t properly do with regression.\r\nJournal of Quantitative Criminology, 26(4), 481–487.\r\nhttps://doi.org/10.1007/s10940-010-9116-4\r\n\r\n\r\nBerk, R., Brown, L., Buja, A., Zhang, K., & Zhao, L. (2013). Valid\r\npost-selection inference. The Annals of Statistics,\r\n41(2), 802–837. https://doi.org/10.1214/12-AOS1077\r\n\r\n\r\nBerk, R., Brown, L., & Zhao, L. (2010). Statistical Inference\r\nAfter Model Selection. Journal of Quantitative\r\nCriminology, 26(2), 217–236. https://doi.org/10.1007/s10940-009-9077-7\r\n\r\n\r\nCohen, J., & Cohen, P. (1983). Applied Multiple\r\nRegression/Correlation Analysis for the\r\nBehavioral Sciences (2nd ed.).\r\n\r\n\r\nFlora, D. B. (2017). Statistical methods for the social and\r\nbehavioural sciences: A model-based approach. SAGE Publications. https://books.google.com.br/books?id=4mcCDgAAQBAJ\r\n\r\n\r\nLukacs, P. M., Burnham, K. P., & Anderson, D. R. (2009). Model\r\nselection bias and Freedmanâ€™s paradox. Annals of the\r\nInstitute of Statistical Mathematics, 62(1), 117. https://doi.org/10.1007/s10463-009-0234-4\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-20-model-selection-bias/model-selection-bias_files/figure-html5/predinc-1.png",
    "last_modified": "2022-04-03T19:01:20-03:00",
    "input_file": "model-selection-bias.knit.md",
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2021-11-04-simulating-regression-artifact/",
    "title": "Regression Artifact - Re-running Farmus et. al (2019)",
    "description": "In this post I'll how false positives can emerge if a model not fit to the research question is chosen. I also provide code for a simple Monte Carlo simulation of the frequency of these false positive results.",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2021-11-04",
    "categories": [
      "simulation",
      "regression"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nLord’s Paradox\r\nand the Regression Artifact\r\nRe-running Farmus et. al\r\n(2019)\r\nConclusion\r\n\r\nMotivation\r\nRecently I came across a publication (Farmus et al., 2019) where the authors\r\noutline a statistical artifact that may arise when researchers add\r\ncontinuous predictors correlated with baseline scores as covariates in\r\nregression models of pretest-posttest analysis. This effect is known\r\nwidely as Lord’s\r\nParadox.\r\nFirst I’ll briefly cover what is the regression artifact discussed in\r\nthe first part of the paper. My main goal, however, is to rerun the\r\nsimulation from part two of their study. This was motivated by the\r\nunavailability of the source code as well as an exercise on building a\r\nsimple simulation study. Replicating simulation studies is a good\r\npractice not only to spot potential errors or biases in the results but\r\nalso to learn and rehearse open research practices (Lohmann et al., 2021).\r\nThe code is pretty straightforward and there is nothing too fancy\r\nabout the methods, but I hope you may learn something from this post\r\nthat you can apply in your own studies. Having to think about how to\r\ntranslate words and equations into code also made me pause and think\r\nwith more depth on what I was reading.\r\nI’ll be using R for the simulations. For pedagogical reasons I have\r\nopted to work with base R, limiting the use of external packages only to\r\nfacilitate summarizing results and building plots. I’m saying that\r\nbecause there is the great faux package\r\ndeveloped by Lisa\r\nDeBruine for simulating factorial designs that could save us many\r\nlines of code.\r\nLord’s Paradox and\r\nthe Regression Artifact\r\nSay that we want to test a treatment in two preexisting groups, A and\r\nB. Given some prior knowledge about the nature of our hypothetical\r\ndependent variable, we have strong basis to suppose that a random\r\nsubject from group B is higher on average on DV scores than a random\r\nsubject from group A. Because those groups occur naturally it isn’t\r\npossible to perform random assignment of subjects to groups.\r\nWe need to specify how we’re going to analyse the data. Given our\r\nresearch design and question there are two potential statistical models\r\nto be chosen from (Farmus et al.,\r\n2019):\r\nA change score model (t test)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + pre_i + e_i\r\n\\tag{1}\r\n\\end{equation}\\]\r\nA regression based model (ANCOVA)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + \\beta_2pre_i + e_i\r\n\\tag{2}\r\n\\end{equation}\\]\r\nHere \\(pre_i\\) and \\(post_i\\) are the pretest and posttest\r\nscores of subject \\(i\\). In equation\r\n(1), \\(\\beta_1\\) is the\r\nmain effect of group (or group mean differences) on gain scores and in\r\nequation (2), it is the partial regression coefficient of\r\n\\(group\\) adjusted for pretest scores.\r\nThe model intercept is \\(\\beta_0\\) and\r\nthe model residuals is represented by \\(e_i\\).\r\nIt should be noted that using \\(post_i\\) or the change score (\\(post_i - pre_i\\)) as the outcome variable\r\nyields the same results if \\(pre_i\\) is\r\na covariate in the model (Senn, 2021).\r\nThough in practice we could choose any of these two models, we may\r\nreach different conclusions depending on which one we use. In fact, the\r\nchoice of the model relies on the questions we’re asking.\r\nThe ANCOVA model is preferred when we do not expect baseline\r\ndifferences between groups. This model estimates the difference in post\r\ntreatment scores controlling for baseline variations (Walker, 2021). Using pretest scores as\r\ncovariate when groups are known to be different at baseline is a poor\r\nchoice because we do not expect the initial differences to disappear\r\nwhen scores are averaged over the group (Pearl, 2016). As we’ll see shortly, this\r\nis exactly why the paradox emerges.\r\nThe change score model do not assume that groups are similar at\r\nbaseline. It tests if groups differ on average on how much they change\r\non the outcome variable from pretest to posttest. The effect of\r\ntreatment is the difference of pre to post differences between the two\r\ngroups (Walker, 2021).\r\nWhat Lord (1967) observed is\r\nthat given a treatment that produces no effect on the\r\noutcome, a researcher using the ANCOVA model would conclude that,\r\nassuming equal baseline scores, one subgroup appears to gain\r\nsignificantly more than the other. A second researcher analyzing the\r\nsame dataset with the change score model would not reject the null\r\nhypothesis that \\(\\beta_1\\) = 0.\r\nThough we have used a categorical predictor \\(group_i\\), the emergence of the paradox is\r\nextended to continuous variables. To give rise to the artifact, there\r\nneeds only to exists some correlation between this predictor and the\r\npretest scores as well as random error in the assessment of change (Eriksson & Häggström, 2014). It can\r\nhappen if, for example, participants are allocated into groups based on\r\nscores on some continuous predictor that happens to be associated with\r\nthe parameter under investigation (Wright, 2006).\r\nIn most research settings (e.g. psychology), measurement is made with\r\nsome degree of error. In the persence of random error of measurement,\r\nthis variance is incorporated in the pre and posttest scores. If we let\r\n\\(U_i\\) represent the “true score”\r\n(i.e. what we would get if there were no measurement error) of subject\r\n\\(i\\), we have:\r\n\\[\\begin{equation}\r\npre_i = U_i^{pre} + e_i^{pre}\r\n\\tag{3}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\npost_i = U_i^{post} + e_i^{post}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nThen, because the expected effect size of treatment is zero, \\(U_i^{post} - U_i^{pre}\\) reflects only the\r\ndifference between random errors (Eriksson & Häggström, 2014).\r\nTo keep consistent with the terminology used in Eriksson & Häggström (2014), we use\r\n\\(P_i\\) to denote the group variable\r\nfor the \\(i^{th}\\) subject, where \\(P_i = 0\\) for Group A and \\(P_i = 1\\) for Group B (keep in mind that\r\nthere is no restriction on \\(P\\) begin\r\nsolely categorical). Because the “true” pretest scores is estimated from\r\nthe grouping variable, we have:\r\n\\[\\begin{equation}\r\nU_i = a + bP_i + \\epsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nwhere \\(a\\) is the model intercept,\r\n\\(b\\) the regression coefficient of\r\n\\(P_i\\) on \\(U_i\\) and \\(\\epsilon_i\\) is the between-individual\r\nvariation in \\(U\\).\r\nLets assume the error term in the measurement of pretest and posttest\r\nscores is drawn from a normal distribution with mean 0 and standard\r\ndeviation, \\(\\sigma\\). Likewise, the\r\nerror term for the regression of property \\(P\\) on true score \\(U_i\\) is drawn from a normal distribution\r\nwith mean 0 and standard deviation \\(s\\).\r\nFor a sample of 200 subjects we set, \\(a\\) = 0.5, \\(b\\) = 0.4, \\(s\\) = 0.1, \\(\\sigma\\) = 0.1.\r\n\r\n\r\nset.seed(7895) # set seed for reproducibility\r\n\r\na <- 0.5 # model intercept\r\nb <- 0.4 # regression coefficient\r\nsigma_error <- .1 # pre and posttest error term standard deviation\r\nvar_error <-  .1  # P error term standard deviation\r\nn <- 200 # sample size\r\n\r\ngroup <- rep(c(1,0), each=n/2) # group predictor\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error) # true score\r\npost <- U + rnorm(n, 0, sigma_error) # posttest scores\r\npre <- U + rnorm(n, 0, sigma_error)  # pretest scorers\r\nchange <- post-pre                   # change scores\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n\r\n\r\n\r\n(ref:sim-plot-1) Scatterplot for the average gain from pre to post\r\ntreatment. Blue line is the slope for group A and the red line is the\r\nslope for group B. The black dashed line is the slope for average gain\r\ntaking both groups together\r\n\r\n\r\nShow code\r\n\r\npre_post <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = post, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = post, color=group),method='lm', \r\n              se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  stat_ellipse(aes(x= pre, y = post, color=group),size = 1) +\r\n  labs(x = \"Pretest scores\",\r\n       y = \"Posttest scores\") +\r\n  scale_color_manual(name = \"Group\", labels = c(\"A\", \"B\"), \r\n                     values = c(\"#1E88E5\", \"#D81B60\")) +\r\n  theme_minimal(12) +\r\n  geom_smooth(aes(x= pre, y = post), method='lm', se = FALSE, color = \"#000000\",\r\n              linetype = \"dashed\", formula= y~x, fullrange=TRUE) +\r\n  xlim(0, NA) + \r\n  ylim(0, NA)\r\n\r\npre_change <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = change, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = change, color=group),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  geom_smooth(aes(x= pre, y = change),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  theme_minimal(12)\r\n\r\npre_post\r\n\r\n\r\n\r\n\r\nFigure 1: (ref:sim-plot-1)\r\n\r\n\r\n\r\nSo, what do we see? The black dashed line crossing both elipses is\r\nclose to 45° which means that, taking both groups as a whole, looks like\r\nthere is no evidence of average differences between groups. In this\r\nparticular example the slopes for both groups behave as we should often\r\nexpect when there is no change in scores between two time points. It is\r\nvisually clear that both are very similar.\r\nNotice that when subjects from groups A and B fall on the same range\r\nof pretest scores we see more red dots on the upper half of the plot,\r\nsuggesting higher posttest scores from subjects in group B. As noted by\r\nEriksson &\r\nHäggström (2014) what is happening is simply an\r\neffect of regression to the mean. Because when subjects from group B\r\n(higher true score) score low on pretest, they are more likely to do\r\nbetter at posttest. A similar logic applies to high scorers from group A\r\nscoring poorer on the followup.\r\nA mediational perspective\r\nIt can also help to visualize the difference between the two models\r\nusing diagrams. As shown by Pearl (2016), we can represent Lord’s paradox\r\nas a mediation model,\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"img/dag2.jpg\")\r\n\r\n\r\n\r\n\r\nFigure 2: Linear version of the model showing pretest scores\r\n(Pre) as a mediator between Group (G) and posttest scores (Post).\r\nAdapted from Pearl (2016)\r\n\r\n\r\n\r\nwhere G is the group variable, Pre\r\nthe pretest scores, Post the posttest scores and\r\nY stands for the difference Post - Pre. What\r\ndifferentiates the ANCOVA and the change score perspectives lies on\r\nwhich effect we wish to estimate. Assuming no confounding, the total\r\neffect is estimated as\r\n\\[\\begin{equation}\r\nTE = (b + ac) - a\r\n\\tag{6}\r\n\\end{equation}\\]\r\nSetting \\(b = a(1-c)\\) all paths\r\ncancel out each other and we obtain the total effect of 0 observed in\r\nour made up experiment.\r\nIf we opt for adjusting for pretest scores, the path \\(ac\\) is blocked and only the direct\r\neffect is estimated, which is,\r\n\\[\\begin{equation}\r\nDE = b\r\n\\tag{7}\r\n\\end{equation}\\]\r\nBecause the direct effect is positive, it becomes clear why the\r\neffect for the ANCOVA model is non-zero.\r\nThe regression artifact\r\nThe issue raised in Farmus et al. (2019) is that Lord’s Paradox isn’t\r\nrestricted to categorical predictors. A similar effect can emerge if a\r\ncontinuous predictor of change is correlated with pretest scores and an\r\nANCOVA model is used. Through a literature review, the authors confirm\r\nthat studies meeting these conditions are not uncommon among papers\r\npublished in top psychology journals.\r\nFor simplicity I’ll not bother to show the mathematical derivation of\r\nthe regression artifact here but the interested reader can refer to\r\nEriksson &\r\nHäggström (2014) or Kim (2018). What we need to have in mind is\r\nthat the regression artifact is the coefficient \\(\\beta_1\\) from equation (2).\r\nUsing Eriksson &\r\nHäggström (2014) notation, the estimate for the\r\nregression artifact is expressed as:\r\n\\[\\begin{equation}\r\n\\hat{K} = \\frac{b\\sigma^2}{(s^2+\\sigma^2)}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nIf we input the parameters set to build figure 1\r\nand calculate the value of \\(K\\),\r\nassuming infinite samples, \\(K\\) =\r\n0.2.\r\nSurprisingly, re-running Eriksson & Häggström (2014)\r\nsimulation of the regression artifact, we get a different estimate for\r\nits standard deviation.\r\n\r\n\r\nShow code\r\n\r\nset.seed(7895)\r\n\r\nK <- c()\r\nreps <- 10000\r\n\r\nfor (i in seq(reps)) {\r\na <- 0.5\r\nb <- 0.4\r\nsigma_error <- .1\r\nvar_error <-  .1\r\nn <- 105\r\n\r\ngroup <- sample(c(0, 1), n, replace = TRUE)\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error)\r\npost <- U + rnorm(n, 0, sigma_error)\r\npre <- U + rnorm(n, 0, sigma_error)\r\nchange <- post-pre\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n  s <- summary(lm(change ~ pre + group, data = sim.df))\r\n  K[i] <- s$coefficients[3,1]\r\n}\r\n\r\nk_mean <- mean(K)\r\nk_sd <- sd(K)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntibble::enframe(K) |> \r\n  ggplot(aes(x = value)) + \r\n  geom_histogram(color = \"black\", fill = \"lightblue1\") +\r\n  labs(x = latex2exp::TeX(\"Coefficient for $\\\\hat{K}$\"),\r\n       y = \"Frequency\") +\r\n  theme_minimal(12)\r\n\r\n\r\n\r\n\r\nFigure 3: Distribution of the coefficients for predictor \\(P\\) on the change score\r\n\r\n\r\n\r\nAs expected, the mean value for \\(\\hat{K}\\) is 0.2. Our estimate for the\r\nstandard deviation of \\(\\hat{K}\\) is\r\n0.04, close to half the value Eriksson & Häggström (2014) reported.\r\nIn their study, they have found 98% of \\(\\hat{K}\\) values greater than zero. In\r\n10,000 simulated datasets, we found the frequency of \\(\\hat{K}\\) > 0 to be 100%1.\r\nRe-running Farmus et. al\r\n(2019)\r\nWe now return to Farmus et al. (2019) study. Including the baseline\r\nscores as covariate in a pretest to posttest change model may lead us to\r\nfalsely conclude a significant association if a non-negligible\r\ncorrelation exists between the predictor and the covariate. The authors\r\nuse Monte Carlo simulations to estimate Type I error rate in scenarios\r\nlike the one described, varying the size of \\(b\\) (equation (5)), and the\r\nsample size.\r\nOther parameters were fixed such that the standard deviation of \\(e_i^{pre}\\), \\(e_i^{post}\\), and \\(\\epsilon_i\\) were set to 1. The variance of\r\n\\(P\\) (\\(\\sigma^2_P\\)) was also set to 1. Both \\(\\beta_0\\) and \\(a\\) were set to 0. Coefficient \\(b\\) ranged from -1 to 1 by 0.5 steps and\r\nsample sizes of 20, 50, 100, and 1,000 were used. To keep every\r\ncondition exact as the original study, 5,000 simulations were run for\r\neach pair and the statistical significance level set at 0.05.\r\nThe simulation will only estimate the Type I error rates because the\r\nsize of parameters like the regression artifact (\\(K\\)) and the correlation between the\r\npretest scores and true score (\\(\\rho(pre,P)\\)) can be estimated from the\r\nparameters values fixed previously.\r\nLooking at equations (3) and (5) we know that,\r\n\\[\\begin{equation}\r\npre_i = a + bP_i + \\epsilon_i + e_i^{pre}\r\n\\tag{9}\r\n\\end{equation}\\]\r\nthen calculating \\(\\rho_{(pre,P)}\\)\r\nbecomes straightforward\r\n\\[\\begin{align}\r\n\\begin{split}\r\n  \\rho_{pre,P} & = \\frac{\\mathrm{Cov}(pre,\r\nP)}{\\sigma_{pre}\\sigma_{P}}\\\\\r\n  & = \\frac{b\\sigma^2_P}{(\\sqrt{b^2\\sigma^2_P + s^2 + \\sigma^2}) +\r\n\\sigma^2_P}\r\n\\end{split}\r\n  \\tag{10}\r\n\\end{align}\\]\r\nOur first step is to create a function that takes as arguments the\r\nnumber of runs, sample size and the population value of coefficient\r\n\\(b\\). We can store the\r\np-value of each analysis and count afterwards how many were\r\nless than the the alpha level.\r\n\r\n\r\nsim.reg.artifact <- function(n.sims, sample.size, beta) {\r\n\r\n  reps <- n.sims\r\n  n <- sample.size\r\n  beta <- beta\r\n  b0 <- 0\r\n  P.sd <- sigma_error <- var_error <-  1\r\n  pval <- NULL\r\n  r <- (beta*(P.sd)^2)/(sqrt((beta^2)*((P.sd)^2) + (sigma_error^2) + (var_error^2)))\r\n  K <- (beta*sigma_error)/(sigma_error + var_error) # the regression artifact\r\n\r\n  for (i in seq(1, reps)) {\r\n\r\n    P = rnorm(n = n, mean = 0, sd = 1) # Property P from equation 5\r\n    U <- b0 + beta*P + rnorm(n, 0, var_error) # The \"true score\" from equation 5\r\n\r\n    pre <- U + rnorm(n, 0, sigma_error) # Pretest scores - equation 3\r\n    post <-  U + rnorm(n, 0, sigma_error) # Posttest scores - equation 4\r\n\r\n    Xy <- as.data.frame(cbind(P, pre, post)) # A dataframe with outcome and predictors\r\n    colnames(Xy) <- c(\"P\", \"pre\", \"post\")\r\n    \r\n    fit <- lm(post ~ ., data = Xy)\r\n    s <- summary(fit)\r\n    pval[i] <- ifelse(s$coefficients[2,4] < 0.05, 1, 0) # p-value for the coefficient of P\r\n\r\n  }\r\n  \r\n  # store the simulated results in a data frame for display\r\n  res <- data.frame(\r\n    sample_size = n,\r\n    bx = beta,\r\n    correlation = r,\r\n    artifact = K,\r\n    error_rate = mean(pval)\r\n  )\r\n\r\n  return(res)\r\n}\r\n\r\n\r\n\r\nNesting for loops can easily become confusing, so I have opted to use\r\nthe purrr::map_dfr() function, which I find to be a simpler\r\nand more elegant solution.\r\n\r\n\r\nsamplesize <- list(20, 50, 100, 1000)\r\ncoefs <- as.list(seq(-1,1, by = 0.5))\r\n\r\n\r\nfinaltable <- purrr::map_dfr(samplesize,\r\n                             ~purrr::map2(.x, coefs,\r\n                                          ~sim.reg.artifact(5000, .x, .y)))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfinaltable |> \r\n  tidyr::pivot_wider(names_from = sample_size, values_from = error_rate) |>\r\n  kbl(\r\n    col.names = c(\"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"N = 20\",\r\n                  \"N = 50\", \"N = 100\", \"N = 1,000\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) |> \r\n  add_header_above(c(\" \" = 3, \"Type I error rate\" = 4 ))\r\n\r\n\r\n\r\n\r\n\r\n\r\nType I error rate\r\n\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nN = 20\r\n\r\n\r\nN = 50\r\n\r\n\r\nN = 100\r\n\r\n\r\nN = 1,000\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.577\r\n\r\n\r\n-0.50\r\n\r\n\r\n0.267\r\n\r\n\r\n0.612\r\n\r\n\r\n0.900\r\n\r\n\r\n1.000\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.333\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.131\r\n\r\n\r\n0.258\r\n\r\n\r\n0.471\r\n\r\n\r\n1.000\r\n\r\n\r\n0.0\r\n\r\n\r\n0.000\r\n\r\n\r\n0.00\r\n\r\n\r\n0.051\r\n\r\n\r\n0.051\r\n\r\n\r\n0.048\r\n\r\n\r\n0.052\r\n\r\n\r\n0.5\r\n\r\n\r\n0.333\r\n\r\n\r\n0.25\r\n\r\n\r\n0.120\r\n\r\n\r\n0.259\r\n\r\n\r\n0.475\r\n\r\n\r\n1.000\r\n\r\n\r\n1.0\r\n\r\n\r\n0.577\r\n\r\n\r\n0.50\r\n\r\n\r\n0.263\r\n\r\n\r\n0.618\r\n\r\n\r\n0.895\r\n\r\n\r\n1.000\r\n\r\n\r\nGreat! Our results match pretty closely those reported by Farmus et al. (2019). Notice\r\nthat in a world where \\(\\rho_{(pre,P)}\\) is zero, in the long run\r\nthe Type I error rate stay within range of what was expected.\r\nTo wrap up, we can use the function just created to also reproduce\r\nthe simulations Farmus et\r\nal. (2019) ran\r\nusing median parameter values discovered in their literature review of\r\nexperimental psychology studies.\r\n\r\n\r\nShow code\r\n\r\nsim2 <- sim.reg.artifact(n.sims = 5000, sample.size = 239, beta = .4)\r\n\r\nsim2 |> \r\n  kbl(\r\n    col.names = c(\"Sample size\", \"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"Type I error rate\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\n\r\n\r\nSample size\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nType I error rate\r\n\r\n\r\n239\r\n\r\n\r\n0.4\r\n\r\n\r\n0.272\r\n\r\n\r\n0.2\r\n\r\n\r\n0.674\r\n\r\n\r\nAgain we have a match of results with the original study. What makes\r\nthose results alarming is the Type I error rate of 0.67! As the authors\r\nemphasize, common conditions in psychological research may elicit\r\nunacceptably high error rates which researchers may not be aware of if\r\nan inappropriate model is employed.\r\nConclusion\r\nWe have covered a definition of Lord’s Paradox and argued that\r\nchoosing a model that does not fit the research question can lead to\r\nbiased conclusions. As Farmus et al. (2019) outline, correlation between the\r\npredictor and the outcome and some amount of measurement error are\r\nnecessary conditions for the emergence of a regression artifact. It is\r\nimportant that researchers take this into consideration before drawing\r\nconclusions about the data.\r\nFollowing recent calls to reproduce simulation studies (Lohmann et al., 2021) we have worked to\r\nreproduce simulations performed by two papers discussing Lord’s Paradox\r\nwith continuous predictors. Monte Carlo simulations are not needed to\r\ndemonstrate the regression artifact and no major recommendations are\r\nbeing proposed by the original studies based on the simulation results.\r\nEven so, a minor discrepancy from Eriksson & Häggström (2014) was\r\nfound, highlighting that computer code is not error free. Reproducing\r\nsimulation analysis proved to be fruitful not only in finding divergent\r\nresults but also as a coding exercise.\r\nLord’s Paradox have been and continues to be widely debated in the\r\nquantitative methods literature. Many of the papers referenced in the\r\ntext broaden the discussion investigating this topic in different and\r\nmore complex scenarios. Hopefully this post has reached its instructive\r\npurpose of teaching a few things about simulating data in R. If you have\r\nany comments or questions, feel free to drop a comment below or reach me\r\nvia email or twitter.\r\n\r\n\r\n\r\nEriksson, K., & Häggström, O. (2014). Lord’s Paradox in\r\na Continuous Setting and a Regression Artifact\r\nin Numerical Cognition Research. PLOS ONE,\r\n9(4), e95949. https://doi.org/f2z5hc\r\n\r\n\r\nFarmus, L., Arpin-Cribbie, C. A., & Cribbie, R. A. (2019).\r\nContinuous predictors of pretest-posttest change: Highlighting the\r\nimpact of the regression artifact. Frontiers in Applied Mathematics\r\nand Statistics, 4, 64. https://doi.org/gk92gx\r\n\r\n\r\nKim, S. B. (2018). Explaining lord’s paradox in introductory statistical\r\ntheory courses. International Journal of Statistics and\r\nProbability, 7(4), 1. https://doi.org/10.5539/ijsp.v7n4p1\r\n\r\n\r\nLohmann, A., Astivia, O. L. O., Morris, T., & Groenwold, R. H. H.\r\n(2021). It’s time! 10 + 1 reasons we should start replicating\r\nsimulation studies. https://doi.org/10.31234/osf.io/agsnt\r\n\r\n\r\nLord, F. M. (1967). A paradox in the interpretation of group\r\ncomparisons. Psychological Bulletin, 68(5), 304–305.\r\nhttps://doi.org/10.1037/h0025105\r\n\r\n\r\nPearl, J. (2016). Lord’s Paradox Revisited \r\n(Oh Lord! Kumbaya!). Journal of Causal Inference,\r\n4(2). https://doi.org/10.1515/jci-2016-0021\r\n\r\n\r\nSenn, S. (2021). Cause for concern. In LinkedIn. LinkedIn. https://www.linkedin.com/pulse/cause-concern-stephen-senn/\r\n\r\n\r\nWalker, J. A. (2021). Models for longitudinal experiments – pre-post\r\ndesigns. In Elements of statistical modeling for experimental\r\nbiology. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html\r\n\r\n\r\nWright, D. B. (2006). Comparing groups in a before-after design: When t\r\ntest and ANCOVA produce different results. British Journal of\r\nEducational Psychology, 76(3), 663–675. https://doi.org/10.1348/000709905x52210\r\n\r\n\r\nI’m not excluding the possibility\r\nthat it may have been me that got it wrong. If this is the case and you\r\nidentify where I screwed up, please let me know!↩︎\r\n",
    "preview": "posts/2021-11-04-simulating-regression-artifact/simulating-regression-artifact_files/figure-html5/sim-plot-1-1.png",
    "last_modified": "2022-04-03T17:34:15-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
