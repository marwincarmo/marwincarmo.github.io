[
  {
    "path": "posts/2021-11-04-simulating-regression-artifact/",
    "title": "Regression Artifact - Re-running Farmus et. al (2019)",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2021-11-04",
    "categories": [
      "simulation",
      "regression"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nLord’s Paradox and the Regression Artifact\r\nRe-running Farmus et. al (2019)\r\nConclusion\r\n\r\nMotivation\r\nRecently I came across a publication (Farmus et al., 2019) where the authors outline a statistical artifact that may arise when researchers add continuous predictors correlated with baseline scores as covariates in regression models of pretest-posttest analysis. This effect is known widely as Lord’s Paradox.\r\nFirst I’ll briefly cover what is the regression artifact discussed in the first part of the paper. My main goal, however, is to rerun the simulation from part two of their study. This was motivated by the unavailability of the source code as well as an exercise on building a simple simulation study. Replicating simulation studies is a good practice not only to spot potential errors or biases in the results but also to learn and rehearse open research practices (Lohmann et al., 2021).\r\nThe code is pretty straightforward and there is nothing too fancy about the methods, but I hope you may learn something from this post that you can apply in your own studies. Having to think about how to translate words and equations into code also made me pause and think with more depth on what I was reading.\r\nI’ll be using R for the simulations. For pedagogical reasons I have opted to work with base R, limiting the use of external packages only to facilitate summarizing results and building plots. I’m saying that because there is the great faux package developed by Lisa DeBruine for simulating factorial designs that could save us many lines of code.\r\nLord’s Paradox and the Regression Artifact\r\nSay that we want to test a treatment in two preexisting groups, A and B. Given some prior knowledge about the nature of our hypothetical dependent variable, we have strong basis to suppose that a random subject from group B is higher on average on DV scores than a random subject from group A. Because those groups occur naturally it isn’t possible to perform random assignment of subjects to groups.\r\nWe need to specify how we’re going to analyse the data. Given our research design and question there are two potential statistical models to be chosen from (Farmus et al., 2019):\r\nA change score model (t test)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + pre_i + e_i\r\n\\tag{1}\r\n\\end{equation}\\]\r\nA regression based model (ANCOVA)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + \\beta_2pre_i + e_i\r\n\\tag{2}\r\n\\end{equation}\\]\r\nHere \\(pre_i\\) and \\(post_i\\) are the pretest and posttest scores of subject \\(i\\). In equation (1), \\(\\beta_1\\) is the main effect of group (or group mean differences) on gain scores and in equation (2), it is the partial regression coefficient of \\(group\\) adjusted for pretest scores. The model intercept is \\(\\beta_0\\) and the model residuals is represented by \\(e_i\\).\r\nIt should be noted that using \\(post_i\\) or the change score (\\(post_i - pre_i\\)) as the outcome variable yields the same results if \\(pre_i\\) is a covariate in the model (Senn, 2021).\r\nThough in practice we could choose any of these two models, we may reach different conclusions depending on which one we use. In fact, the choice of the model relies on the questions we’re asking.\r\nThe ANCOVA model is preferred when we do not expect baseline differences between groups. This model estimates the difference in post treatment scores controlling for baseline variations (Walker, 2021). Using pretest scores as covariate when groups are known to be different at baseline is a poor choice because we do not expect the initial differences to disappear when scores are averaged over the group (Pearl, 2016). As we’ll see shortly, this is exactly why the paradox emerges.\r\nThe change score model do not assume that groups are similar at baseline. It tests if groups differ on average on how much they change on the outcome variable from pretest to posttest. The effect of treatment is the difference of pre to post differences between the two groups (Walker, 2021).\r\nWhat Lord (1967) observed is that given a treatment that produces no effect on the outcome, a researcher using the ANCOVA model would conclude that, assuming equal baseline scores, one subgroup appears to gain significantly more than the other. A second researcher analyzing the same dataset with the change score model would not reject the null hypothesis that \\(\\beta_1\\) = 0.\r\nThough we have used a categorical predictor \\(group_i\\), the emergence of the paradox is extended to continuous variables. To give rise to the artifact, there needs only to exists some correlation between this predictor and the pretest scores as well as random error in the assessment of change (Eriksson & Häggström, 2014). It can happen if, for example, participants are allocated into groups based on scores on some continuous predictor that happens to be associated with the parameter under investigation (Wright, 2006).\r\nIn most research settings (e.g. psychology), measurement is made with some degree of error. In the persence of random error of measurement, this variance is incorporated in the pre and posttest scores. If we let \\(U_i\\) represent the “true score” (i.e. what we would get if there were no measurement error) of subject \\(i\\), we have:\r\n\\[\\begin{equation}\r\npre_i = U_i^{pre} + e_i^{pre}\r\n\\tag{3}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\npost_i = U_i^{post} + e_i^{post}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nThen, because the expected effect size of treatment is zero, \\(U_i^{post} - U_i^{pre}\\) reflects only the difference between random errors (Eriksson & Häggström, 2014).\r\nTo keep consistent with the terminology used in Eriksson & Häggström (2014), we use \\(P_i\\) to denote the group variable for the \\(i^{th}\\) subject, where \\(P_i = 0\\) for Group A and \\(P_i = 1\\) for Group B (keep in mind that there is no restriction on \\(P\\) begin solely categorical). Because the “true” pretest scores is estimated from the grouping variable, we have:\r\n\\[\\begin{equation}\r\nU_i = a + bP_i + \\epsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nwhere \\(a\\) is the model intercept, \\(b\\) the regression coefficient of \\(P_i\\) on \\(U_i\\) and \\(\\epsilon_i\\) is the between-individual variation in \\(U\\).\r\nLets assume the error term in the measurement of pretest and posttest scores is drawn from a normal distribution with mean 0 and standard deviation, \\(\\sigma\\). Likewise, the error term for the regression of property \\(P\\) on true score \\(U_i\\) is drawn from a normal distribution with mean 0 and standard deviation \\(s\\).\r\nFor a sample of 200 subjects we set, \\(a\\) = 0.5, \\(b\\) = 0.4, \\(s\\) = 0.1, \\(\\sigma\\) = 0.1.\r\n\r\n\r\nset.seed(7895) # set seed for reproducibility\r\n\r\na <- 0.5 # model intercept\r\nb <- 0.4 # regression coefficient\r\nsigma_error <- .1 # pre and posttest error term standard deviation\r\nvar_error <-  .1  # P error term standard deviation\r\nn <- 200 # sample size\r\n\r\ngroup <- rep(c(1,0), each=n/2) # group predictor\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error) # true score\r\npost <- U + rnorm(n, 0, sigma_error) # posttest scores\r\npre <- U + rnorm(n, 0, sigma_error)  # pretest scorers\r\nchange <- post-pre                   # change scores\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\npre_post <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = post, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = post, color=group),method='lm', \r\n              se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  stat_ellipse(aes(x= pre, y = post, color=group),size = 1) +\r\n  labs(x = \"Pretest scores\",\r\n       y = \"Posttest scores\") +\r\n  scale_color_manual(name = \"Group\", labels = c(\"A\", \"B\"), \r\n                     values = c(\"#1E88E5\", \"#D81B60\")) +\r\n  theme_minimal(12) +\r\n  geom_smooth(aes(x= pre, y = post), method='lm', se = FALSE, color = \"#000000\",\r\n              linetype = \"dashed\", formula= y~x, fullrange=TRUE) +\r\n  xlim(0, NA) + \r\n  ylim(0, NA)\r\n\r\npre_change <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = change, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = change, color=group),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  geom_smooth(aes(x= pre, y = change),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  theme_minimal(12)\r\n\r\npre_post\r\n\r\n\r\n\r\n\r\nFigure 1: Scatterplot for the average gain from pre to post treatment. Blue line is the slope for group A and the red line is the slope for group B. The black dashed line is the slope for average gain taking both groups together\r\n\r\n\r\n\r\nSo, what do we see? The black dashed line crossing both elipses is close to 45° which means that, taking both groups as a whole, looks like there is no evidence of average differences between groups. In this particular example the slopes for both groups behave as we should often expect when there is no change in scores between two time points. It is visually clear that both are very similar.\r\nNotice that when subjects from groups A and B fall on the same range of pretest scores we see more red dots on the upper half of the plot, suggesting higher posttest scores from subjects in group B. As noted by Eriksson & Häggström (2014) what is happening is simply an effect of regression to the mean. Because when subjects from group B (higher true score) score low on pretest, they are more likely to do better at posttest. A similar logic applies to high scorers from group A scoring poorer on the followup.\r\nA mediational perspective\r\nIt can also help to visualize the difference between the two models using diagrams. As shown by Pearl (2016), we can represent Lord’s paradox as a mediation model,\r\n\r\n\r\nknitr::include_graphics(\"img/dag2.jpg\")\r\n\r\n\r\n\r\n\r\nFigure 2: Linear version of the model showing pretest scores (Pre) as a mediator between Group (G) and posttest scores (Post). Adapted from Pearl (2016)\r\n\r\n\r\n\r\nwhere G is the group variable, Pre the pretest scores, Post the posttest scores and Y stands for the difference Post - Pre. What differentiates the ANCOVA and the change score perspectives lies on which effect we wish to estimate. Assuming no confounding, the total effect is estimated as\r\n\\[\\begin{equation}\r\nTE = (b + ac) - a\r\n\\tag{6}\r\n\\end{equation}\\]\r\nSetting \\(b = a(1-c)\\) all paths cancel out each other and we obtain the total effect of 0 observed in our made up experiment.\r\nIf we opt for adjusting for pretest scores, the path \\(ac\\) is blocked and only the direct effect is estimated, which is,\r\n\\[\\begin{equation}\r\nDE = b\r\n\\tag{7}\r\n\\end{equation}\\]\r\nBecause the direct effect is positive, it becomes clear why the effect for the ANCOVA model is non-zero.\r\nThe regression artifact\r\nThe issue raised in Farmus et al. (2019) is that Lord’s Paradox isn’t restricted to categorical predictors. A similar effect can emerge if a continuous predictor of change is correlated with pretest scores and an ANCOVA model is used. Through a literature review, the authors confirm that studies meeting these conditions are not uncommon among papers published in top psychology journals.\r\nFor simplicity I’ll not bother to show the mathematical derivation of the regression artifact here but the interested reader can refer to Eriksson & Häggström (2014) or Kim (2018). What we need to have in mind is that the regression artifact is the coefficient \\(\\beta_1\\) from equation (2). Using Eriksson & Häggström (2014) notation, the estimate for the regression artifact is expressed as:\r\n\\[\\begin{equation}\r\n\\hat{K} = \\frac{b\\sigma^2}{(s^2+\\sigma^2)}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nIf we input the parameters set to build figure 1 and calculate the value of \\(K\\), assuming infinite samples, \\(K\\) = 0.2.\r\nSurprisingly, re-running Eriksson & Häggström (2014) simulation of the regression artifact, we get a different estimate for its standard deviation.\r\n\r\n\r\nShow code\r\n\r\nset.seed(7895)\r\n\r\nK <- c()\r\nreps <- 10000\r\n\r\nfor (i in seq(reps)) {\r\na <- 0.5\r\nb <- 0.4\r\nsigma_error <- .1\r\nvar_error <-  .1\r\nn <- 105\r\n\r\ngroup <- sample(c(0, 1), n, replace = TRUE)\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error)\r\npost <- U + rnorm(n, 0, sigma_error)\r\npre <- U + rnorm(n, 0, sigma_error)\r\nchange <- post-pre\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n  s <- summary(lm(change ~ pre + group, data = sim.df))\r\n  K[i] <- s$coefficients[3,1]\r\n}\r\n\r\nk_mean <- mean(K)\r\nk_sd <- sd(K)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntibble::enframe(K) |> \r\n  ggplot(aes(x = value)) + \r\n  geom_histogram(color = \"black\", fill = \"lightblue1\") +\r\n  labs(x = latex2exp::TeX(\"Coefficient for $\\\\hat{K}$\"),\r\n       y = \"Frequency\") +\r\n  theme_minimal(12)\r\n\r\n\r\n\r\n\r\nFigure 3: Distribution of the coefficients for predictor \\(P\\) on the change score\r\n\r\n\r\n\r\nAs expected, the mean value for \\(\\hat{K}\\) is 0.2. Our estimate for the standard deviation of \\(\\hat{K}\\) is 0.04, close to half the value Eriksson & Häggström (2014) reported. In their study, they have found 98% of \\(\\hat{K}\\) values greater than zero. In 10,000 simulated datasets, we found the frequency of \\(\\hat{K}\\) > 0 to be 100%1.\r\nRe-running Farmus et. al (2019)\r\nWe now return to Farmus et al. (2019) study. Including the baseline scores as covariate in a pretest to posttest change model may lead us to falsely conclude a significant association if a non-negligible correlation exists between the predictor and the covariate. The authors use Monte Carlo simulations to estimate Type I error rate in scenarios like the one described, varying the size of \\(b\\) (equation (5)), and the sample size.\r\nOther parameters were fixed such that the standard deviation of \\(e_i^{pre}\\), \\(e_i^{post}\\), and \\(\\epsilon_i\\) were set to 1. The variance of \\(P\\) (\\(\\sigma^2_P\\)) was also set to 1. Both \\(\\beta_0\\) and \\(a\\) were set to 0. Coefficient \\(b\\) ranged from -1 to 1 by 0.5 steps and sample sizes of 20, 50, 100, and 1,000 were used. To keep every condition exact as the original study, 5,000 simulations were run for each pair and the statistical significance level set at 0.05.\r\nThe simulation will only estimate the Type I error rates because the size of parameters like the regression artifact (\\(K\\)) and the correlation between the pretest scores and true score (\\(\\rho(pre,P)\\)) can be estimated from the parameters values fixed previously.\r\nLooking at equations (3) and (5) we know that,\r\n\\[\\begin{equation}\r\npre_i = a + bP_i + \\epsilon_i + e_i^{pre}\r\n\\tag{9}\r\n\\end{equation}\\]\r\nthen calculating \\(\\rho_{(pre,P)}\\) becomes straightforward\r\n\\[\\begin{align}\r\n\\begin{split}\r\n  \\rho_{pre,P} & = \\frac{\\mathrm{Cov}(pre, P)}{\\sigma_{pre}\\sigma_{P}}\\\\\r\n  & = \\frac{b\\sigma^2_P}{(\\sqrt{b^2\\sigma^2_P + s^2 + \\sigma^2}) + \\sigma^2_P}\r\n\\end{split}\r\n  \\tag{10}\r\n\\end{align}\\]\r\nOur first step is to create a function that takes as arguments the number of runs, sample size and the population value of coefficient \\(b\\). We can store the p-value of each analysis and count afterwards how many were less than the the alpha level.\r\n\r\n\r\nsim.reg.artifact <- function(n.sims, sample.size, beta) {\r\n\r\n  reps <- n.sims\r\n  n <- sample.size\r\n  beta <- beta\r\n  b0 <- 0\r\n  X.sd <- sigma_error <- var_error <-  1\r\n  pval <- NULL\r\n  r <- (beta*(X.sd)^2)/(sqrt((beta^2)*((X.sd)^2) + (sigma_error^2) + (var_error^2)))\r\n  K <- (beta*sigma_error)/(sigma_error + var_error) # the regression artifact\r\n\r\n  for (i in seq(1, reps)) {\r\n\r\n    P = rnorm(n = n, mean = 0, sd = 1) # Property P from equation 5\r\n    U <- b0 + beta*P + rnorm(n, 0, var_error) # The \"true score\" from equation 5\r\n\r\n    pre <- U + rnorm(n, 0, sigma_error) # Pretest scores - equation 3\r\n    post <-  U + rnorm(n, 0, sigma_error) # Posttest scores - equation 4\r\n\r\n    Xy <- as.data.frame(cbind(P, pre, post)) # A dataframe with outcome and predictors\r\n    colnames(Xy) <- c(\"P\", \"pre\", \"post\")\r\n    \r\n    fit <- lm(post ~ ., data = Xy)\r\n    s <- summary(fit)\r\n    pval[i] <- ifelse(s$coefficients[2,4] < 0.05, 1, 0) # p-value for the coefficient of P\r\n\r\n  }\r\n  \r\n  # store the simulated results in a data frame for display\r\n  res <- data.frame(\r\n    sample_size = n,\r\n    bx = beta,\r\n    correlation = r,\r\n    artifact = K,\r\n    error_rate = mean(pval)\r\n  )\r\n\r\n  return(res)\r\n}\r\n\r\n\r\n\r\nNesting for loops can easily become confusing, so I have opted to use the purrr::map_dfr() function, which I find to be a simpler and more elegant solution.\r\n\r\n\r\nsamplesize <- list(20, 50, 100, 1000)\r\ncoefs <- as.list(seq(-1,1, by = 0.5))\r\n\r\n\r\nfinaltable <- purrr::map_dfr(samplesize,\r\n                             ~purrr::map2(.x, coefs,\r\n                                          ~sim.reg.artifact(5000, .x, .y)))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfinaltable |> \r\n  tidyr::pivot_wider(names_from = sample_size, values_from = error_rate) |>\r\n  kbl(\r\n    col.names = c(\"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"N = 20\",\r\n                  \"N = 50\", \"N = 100\", \"N = 1,000\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) |> \r\n  add_header_above(c(\" \" = 3, \"Type I error rate\" = 4 ))\r\n\r\n\r\n\r\n\r\n\r\n\r\nType I error rate\r\n\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nN = 20\r\n\r\n\r\nN = 50\r\n\r\n\r\nN = 100\r\n\r\n\r\nN = 1,000\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.577\r\n\r\n\r\n-0.50\r\n\r\n\r\n0.267\r\n\r\n\r\n0.612\r\n\r\n\r\n0.900\r\n\r\n\r\n1.000\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.333\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.131\r\n\r\n\r\n0.258\r\n\r\n\r\n0.471\r\n\r\n\r\n1.000\r\n\r\n\r\n0.0\r\n\r\n\r\n0.000\r\n\r\n\r\n0.00\r\n\r\n\r\n0.051\r\n\r\n\r\n0.051\r\n\r\n\r\n0.048\r\n\r\n\r\n0.052\r\n\r\n\r\n0.5\r\n\r\n\r\n0.333\r\n\r\n\r\n0.25\r\n\r\n\r\n0.120\r\n\r\n\r\n0.259\r\n\r\n\r\n0.475\r\n\r\n\r\n1.000\r\n\r\n\r\n1.0\r\n\r\n\r\n0.577\r\n\r\n\r\n0.50\r\n\r\n\r\n0.263\r\n\r\n\r\n0.618\r\n\r\n\r\n0.895\r\n\r\n\r\n1.000\r\n\r\n\r\nGreat! Our results match pretty closely those reported by Farmus et al. (2019). Notice that in a world where \\(\\rho_{(pre,P)}\\) is zero, in the long run the Type I error rate stay within range of what was expected.\r\nTo wrap up, we can use the function just created to also reproduce the simulations Farmus et al. (2019) ran using median parameter values discovered in their literature review of experimental psychology studies.\r\n\r\n\r\nShow code\r\n\r\nsim2 <- sim.reg.artifact(n.sims = 5000, sample.size = 239, beta = .4)\r\n\r\nsim2 |> \r\n  kbl(\r\n    col.names = c(\"Sample size\", \"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"Type I error rate\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\n\r\n\r\nSample size\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nType I error rate\r\n\r\n\r\n239\r\n\r\n\r\n0.4\r\n\r\n\r\n0.272\r\n\r\n\r\n0.2\r\n\r\n\r\n0.674\r\n\r\n\r\nAgain we have a match of results with the original study. What makes those results alarming is the Type I error rate of 0.67! As the authors emphasize, common conditions in psychological research may elicit unacceptably high error rates which researchers may not be aware of if an inappropriate model is employed.\r\nConclusion\r\nWe have covered a definition of Lord’s Paradox and argued that choosing a model that does not fit the research question can lead to biased conclusions. As Farmus et al. (2019) outline, correlation between the predictor and the outcome and some amount of measurement error are necessary conditions for the emergence of a regression artifact. It is important that researchers take this into consideration before drawing conclusions about the data.\r\nFollowing recent calls to reproduce simulation studies (Lohmann et al., 2021) we have worked to reproduce simulations performed by two papers discussing Lord’s Paradox with continuous predictors. Monte Carlo simulations are not needed to demonstrate the regression artifact and no major recommendations are being proposed by the original studies based on the simulation results. Even so, a minor discrepancy from Eriksson & Häggström (2014) was found, highlighting that computer code is not error free. Reproducing simulation analysis proved to be fruitful not only in finding divergent results but also as a coding exercise.\r\nLord’s Paradox have been and continues to be widely debated in the quantitative methods literature. Many of the papers referenced in the text broaden the discussion investigating this topic in different and more complex scenarios. Hopefully this post has reached its instructive purpose of teaching a few things about simulating data in R. If you have any comments or questions, feel free to reach me via email or twitter.\r\n\r\n\r\n\r\nEriksson, K., & Häggström, O. (2014). Lord’s Paradox in a Continuous Setting and a Regression Artifact in Numerical Cognition Research. PLOS ONE, 9(4), e95949. https://doi.org/f2z5hc\r\n\r\n\r\nFarmus, L., Arpin-Cribbie, C. A., & Cribbie, R. A. (2019). Continuous predictors of pretest-posttest change: Highlighting the impact of the regression artifact. Frontiers in Applied Mathematics and Statistics, 4, 64. https://doi.org/gk92gx\r\n\r\n\r\nKim, S. B. (2018). Explaining lord’s paradox in introductory statistical theory courses. International Journal of Statistics and Probability, 7(4), 1. https://doi.org/10.5539/ijsp.v7n4p1\r\n\r\n\r\nLohmann, A., Astivia, O. L. O., Morris, T., & Groenwold, R. H. H. (2021). It’s time! 10 + 1 reasons we should start replicating simulation studies. https://doi.org/10.31234/osf.io/agsnt\r\n\r\n\r\nLord, F. M. (1967). A paradox in the interpretation of group comparisons. Psychological Bulletin, 68(5), 304–305. https://doi.org/10.1037/h0025105\r\n\r\n\r\nPearl, J. (2016). Lord’s Paradox Revisited  (Oh Lord! Kumbaya!). Journal of Causal Inference, 4(2). https://doi.org/10.1515/jci-2016-0021\r\n\r\n\r\nSenn, S. (2021). Cause for concern. In LinkedIn. LinkedIn. https://www.linkedin.com/pulse/cause-concern-stephen-senn/\r\n\r\n\r\nWalker, J. A. (2021). Models for longitudinal experiments – pre-post designs. In Elements of statistical modeling for experimental biology. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html\r\n\r\n\r\nWright, D. B. (2006). Comparing groups in a before-after design: When t test and ANCOVA produce different results. British Journal of Educational Psychology, 76(3), 663–675. https://doi.org/10.1348/000709905x52210\r\n\r\n\r\nI’m not excluding the possibility that it may have been me that got it wrong. If this is the case and you identify where I screwed up, please let me know!↩︎\r\n",
    "preview": "posts/2021-11-04-simulating-regression-artifact/simulating-regression-artifact_files/figure-html5/sim-plot-1-1.png",
    "last_modified": "2021-11-12T21:12:10-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My blog",
    "description": "Welcome to our new blog, My blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-11-04",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-04T19:39:51-03:00",
    "input_file": {}
  }
]
