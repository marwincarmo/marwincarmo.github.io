[
  {
    "path": "posts/2021-11-04-simulating-regression-artifact/",
    "title": "Regression Artifact - Re-running Farmus et. al (2019)",
    "description": "In this post I'll how false positives can emerge if a model not fit to the research question is chosen. I also provide code for a simple Monte Carlo simulation of the frequency of these false positive results.",
    "author": [
      {
        "name": "Marwin Carmo",
        "url": "https://github.com/marwincarmo"
      }
    ],
    "date": "2021-11-04",
    "categories": [
      "simulation",
      "regression"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nLord’s Paradox\r\nand the Regression Artifact\r\nRe-running Farmus et. al\r\n(2019)\r\nConclusion\r\n\r\nMotivation\r\nRecently I came across a publication (Farmus et al., 2019) where the authors\r\noutline a statistical artifact that may arise when researchers add\r\ncontinuous predictors correlated with baseline scores as covariates in\r\nregression models of pretest-posttest analysis. This effect is known\r\nwidely as Lord’s\r\nParadox.\r\nFirst I’ll briefly cover what is the regression artifact discussed in\r\nthe first part of the paper. My main goal, however, is to rerun the\r\nsimulation from part two of their study. This was motivated by the\r\nunavailability of the source code as well as an exercise on building a\r\nsimple simulation study. Replicating simulation studies is a good\r\npractice not only to spot potential errors or biases in the results but\r\nalso to learn and rehearse open research practices (Lohmann et al., 2021).\r\nThe code is pretty straightforward and there is nothing too fancy\r\nabout the methods, but I hope you may learn something from this post\r\nthat you can apply in your own studies. Having to think about how to\r\ntranslate words and equations into code also made me pause and think\r\nwith more depth on what I was reading.\r\nI’ll be using R for the simulations. For pedagogical reasons I have\r\nopted to work with base R, limiting the use of external packages only to\r\nfacilitate summarizing results and building plots. I’m saying that\r\nbecause there is the great faux package\r\ndeveloped by Lisa\r\nDeBruine for simulating factorial designs that could save us many\r\nlines of code.\r\nLord’s Paradox and\r\nthe Regression Artifact\r\nSay that we want to test a treatment in two preexisting groups, A and\r\nB. Given some prior knowledge about the nature of our hypothetical\r\ndependent variable, we have strong basis to suppose that a random\r\nsubject from group B is higher on average on DV scores than a random\r\nsubject from group A. Because those groups occur naturally it isn’t\r\npossible to perform random assignment of subjects to groups.\r\nWe need to specify how we’re going to analyse the data. Given our\r\nresearch design and question there are two potential statistical models\r\nto be chosen from (Farmus et al.,\r\n2019):\r\nA change score model (t test)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + pre_i + e_i\r\n\\tag{1}\r\n\\end{equation}\\]\r\nA regression based model (ANCOVA)\r\n\\[\\begin{equation}\r\npost_i = \\beta_0 + \\beta_1group_i + \\beta_2pre_i + e_i\r\n\\tag{2}\r\n\\end{equation}\\]\r\nHere \\(pre_i\\) and \\(post_i\\) are the pretest and posttest\r\nscores of subject \\(i\\). In equation\r\n(1), \\(\\beta_1\\) is the\r\nmain effect of group (or group mean differences) on gain scores and in\r\nequation (2), it is the partial regression coefficient of\r\n\\(group\\) adjusted for pretest scores.\r\nThe model intercept is \\(\\beta_0\\) and\r\nthe model residuals is represented by \\(e_i\\).\r\nIt should be noted that using \\(post_i\\) or the change score (\\(post_i - pre_i\\)) as the outcome variable\r\nyields the same results if \\(pre_i\\) is\r\na covariate in the model (Senn, 2021).\r\nThough in practice we could choose any of these two models, we may\r\nreach different conclusions depending on which one we use. In fact, the\r\nchoice of the model relies on the questions we’re asking.\r\nThe ANCOVA model is preferred when we do not expect baseline\r\ndifferences between groups. This model estimates the difference in post\r\ntreatment scores controlling for baseline variations (Walker, 2021). Using pretest scores as\r\ncovariate when groups are known to be different at baseline is a poor\r\nchoice because we do not expect the initial differences to disappear\r\nwhen scores are averaged over the group (Pearl, 2016). As we’ll see shortly, this\r\nis exactly why the paradox emerges.\r\nThe change score model do not assume that groups are similar at\r\nbaseline. It tests if groups differ on average on how much they change\r\non the outcome variable from pretest to posttest. The effect of\r\ntreatment is the difference of pre to post differences between the two\r\ngroups (Walker, 2021).\r\nWhat Lord (1967) observed is\r\nthat given a treatment that produces no effect on the\r\noutcome, a researcher using the ANCOVA model would conclude that,\r\nassuming equal baseline scores, one subgroup appears to gain\r\nsignificantly more than the other. A second researcher analyzing the\r\nsame dataset with the change score model would not reject the null\r\nhypothesis that \\(\\beta_1\\) = 0.\r\nThough we have used a categorical predictor \\(group_i\\), the emergence of the paradox is\r\nextended to continuous variables. To give rise to the artifact, there\r\nneeds only to exists some correlation between this predictor and the\r\npretest scores as well as random error in the assessment of change (Eriksson & Häggström, 2014). It can\r\nhappen if, for example, participants are allocated into groups based on\r\nscores on some continuous predictor that happens to be associated with\r\nthe parameter under investigation (Wright, 2006).\r\nIn most research settings (e.g. psychology), measurement is made with\r\nsome degree of error. In the persence of random error of measurement,\r\nthis variance is incorporated in the pre and posttest scores. If we let\r\n\\(U_i\\) represent the “true score”\r\n(i.e. what we would get if there were no measurement error) of subject\r\n\\(i\\), we have:\r\n\\[\\begin{equation}\r\npre_i = U_i^{pre} + e_i^{pre}\r\n\\tag{3}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\npost_i = U_i^{post} + e_i^{post}\r\n\\tag{4}\r\n\\end{equation}\\]\r\nThen, because the expected effect size of treatment is zero, \\(U_i^{post} - U_i^{pre}\\) reflects only the\r\ndifference between random errors (Eriksson & Häggström, 2014).\r\nTo keep consistent with the terminology used in Eriksson & Häggström (2014), we use\r\n\\(P_i\\) to denote the group variable\r\nfor the \\(i^{th}\\) subject, where \\(P_i = 0\\) for Group A and \\(P_i = 1\\) for Group B (keep in mind that\r\nthere is no restriction on \\(P\\) begin\r\nsolely categorical). Because the “true” pretest scores is estimated from\r\nthe grouping variable, we have:\r\n\\[\\begin{equation}\r\nU_i = a + bP_i + \\epsilon_i\r\n\\tag{5}\r\n\\end{equation}\\]\r\nwhere \\(a\\) is the model intercept,\r\n\\(b\\) the regression coefficient of\r\n\\(P_i\\) on \\(U_i\\) and \\(\\epsilon_i\\) is the between-individual\r\nvariation in \\(U\\).\r\nLets assume the error term in the measurement of pretest and posttest\r\nscores is drawn from a normal distribution with mean 0 and standard\r\ndeviation, \\(\\sigma\\). Likewise, the\r\nerror term for the regression of property \\(P\\) on true score \\(U_i\\) is drawn from a normal distribution\r\nwith mean 0 and standard deviation \\(s\\).\r\nFor a sample of 200 subjects we set, \\(a\\) = 0.5, \\(b\\) = 0.4, \\(s\\) = 0.1, \\(\\sigma\\) = 0.1.\r\n\r\n\r\nset.seed(7895) # set seed for reproducibility\r\n\r\na <- 0.5 # model intercept\r\nb <- 0.4 # regression coefficient\r\nsigma_error <- .1 # pre and posttest error term standard deviation\r\nvar_error <-  .1  # P error term standard deviation\r\nn <- 200 # sample size\r\n\r\ngroup <- rep(c(1,0), each=n/2) # group predictor\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error) # true score\r\npost <- U + rnorm(n, 0, sigma_error) # posttest scores\r\npre <- U + rnorm(n, 0, sigma_error)  # pretest scorers\r\nchange <- post-pre                   # change scores\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n\r\n\r\n\r\n(ref:sim-plot-1) Scatterplot for the average gain from pre to post\r\ntreatment. Blue line is the slope for group A and the red line is the\r\nslope for group B. The black dashed line is the slope for average gain\r\ntaking both groups together\r\n\r\n\r\nShow code\r\n\r\npre_post <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = post, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = post, color=group),method='lm', \r\n              se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  stat_ellipse(aes(x= pre, y = post, color=group),size = 1) +\r\n  labs(x = \"Pretest scores\",\r\n       y = \"Posttest scores\") +\r\n  scale_color_manual(name = \"Group\", labels = c(\"A\", \"B\"), \r\n                     values = c(\"#1E88E5\", \"#D81B60\")) +\r\n  theme_minimal(12) +\r\n  geom_smooth(aes(x= pre, y = post), method='lm', se = FALSE, color = \"#000000\",\r\n              linetype = \"dashed\", formula= y~x, fullrange=TRUE) +\r\n  xlim(0, NA) + \r\n  ylim(0, NA)\r\n\r\npre_change <- ggplot(sim.df) +\r\n  geom_point(aes(x= pre, y = change, color=group),size = 2.5) +\r\n  geom_smooth(aes(x= pre, y = change, color=group),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  geom_smooth(aes(x= pre, y = change),method='lm', se = FALSE, formula= y~x, fullrange=TRUE) +\r\n  theme_minimal(12)\r\n\r\npre_post\r\n\r\n\r\n\r\n\r\nFigure 1: (ref:sim-plot-1)\r\n\r\n\r\n\r\nSo, what do we see? The black dashed line crossing both elipses is\r\nclose to 45° which means that, taking both groups as a whole, looks like\r\nthere is no evidence of average differences between groups. In this\r\nparticular example the slopes for both groups behave as we should often\r\nexpect when there is no change in scores between two time points. It is\r\nvisually clear that both are very similar.\r\nNotice that when subjects from groups A and B fall on the same range\r\nof pretest scores we see more red dots on the upper half of the plot,\r\nsuggesting higher posttest scores from subjects in group B. As noted by\r\nEriksson &\r\nHäggström (2014) what is happening is simply an\r\neffect of regression to the mean. Because when subjects from group B\r\n(higher true score) score low on pretest, they are more likely to do\r\nbetter at posttest. A similar logic applies to high scorers from group A\r\nscoring poorer on the followup.\r\nA mediational perspective\r\nIt can also help to visualize the difference between the two models\r\nusing diagrams. As shown by Pearl (2016), we can represent Lord’s paradox\r\nas a mediation model,\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"img/dag2.jpg\")\r\n\r\n\r\n\r\n\r\nFigure 2: Linear version of the model showing pretest scores\r\n(Pre) as a mediator between Group (G) and posttest scores (Post).\r\nAdapted from Pearl (2016)\r\n\r\n\r\n\r\nwhere G is the group variable, Pre\r\nthe pretest scores, Post the posttest scores and\r\nY stands for the difference Post - Pre. What\r\ndifferentiates the ANCOVA and the change score perspectives lies on\r\nwhich effect we wish to estimate. Assuming no confounding, the total\r\neffect is estimated as\r\n\\[\\begin{equation}\r\nTE = (b + ac) - a\r\n\\tag{6}\r\n\\end{equation}\\]\r\nSetting \\(b = a(1-c)\\) all paths\r\ncancel out each other and we obtain the total effect of 0 observed in\r\nour made up experiment.\r\nIf we opt for adjusting for pretest scores, the path \\(ac\\) is blocked and only the direct\r\neffect is estimated, which is,\r\n\\[\\begin{equation}\r\nDE = b\r\n\\tag{7}\r\n\\end{equation}\\]\r\nBecause the direct effect is positive, it becomes clear why the\r\neffect for the ANCOVA model is non-zero.\r\nThe regression artifact\r\nThe issue raised in Farmus et al. (2019) is that Lord’s Paradox isn’t\r\nrestricted to categorical predictors. A similar effect can emerge if a\r\ncontinuous predictor of change is correlated with pretest scores and an\r\nANCOVA model is used. Through a literature review, the authors confirm\r\nthat studies meeting these conditions are not uncommon among papers\r\npublished in top psychology journals.\r\nFor simplicity I’ll not bother to show the mathematical derivation of\r\nthe regression artifact here but the interested reader can refer to\r\nEriksson &\r\nHäggström (2014) or Kim (2018). What we need to have in mind is\r\nthat the regression artifact is the coefficient \\(\\beta_1\\) from equation (2).\r\nUsing Eriksson &\r\nHäggström (2014) notation, the estimate for the\r\nregression artifact is expressed as:\r\n\\[\\begin{equation}\r\n\\hat{K} = \\frac{b\\sigma^2}{(s^2+\\sigma^2)}\r\n\\tag{8}\r\n\\end{equation}\\]\r\nIf we input the parameters set to build figure 1\r\nand calculate the value of \\(K\\),\r\nassuming infinite samples, \\(K\\) =\r\n0.2.\r\nSurprisingly, re-running Eriksson & Häggström (2014)\r\nsimulation of the regression artifact, we get a different estimate for\r\nits standard deviation.\r\n\r\n\r\nShow code\r\n\r\nset.seed(7895)\r\n\r\nK <- c()\r\nreps <- 10000\r\n\r\nfor (i in seq(reps)) {\r\na <- 0.5\r\nb <- 0.4\r\nsigma_error <- .1\r\nvar_error <-  .1\r\nn <- 105\r\n\r\ngroup <- sample(c(0, 1), n, replace = TRUE)\r\nU <- cbind(1, group) %*% c(a, b) + rnorm(n, 0, var_error)\r\npost <- U + rnorm(n, 0, sigma_error)\r\npre <- U + rnorm(n, 0, sigma_error)\r\nchange <- post-pre\r\n\r\nsim.df <- data.frame(\"post\" =  post, \r\n                     \"pre\" =  pre, \r\n                     \"change\" = change, \r\n                     \"group\"= factor(group))\r\n  s <- summary(lm(change ~ pre + group, data = sim.df))\r\n  K[i] <- s$coefficients[3,1]\r\n}\r\n\r\nk_mean <- mean(K)\r\nk_sd <- sd(K)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntibble::enframe(K) |> \r\n  ggplot(aes(x = value)) + \r\n  geom_histogram(color = \"black\", fill = \"lightblue1\") +\r\n  labs(x = latex2exp::TeX(\"Coefficient for $\\\\hat{K}$\"),\r\n       y = \"Frequency\") +\r\n  theme_minimal(12)\r\n\r\n\r\n\r\n\r\nFigure 3: Distribution of the coefficients for predictor \\(P\\) on the change score\r\n\r\n\r\n\r\nAs expected, the mean value for \\(\\hat{K}\\) is 0.2. Our estimate for the\r\nstandard deviation of \\(\\hat{K}\\) is\r\n0.04, close to half the value Eriksson & Häggström (2014) reported.\r\nIn their study, they have found 98% of \\(\\hat{K}\\) values greater than zero. In\r\n10,000 simulated datasets, we found the frequency of \\(\\hat{K}\\) > 0 to be 100%1.\r\nRe-running Farmus et. al\r\n(2019)\r\nWe now return to Farmus et al. (2019) study. Including the baseline\r\nscores as covariate in a pretest to posttest change model may lead us to\r\nfalsely conclude a significant association if a non-negligible\r\ncorrelation exists between the predictor and the covariate. The authors\r\nuse Monte Carlo simulations to estimate Type I error rate in scenarios\r\nlike the one described, varying the size of \\(b\\) (equation (5)), and the\r\nsample size.\r\nOther parameters were fixed such that the standard deviation of \\(e_i^{pre}\\), \\(e_i^{post}\\), and \\(\\epsilon_i\\) were set to 1. The variance of\r\n\\(P\\) (\\(\\sigma^2_P\\)) was also set to 1. Both \\(\\beta_0\\) and \\(a\\) were set to 0. Coefficient \\(b\\) ranged from -1 to 1 by 0.5 steps and\r\nsample sizes of 20, 50, 100, and 1,000 were used. To keep every\r\ncondition exact as the original study, 5,000 simulations were run for\r\neach pair and the statistical significance level set at 0.05.\r\nThe simulation will only estimate the Type I error rates because the\r\nsize of parameters like the regression artifact (\\(K\\)) and the correlation between the\r\npretest scores and true score (\\(\\rho(pre,P)\\)) can be estimated from the\r\nparameters values fixed previously.\r\nLooking at equations (3) and (5) we know that,\r\n\\[\\begin{equation}\r\npre_i = a + bP_i + \\epsilon_i + e_i^{pre}\r\n\\tag{9}\r\n\\end{equation}\\]\r\nthen calculating \\(\\rho_{(pre,P)}\\)\r\nbecomes straightforward\r\n\\[\\begin{align}\r\n\\begin{split}\r\n  \\rho_{pre,P} & = \\frac{\\mathrm{Cov}(pre,\r\nP)}{\\sigma_{pre}\\sigma_{P}}\\\\\r\n  & = \\frac{b\\sigma^2_P}{(\\sqrt{b^2\\sigma^2_P + s^2 + \\sigma^2}) +\r\n\\sigma^2_P}\r\n\\end{split}\r\n  \\tag{10}\r\n\\end{align}\\]\r\nOur first step is to create a function that takes as arguments the\r\nnumber of runs, sample size and the population value of coefficient\r\n\\(b\\). We can store the\r\np-value of each analysis and count afterwards how many were\r\nless than the the alpha level.\r\n\r\n\r\nsim.reg.artifact <- function(n.sims, sample.size, beta) {\r\n\r\n  reps <- n.sims\r\n  n <- sample.size\r\n  beta <- beta\r\n  b0 <- 0\r\n  P.sd <- sigma_error <- var_error <-  1\r\n  pval <- NULL\r\n  r <- (beta*(P.sd)^2)/(sqrt((beta^2)*((P.sd)^2) + (sigma_error^2) + (var_error^2)))\r\n  K <- (beta*sigma_error)/(sigma_error + var_error) # the regression artifact\r\n\r\n  for (i in seq(1, reps)) {\r\n\r\n    P = rnorm(n = n, mean = 0, sd = 1) # Property P from equation 5\r\n    U <- b0 + beta*P + rnorm(n, 0, var_error) # The \"true score\" from equation 5\r\n\r\n    pre <- U + rnorm(n, 0, sigma_error) # Pretest scores - equation 3\r\n    post <-  U + rnorm(n, 0, sigma_error) # Posttest scores - equation 4\r\n\r\n    Xy <- as.data.frame(cbind(P, pre, post)) # A dataframe with outcome and predictors\r\n    colnames(Xy) <- c(\"P\", \"pre\", \"post\")\r\n    \r\n    fit <- lm(post ~ ., data = Xy)\r\n    s <- summary(fit)\r\n    pval[i] <- ifelse(s$coefficients[2,4] < 0.05, 1, 0) # p-value for the coefficient of P\r\n\r\n  }\r\n  \r\n  # store the simulated results in a data frame for display\r\n  res <- data.frame(\r\n    sample_size = n,\r\n    bx = beta,\r\n    correlation = r,\r\n    artifact = K,\r\n    error_rate = mean(pval)\r\n  )\r\n\r\n  return(res)\r\n}\r\n\r\n\r\n\r\nNesting for loops can easily become confusing, so I have opted to use\r\nthe purrr::map_dfr() function, which I find to be a simpler\r\nand more elegant solution.\r\n\r\n\r\nsamplesize <- list(20, 50, 100, 1000)\r\ncoefs <- as.list(seq(-1,1, by = 0.5))\r\n\r\n\r\nfinaltable <- purrr::map_dfr(samplesize,\r\n                             ~purrr::map2(.x, coefs,\r\n                                          ~sim.reg.artifact(5000, .x, .y)))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfinaltable |> \r\n  tidyr::pivot_wider(names_from = sample_size, values_from = error_rate) |>\r\n  kbl(\r\n    col.names = c(\"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"N = 20\",\r\n                  \"N = 50\", \"N = 100\", \"N = 1,000\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) |> \r\n  add_header_above(c(\" \" = 3, \"Type I error rate\" = 4 ))\r\n\r\n\r\n\r\n\r\n\r\n\r\nType I error rate\r\n\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nN = 20\r\n\r\n\r\nN = 50\r\n\r\n\r\nN = 100\r\n\r\n\r\nN = 1,000\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.577\r\n\r\n\r\n-0.50\r\n\r\n\r\n0.267\r\n\r\n\r\n0.612\r\n\r\n\r\n0.900\r\n\r\n\r\n1.000\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.333\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.131\r\n\r\n\r\n0.258\r\n\r\n\r\n0.471\r\n\r\n\r\n1.000\r\n\r\n\r\n0.0\r\n\r\n\r\n0.000\r\n\r\n\r\n0.00\r\n\r\n\r\n0.051\r\n\r\n\r\n0.051\r\n\r\n\r\n0.048\r\n\r\n\r\n0.052\r\n\r\n\r\n0.5\r\n\r\n\r\n0.333\r\n\r\n\r\n0.25\r\n\r\n\r\n0.120\r\n\r\n\r\n0.259\r\n\r\n\r\n0.475\r\n\r\n\r\n1.000\r\n\r\n\r\n1.0\r\n\r\n\r\n0.577\r\n\r\n\r\n0.50\r\n\r\n\r\n0.263\r\n\r\n\r\n0.618\r\n\r\n\r\n0.895\r\n\r\n\r\n1.000\r\n\r\n\r\nGreat! Our results match pretty closely those reported by Farmus et al. (2019). Notice\r\nthat in a world where \\(\\rho_{(pre,P)}\\) is zero, in the long run\r\nthe Type I error rate stay within range of what was expected.\r\nTo wrap up, we can use the function just created to also reproduce\r\nthe simulations Farmus et\r\nal. (2019) ran\r\nusing median parameter values discovered in their literature review of\r\nexperimental psychology studies.\r\n\r\n\r\nShow code\r\n\r\nsim2 <- sim.reg.artifact(n.sims = 5000, sample.size = 239, beta = .4)\r\n\r\nsim2 |> \r\n  kbl(\r\n    col.names = c(\"Sample size\", \"$b_x$\", \"$\\\\rho(pre,P)$\", \"Artifact\", \"Type I error rate\"),\r\n    digits = 3,\r\n    align = 'c'\r\n  ) |> \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\n\r\n\r\nSample size\r\n\r\n\r\n\\(b_x\\)\r\n\r\n\r\n\\(\\rho(pre,P)\\)\r\n\r\n\r\nArtifact\r\n\r\n\r\nType I error rate\r\n\r\n\r\n239\r\n\r\n\r\n0.4\r\n\r\n\r\n0.272\r\n\r\n\r\n0.2\r\n\r\n\r\n0.674\r\n\r\n\r\nAgain we have a match of results with the original study. What makes\r\nthose results alarming is the Type I error rate of 0.67! As the authors\r\nemphasize, common conditions in psychological research may elicit\r\nunacceptably high error rates which researchers may not be aware of if\r\nan inappropriate model is employed.\r\nConclusion\r\nWe have covered a definition of Lord’s Paradox and argued that\r\nchoosing a model that does not fit the research question can lead to\r\nbiased conclusions. As Farmus et al. (2019) outline, correlation between the\r\npredictor and the outcome and some amount of measurement error are\r\nnecessary conditions for the emergence of a regression artifact. It is\r\nimportant that researchers take this into consideration before drawing\r\nconclusions about the data.\r\nFollowing recent calls to reproduce simulation studies (Lohmann et al., 2021) we have worked to\r\nreproduce simulations performed by two papers discussing Lord’s Paradox\r\nwith continuous predictors. Monte Carlo simulations are not needed to\r\ndemonstrate the regression artifact and no major recommendations are\r\nbeing proposed by the original studies based on the simulation results.\r\nEven so, a minor discrepancy from Eriksson & Häggström (2014) was\r\nfound, highlighting that computer code is not error free. Reproducing\r\nsimulation analysis proved to be fruitful not only in finding divergent\r\nresults but also as a coding exercise.\r\nLord’s Paradox have been and continues to be widely debated in the\r\nquantitative methods literature. Many of the papers referenced in the\r\ntext broaden the discussion investigating this topic in different and\r\nmore complex scenarios. Hopefully this post has reached its instructive\r\npurpose of teaching a few things about simulating data in R. If you have\r\nany comments or questions, feel free to drop a comment below or reach me\r\nvia email or twitter.\r\n\r\n\r\n\r\nEriksson, K., & Häggström, O. (2014). Lord’s Paradox in\r\na Continuous Setting and a Regression Artifact\r\nin Numerical Cognition Research. PLOS ONE,\r\n9(4), e95949. https://doi.org/f2z5hc\r\n\r\n\r\nFarmus, L., Arpin-Cribbie, C. A., & Cribbie, R. A. (2019).\r\nContinuous predictors of pretest-posttest change: Highlighting the\r\nimpact of the regression artifact. Frontiers in Applied Mathematics\r\nand Statistics, 4, 64. https://doi.org/gk92gx\r\n\r\n\r\nKim, S. B. (2018). Explaining lord’s paradox in introductory statistical\r\ntheory courses. International Journal of Statistics and\r\nProbability, 7(4), 1. https://doi.org/10.5539/ijsp.v7n4p1\r\n\r\n\r\nLohmann, A., Astivia, O. L. O., Morris, T., & Groenwold, R. H. H.\r\n(2021). It’s time! 10 + 1 reasons we should start replicating\r\nsimulation studies. https://doi.org/10.31234/osf.io/agsnt\r\n\r\n\r\nLord, F. M. (1967). A paradox in the interpretation of group\r\ncomparisons. Psychological Bulletin, 68(5), 304–305.\r\nhttps://doi.org/10.1037/h0025105\r\n\r\n\r\nPearl, J. (2016). Lord’s Paradox Revisited \r\n(Oh Lord! Kumbaya!). Journal of Causal Inference,\r\n4(2). https://doi.org/10.1515/jci-2016-0021\r\n\r\n\r\nSenn, S. (2021). Cause for concern. In LinkedIn. LinkedIn. https://www.linkedin.com/pulse/cause-concern-stephen-senn/\r\n\r\n\r\nWalker, J. A. (2021). Models for longitudinal experiments – pre-post\r\ndesigns. In Elements of statistical modeling for experimental\r\nbiology. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html\r\n\r\n\r\nWright, D. B. (2006). Comparing groups in a before-after design: When t\r\ntest and ANCOVA produce different results. British Journal of\r\nEducational Psychology, 76(3), 663–675. https://doi.org/10.1348/000709905x52210\r\n\r\n\r\nI’m not excluding the possibility\r\nthat it may have been me that got it wrong. If this is the case and you\r\nidentify where I screwed up, please let me know!↩︎\r\n",
    "preview": "posts/2021-11-04-simulating-regression-artifact/simulating-regression-artifact_files/figure-html5/sim-plot-1-1.png",
    "last_modified": "2022-04-03T17:34:15-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
